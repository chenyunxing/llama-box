diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index 4e1d9115..ca67b428 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -117,7 +117,7 @@ llama_context::llama_context(
 
     LLAMA_LOG_INFO("%s: n_seq_max     = %u\n",   __func__, cparams.n_seq_max);
     LLAMA_LOG_INFO("%s: n_ctx         = %u\n",   __func__, cparams.n_ctx);
-    LLAMA_LOG_INFO("%s: n_ctx_per_seq = %u\n",   __func__, n_ctx_per_seq);
+//    LLAMA_LOG_INFO("%s: n_ctx_per_seq = %u\n",   __func__, n_ctx_per_seq);
     LLAMA_LOG_INFO("%s: n_batch       = %u\n",   __func__, cparams.n_batch);
     LLAMA_LOG_INFO("%s: n_ubatch      = %u\n",   __func__, cparams.n_ubatch);
     LLAMA_LOG_INFO("%s: causal_attn   = %d\n",   __func__, cparams.causal_attn);
@@ -126,14 +126,14 @@ llama_context::llama_context(
     LLAMA_LOG_INFO("%s: freq_base     = %.1f\n", __func__, cparams.rope_freq_base);
     LLAMA_LOG_INFO("%s: freq_scale    = %g\n",   __func__, cparams.rope_freq_scale);
 
-    if (n_ctx_per_seq < hparams.n_ctx_train) {
-        LLAMA_LOG_WARN("%s: n_ctx_per_seq (%u) < n_ctx_train (%u) -- the full capacity of the model will not be utilized\n",
-                __func__, n_ctx_per_seq, hparams.n_ctx_train);
+    if (cparams.n_ctx < hparams.n_ctx_train) {
+        LLAMA_LOG_WARN("%s: n_ctx (%u) < n_ctx_train (%u) -- the full capacity of the model will not be utilized\n",
+                __func__, cparams.n_ctx, hparams.n_ctx_train);
     }
 
-    if (n_ctx_per_seq > hparams.n_ctx_train) {
-        LLAMA_LOG_WARN("%s: n_ctx_per_seq (%u) > n_ctx_train (%u) -- possible training context overflow\n",
-                __func__, n_ctx_per_seq, hparams.n_ctx_train);
+    if (cparams.n_ctx > hparams.n_ctx_train) {
+        LLAMA_LOG_WARN("%s: n_ctx (%u) > n_ctx_train (%u) -- possible training context overflow\n",
+                __func__, cparams.n_ctx, hparams.n_ctx_train);
     }
 
     if (!params.swa_full && cparams.n_seq_max > 1 && hparams.is_swa_any()) {
@@ -639,19 +639,19 @@ void llama_context::set_abort_callback(bool (*abort_callback)(void * data), void
 }
 
 void llama_context::set_embeddings(bool value) {
-    LLAMA_LOG_DEBUG("%s: value = %d\n", __func__, value);
+//    LLAMA_LOG_DEBUG("%s: value = %d\n", __func__, value);
 
     cparams.embeddings = value;
 }
 
 void llama_context::set_causal_attn(bool value) {
-    LLAMA_LOG_DEBUG("%s: value = %d\n", __func__, value);
+//    LLAMA_LOG_DEBUG("%s: value = %d\n", __func__, value);
 
     cparams.causal_attn = value;
 }
 
 void llama_context::set_warmup(bool value) {
-    LLAMA_LOG_DEBUG("%s: value = %d\n", __func__, value);
+//    LLAMA_LOG_DEBUG("%s: value = %d\n", __func__, value);
 
     cparams.warmup = value;
 }
@@ -659,14 +659,14 @@ void llama_context::set_warmup(bool value) {
 void llama_context::set_adapter_lora(
             llama_adapter_lora * adapter,
             float scale) {
-    LLAMA_LOG_DEBUG("%s: adapter = %p, scale = %f\n", __func__, (void *) adapter, scale);
+//    LLAMA_LOG_DEBUG("%s: adapter = %p, scale = %f\n", __func__, (void *) adapter, scale);
 
     loras[adapter] = scale;
 }
 
 bool llama_context::rm_adapter_lora(
             llama_adapter_lora * adapter) {
-    LLAMA_LOG_DEBUG("%s: adapter = %p\n", __func__, (void *) adapter);
+//    LLAMA_LOG_DEBUG("%s: adapter = %p\n", __func__, (void *) adapter);
 
     auto pos = loras.find(adapter);
     if (pos != loras.end()) {
@@ -678,7 +678,7 @@ bool llama_context::rm_adapter_lora(
 }
 
 void llama_context::clear_adapter_lora() {
-    LLAMA_LOG_DEBUG("%s: call\n", __func__);
+//    LLAMA_LOG_DEBUG("%s: call\n", __func__);
 
     loras.clear();
 }
@@ -689,7 +689,7 @@ bool llama_context::apply_adapter_cvec(
                 int32_t   n_embd,
                 int32_t   il_start,
                 int32_t   il_end) {
-    LLAMA_LOG_DEBUG("%s: il_start = %d, il_end = %d\n", __func__, il_start, il_end);
+//    LLAMA_LOG_DEBUG("%s: il_start = %d, il_end = %d\n", __func__, il_start, il_end);
 
     return cvec.apply(model, data, len, n_embd, il_start, il_end);
 }
@@ -924,7 +924,7 @@ int llama_context::decode(const llama_batch & batch_inp) {
     GGML_ASSERT((!batch_inp.token && batch_inp.embd) || (batch_inp.token && !batch_inp.embd)); // NOLINT
 
     if (!memory) {
-        LLAMA_LOG_DEBUG("%s: cannot decode batches with this context (calling encode() instead)\n", __func__);
+//        LLAMA_LOG_DEBUG("%s: cannot decode batches with this context (calling encode() instead)\n", __func__);
         return encode(batch_inp);
     }
 
@@ -2254,8 +2254,8 @@ llama_context * llama_init_from_model(
     }
 
     if (ggml_is_quantized(params.type_v) && !params.flash_attn) {
-        LLAMA_LOG_ERROR("%s: V cache quantization requires flash_attn\n", __func__);
-        return nullptr;
+        LLAMA_LOG_WARN("%s: V cache quantization requires flash_attn - reset V cache to f16\n", __func__);
+        params.type_v = GGML_TYPE_F16;
     }
 
     try {
