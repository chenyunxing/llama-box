diff --git a/src/llama-chat.cpp b/src/llama-chat.cpp
index 735d2619..0ffaf533 100644
--- a/src/llama-chat.cpp
+++ b/src/llama-chat.cpp
@@ -58,6 +58,9 @@ static const std::map<std::string, llm_chat_template> LLM_CHAT_TEMPLATES = {
     { "rwkv-world",        LLM_CHAT_TEMPLATE_RWKV_WORLD        },
     { "granite",           LLM_CHAT_TEMPLATE_GRANITE           },
     { "gigachat",          LLM_CHAT_TEMPLATE_GIGACHAT          },
+    { "falcon",            LLM_CHAT_TEMPLATE_FALCON            },
+    { "llava",             LLM_CHAT_TEMPLATE_LLAVA             },
+    { "llava-mistral",     LLM_CHAT_TEMPLATE_LLAVA_MISTRAL     },
     { "megrez",            LLM_CHAT_TEMPLATE_MEGREZ            },
     { "yandex",            LLM_CHAT_TEMPLATE_YANDEX            },
     { "bailing",           LLM_CHAT_TEMPLATE_BAILING           },
@@ -174,6 +177,8 @@ llm_chat_template llm_chat_detect_template(const std::string & tmpl) {
         return LLM_CHAT_TEMPLATE_GRANITE;
     } else if (tmpl_contains("message['role'] + additional_special_tokens[0] + message['content'] + additional_special_tokens[1]")) {
         return LLM_CHAT_TEMPLATE_GIGACHAT;
+    } else if ((tmpl_contains("Falcon:") && tmpl_contains("message['content']"))) {
+        return LLM_CHAT_TEMPLATE_FALCON;
     } else if (tmpl_contains("<|role_start|>")) {
         return LLM_CHAT_TEMPLATE_MEGREZ;
     } else if (tmpl_contains(" Ассистент:")) {
@@ -563,6 +568,61 @@ int32_t llm_chat_apply_template(
         if (add_ass) {
             ss << "assistant<|role_sep|>";
         }
+    } else if (tmpl == LLM_CHAT_TEMPLATE_FALCON) {
+        // Falcon
+        std::string system_prompt;
+        for (const auto &message : chat) {
+            std::string role(message->role);
+            if (role == "system") {
+                system_prompt = trim(message->content);
+            } else if (role == "user") {
+                if (!system_prompt.empty()) {
+                    ss << "\n" << system_prompt << "\n\n\n";
+                    system_prompt = "";
+                }
+                ss << "\nUser: " << trim(message->content) << "\n\n\n";
+            } else if (role == "assistant") {
+                ss << "\nAssistant: " << trim(message->content) << "\n\n\n";
+            }
+        }
+        if (add_ass) {
+            ss << "\nAssistant:";
+        }
+    } else if (tmpl == LLM_CHAT_TEMPLATE_LLAVA || tmpl == LLM_CHAT_TEMPLATE_LLAVA_MISTRAL) {
+        // llava 1.5
+        if (tmpl != LLM_CHAT_TEMPLATE_LLAVA_MISTRAL) {
+            ss << "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n";
+        }
+        for (auto message : chat) {
+            std::string role(message->role);
+            if (role == "user") {
+                std::string content(message->content);
+                if (tmpl != LLM_CHAT_TEMPLATE_LLAVA_MISTRAL) {
+                    ss << "USER:" << message->content << "\n";
+                } else {
+                    const std::string sign = "<image>\n";
+                    const size_t sign_pos = content.find(sign);
+                    if (sign_pos != std::string::npos) {
+                        content = content.replace(sign_pos, sign.size(), "");
+                        ss << "<image>\n";
+                    }
+                    ss << "USER:\n" << content.c_str() << "\n";
+                }
+            } else if (role == "assistant") {
+                if (tmpl != LLM_CHAT_TEMPLATE_LLAVA_MISTRAL) {
+                    ss << "ASSISTANT:" << message->content << "</s>\n";
+                } else {
+                    ss << "ASSISTANT:\n" << message->content << "</s>\n";
+                }
+            }
+        }
+        if (add_ass) {
+            if (tmpl != LLM_CHAT_TEMPLATE_LLAVA_MISTRAL) {
+                ss << "ASSISTANT:";
+            } else {
+                ss << "ASSISTANT:\n";
+            }
+        }
     }  else if (tmpl == LLM_CHAT_TEMPLATE_MEGREZ) {
         // Megrez template
         for (auto message : chat) {
diff --git a/src/llama-chat.h b/src/llama-chat.h
index 3f584346..bbe06aa3 100644
--- a/src/llama-chat.h
+++ b/src/llama-chat.h
@@ -37,6 +37,9 @@ enum llm_chat_template {
     LLM_CHAT_TEMPLATE_RWKV_WORLD,
     LLM_CHAT_TEMPLATE_GRANITE,
     LLM_CHAT_TEMPLATE_GIGACHAT,
+    LLM_CHAT_TEMPLATE_FALCON,
+    LLM_CHAT_TEMPLATE_LLAVA,
+    LLM_CHAT_TEMPLATE_LLAVA_MISTRAL,
     LLM_CHAT_TEMPLATE_MEGREZ,
     LLM_CHAT_TEMPLATE_YANDEX,
     LLM_CHAT_TEMPLATE_BAILING,
