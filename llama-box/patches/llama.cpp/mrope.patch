diff --git a/common/common.cpp b/common/common.cpp
index 6dea8e3d..6ca4ed0e 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -945,6 +945,11 @@ struct common_init_result common_init_from_params(common_params & params) {
 
     auto cparams = common_context_params_to_llama(params);
 
+    if (llama_model_needs_mrope(model)) {
+        LOG_INF("%s: model requires M-RoPE, increasing batch size by 4x\n", __func__);
+        cparams.n_batch *= 4;
+    }
+
     llama_context * lctx = llama_init_from_model(model, cparams);
     if (lctx == NULL) {
         LOG_ERR("%s: failed to create context with model '%s'\n", __func__, params.model.c_str());
diff --git a/ggml/src/ggml-cpu/ggml-cpu.c b/ggml/src/ggml-cpu/ggml-cpu.c
index e809f05d..4b4c013f 100644
--- a/ggml/src/ggml-cpu/ggml-cpu.c
+++ b/ggml/src/ggml-cpu/ggml-cpu.c
@@ -9322,10 +9322,6 @@ static void ggml_compute_forward_rope_f32(
     const bool is_mrope = mode & GGML_ROPE_TYPE_MROPE;  // ggml_rope_multi, multimodal rotary position embedding
     const bool is_vision = mode == GGML_ROPE_TYPE_VISION;
 
-    if (is_mrope) {
-        GGML_ASSERT(sections[0] > 0 || sections[1] > 0 || sections[2] > 0);
-    }
-
     if (is_vision) {
         GGML_ASSERT(n_dims == ne0/2);
     }
@@ -9508,10 +9504,6 @@ static void ggml_compute_forward_rope_f16(
     const bool is_mrope = mode & GGML_ROPE_TYPE_MROPE;
     const bool is_vision = mode == GGML_ROPE_TYPE_VISION;
 
-    if (is_mrope) {
-        GGML_ASSERT(sections[0] > 0 || sections[1] > 0 || sections[2] > 0);
-    }
-
     if (is_vision) {
         GGML_ASSERT(n_dims == ne0/2);
     }
diff --git a/ggml/src/ggml-cuda/rope.cu b/ggml/src/ggml-cuda/rope.cu
index 18f691b2..f5a4da3a 100644
--- a/ggml/src/ggml-cuda/rope.cu
+++ b/ggml/src/ggml-cuda/rope.cu
@@ -377,10 +377,6 @@ void ggml_cuda_op_rope_impl(ggml_backend_cuda_context & ctx, ggml_tensor * dst)
     const bool is_mrope = mode & GGML_ROPE_TYPE_MROPE;
     const bool is_vision = mode == GGML_ROPE_TYPE_VISION;
 
-    if (is_mrope) {
-        GGML_ASSERT(sections.v[0] > 0 || sections.v[1] > 0 || sections.v[2] > 0);
-    }
-
     if (is_vision) {
         GGML_ASSERT(n_dims == ne00/2);
     }
diff --git a/include/llama.h b/include/llama.h
index 3b75e760..fec09789 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -526,6 +526,9 @@ extern "C" {
     // to the decoder to start generating output sequence. For other models, it returns -1.
     LLAMA_API llama_token llama_model_decoder_start_token(const struct llama_model * model);
 
+    // Returns true if the model needs M-RoPE (like Qwen2VL, etc.)
+    LLAMA_API bool llama_model_needs_mrope(const struct llama_model * model);
+
     // Returns true if the model is recurrent (like Mamba, RWKV, etc.)
     LLAMA_API bool llama_model_is_recurrent(const struct llama_model * model);
 
diff --git a/src/llama-model.cpp b/src/llama-model.cpp
index 18bd0b07..3a6af700 100644
--- a/src/llama-model.cpp
+++ b/src/llama-model.cpp
@@ -3991,6 +3991,10 @@ llama_token llama_model_decoder_start_token(const struct llama_model * model) {
     return model->hparams.dec_start_token_id;
 }
 
+bool llama_model_needs_mrope(const struct llama_model * model) {
+    return std::any_of(model->hparams.rope_sections.begin(), model->hparams.rope_sections.end(), [](const int32_t & x) { return x != 0; });
+}
+
 bool llama_model_is_recurrent(const struct llama_model * model) {
     switch (model->arch) {
         case LLM_ARCH_MAMBA:  return true;
