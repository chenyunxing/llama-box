diff --git a/common/common.cpp b/common/common.cpp
index a6f9252b..e16beb15 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -902,6 +902,11 @@ struct common_init_result common_init_from_params(common_params & params) {
 
     auto cparams = common_context_params_to_llama(params);
 
+    if (llama_model_needs_mrope(model)) {
+        LOG_INF("%s: model requires M-RoPE, increasing batch size by 4x\n", __func__);
+        cparams.n_batch *= 4;
+    }
+
     llama_context * lctx = llama_init_from_model(model, cparams);
     if (lctx == NULL) {
         LOG_ERR("%s: failed to create context with model '%s'\n", __func__, params.model.c_str());
diff --git a/include/llama.h b/include/llama.h
index a184884c..2190383a 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -518,6 +518,9 @@ extern "C" {
     // to the decoder to start generating output sequence. For other models, it returns -1.
     LLAMA_API llama_token llama_model_decoder_start_token(const struct llama_model * model);
 
+    // Returns true if the model needs M-RoPE (like Qwen2VL, etc.)
+    LLAMA_API bool llama_model_needs_mrope(const struct llama_model * model);
+
     // Returns true if the model is recurrent (like Mamba, RWKV, etc.)
     LLAMA_API bool llama_model_is_recurrent(const struct llama_model * model);
 
diff --git a/src/llama-model.cpp b/src/llama-model.cpp
index f90f5e74..fe208282 100644
--- a/src/llama-model.cpp
+++ b/src/llama-model.cpp
@@ -3944,6 +3944,10 @@ llama_token llama_model_decoder_start_token(const struct llama_model * model) {
     return model->hparams.dec_start_token_id;
 }
 
+bool llama_model_needs_mrope(const struct llama_model * model) {
+    return std::any_of(model->hparams.rope_sections.begin(), model->hparams.rope_sections.end(), [](const int32_t & x) { return x != 0; });
+}
+
 bool llama_model_is_recurrent(const struct llama_model * model) {
     switch (model->arch) {
         case LLM_ARCH_MAMBA:  return true;
