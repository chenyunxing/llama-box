diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index e49225aa..79546807 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -2266,8 +2266,8 @@ llama_context * llama_init_from_model(
     }
 
     if (ggml_is_quantized(params.type_v) && !params.flash_attn) {
-        LLAMA_LOG_ERROR("%s: V cache quantization requires flash_attn\n", __func__);
-        return nullptr;
+        LLAMA_LOG_WARN("%s: V cache quantization requires flash_attn - reset V cache to f16\n", __func__);
+        params.type_v = GGML_TYPE_F16;
     }
 
     try {
