diff --git a/src/llama.cpp b/src/llama.cpp
index 192b20a2..cecb4f73 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -9557,8 +9557,8 @@ struct llama_context * llama_init_from_model(
     }
 
     if (ggml_is_quantized(params.type_v) && !params.flash_attn) {
-        LLAMA_LOG_ERROR("%s: V cache quantization requires flash_attn\n", __func__);
-        return nullptr;
+        LLAMA_LOG_WARN("%s: V cache quantization requires flash_attn - reset V cache to f16\n", __func__);
+        params.type_v = GGML_TYPE_F16;
     }
 
     llama_context * ctx = new llama_context(*model);
