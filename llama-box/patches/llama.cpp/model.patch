diff --git a/src/llama-arch.cpp b/src/llama-arch.cpp
index 814ac93a..d2d26e75 100644
--- a/src/llama-arch.cpp
+++ b/src/llama-arch.cpp
@@ -17,6 +17,7 @@ static const std::map<llm_arch, const char *> LLM_ARCH_NAMES = {
     { LLM_ARCH_BAICHUAN,         "baichuan"         },
     { LLM_ARCH_STARCODER,        "starcoder"        },
     { LLM_ARCH_REFACT,           "refact"           },
+    { LLM_ARCH_NEW,              "new"              },
     { LLM_ARCH_BERT,             "bert"             },
     { LLM_ARCH_NOMIC_BERT,       "nomic-bert"       },
     { LLM_ARCH_NOMIC_BERT_MOE,   "nomic-bert-moe"   },
@@ -479,6 +480,22 @@ static const std::map<llm_arch, std::map<llm_tensor, const char *>> LLM_TENSOR_N
             { LLM_TENSOR_FFN_UP,          "blk.%d.ffn_up" },
         },
     },
+    {
+         LLM_ARCH_NEW,
+         {
+            { LLM_TENSOR_TOKEN_EMBD,      "token_embd" },
+            { LLM_TENSOR_TOKEN_EMBD_NORM, "token_embd_norm" },
+            { LLM_TENSOR_TOKEN_TYPES,     "token_types" },
+            { LLM_TENSOR_ATTN_OUT_NORM,   "blk.%d.attn_output_norm" },
+            { LLM_TENSOR_ATTN_QKV,        "blk.%d.attn_qkv" },
+            { LLM_TENSOR_ATTN_OUT,        "blk.%d.attn_output" },
+            { LLM_TENSOR_LAYER_OUT_NORM,  "blk.%d.layer_output_norm" },
+            { LLM_TENSOR_FFN_DOWN,        "blk.%d.ffn_down" },
+            { LLM_TENSOR_FFN_UP,          "blk.%d.ffn_up" },
+            { LLM_TENSOR_CLS,             "cls" },
+            { LLM_TENSOR_CLS_OUT,         "cls.output" },
+         },
+    },
     {
         LLM_ARCH_BERT,
         {
diff --git a/src/llama-arch.h b/src/llama-arch.h
index d09b7d78..74696b0d 100644
--- a/src/llama-arch.h
+++ b/src/llama-arch.h
@@ -21,6 +21,7 @@ enum llm_arch {
     LLM_ARCH_MPT,
     LLM_ARCH_STARCODER,
     LLM_ARCH_REFACT,
+    LLM_ARCH_NEW,
     LLM_ARCH_BERT,
     LLM_ARCH_NOMIC_BERT,
     LLM_ARCH_NOMIC_BERT_MOE,
diff --git a/src/llama-model.cpp b/src/llama-model.cpp
index 2d90ec1a..d0fae4cd 100644
--- a/src/llama-model.cpp
+++ b/src/llama-model.cpp
@@ -753,6 +753,16 @@ void llama_model::load_hparams(llama_model_loader & ml) {
                     default: type = LLM_TYPE_UNKNOWN;
                 }
             } break;
+        case LLM_ARCH_NEW:
+            {
+                ml.get_key(LLM_KV_ATTENTION_LAYERNORM_EPS,    hparams.f_norm_eps);
+                ml.get_key(LLM_KV_ATTENTION_CAUSAL,           hparams.causal_attn);
+                ml.get_key(LLM_KV_POOLING_TYPE,               hparams.pooling_type, false);
+
+                if (hparams.n_layer == 12 && hparams.n_embd == 768) {
+                    type = LLM_TYPE_137M;
+                }
+            } break;
         case LLM_ARCH_NOMIC_BERT:
         case LLM_ARCH_NOMIC_BERT_MOE:
             {
@@ -2430,6 +2440,7 @@ bool llama_model::load_tensors(llama_model_loader & ml) {
                         layer.ffn_up_b = create_tensor(tn(LLM_TENSOR_FFN_UP, "bias", i),     {n_ff}, 0);
                     }
                 } break;
+            case LLM_ARCH_NEW:
             case LLM_ARCH_BERT:
             case LLM_ARCH_NOMIC_BERT:
             case LLM_ARCH_NOMIC_BERT_MOE:
@@ -2437,7 +2448,7 @@ bool llama_model::load_tensors(llama_model_loader & ml) {
                     tok_embd     = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD,  "weight"), {n_embd, n_vocab}, 0);
                     type_embd    = create_tensor(tn(LLM_TENSOR_TOKEN_TYPES, "weight"), {n_embd, n_token_types}, TENSOR_NOT_REQUIRED);
 
-                    if (arch == LLM_ARCH_BERT) {
+                    if (arch == LLM_ARCH_BERT || arch == LLM_ARCH_NEW) {
                         pos_embd = create_tensor(tn(LLM_TENSOR_POS_EMBD,    "weight"), {n_embd, n_ctx_train}, 0);
 
                         cls   = create_tensor(tn(LLM_TENSOR_CLS, "weight"), {n_embd, n_embd}, TENSOR_NOT_REQUIRED);
@@ -2478,12 +2489,16 @@ bool llama_model::load_tensors(llama_model_loader & ml) {
                             layer.ffn_down_exps = create_tensor(tn(LLM_TENSOR_FFN_DOWN_EXPS, "weight", i), {  n_ff,   n_embd, n_expert}, 0);
                             layer.ffn_gate_inp = create_tensor(tn(LLM_TENSOR_FFN_GATE_INP,   "weight", i), {n_embd, n_expert}, 0);
                         } else {
-                            layer.ffn_up   = create_tensor(tn(LLM_TENSOR_FFN_UP,        "weight", i), {n_embd, n_ff}, 0);
+                            if (arch == LLM_ARCH_NEW) {
+                                layer.ffn_up   = create_tensor(tn(LLM_TENSOR_FFN_UP,    "weight", i), {n_embd, 2*n_ff}, 0);
+                            } else {
+                                layer.ffn_up   = create_tensor(tn(LLM_TENSOR_FFN_UP,    "weight", i), {n_embd, n_ff}, 0);
+                            }
                             layer.ffn_down = create_tensor(tn(LLM_TENSOR_FFN_DOWN,      "weight", i), {n_ff, n_embd}, 0);
 
-                            if (arch == LLM_ARCH_BERT || arch == LLM_ARCH_NOMIC_BERT_MOE) {
+                            if (arch == LLM_ARCH_BERT || arch == LLM_ARCH_NEW  || arch == LLM_ARCH_NOMIC_BERT_MOE) {
                                 layer.bo         = create_tensor(tn(LLM_TENSOR_ATTN_OUT, "bias", i), {n_embd}, 0);
-                                layer.ffn_up_b   = create_tensor(tn(LLM_TENSOR_FFN_UP,   "bias", i), {n_ff}, 0);
+                                layer.ffn_up_b   = create_tensor(tn(LLM_TENSOR_FFN_UP,   "bias", i), {n_ff}, TENSOR_NOT_REQUIRED);
                                 layer.ffn_down_b = create_tensor(tn(LLM_TENSOR_FFN_DOWN, "bias", i), {n_embd}, 0);
                             } else {
                                 layer.ffn_gate = create_tensor(tn(LLM_TENSOR_FFN_GATE, "weight", i), {n_embd, n_ff}, 0);
@@ -6943,7 +6958,7 @@ struct llm_build_bert : public llm_graph_context {
             ggml_tensor * type_row0 = ggml_view_1d(ctx0, model.type_embd, n_embd, 0);
             inpL = ggml_add(ctx0, inpL, type_row0);
         }
-        if (model.arch == LLM_ARCH_BERT) {
+        if (model.pos_embd != nullptr) {
             inpL = ggml_add(ctx0, ggml_get_rows(ctx0, model.pos_embd, inp_pos), inpL);
         }
         cb(inpL, "inp_embd", -1);
@@ -7002,7 +7017,7 @@ struct llm_build_bert : public llm_graph_context {
                 Vcur = ggml_reshape_3d(ctx0, Vcur, n_embd_head, n_head_kv, n_tokens);
 
                 // RoPE
-                if (model.arch == LLM_ARCH_NOMIC_BERT || model.arch == LLM_ARCH_NOMIC_BERT_MOE) {
+                if (model.arch == LLM_ARCH_NOMIC_BERT || model.arch == LLM_ARCH_NEW || model.arch == LLM_ARCH_NOMIC_BERT_MOE) {
                     Qcur = ggml_rope_ext(
                             ctx0, Qcur, inp_pos, nullptr,
                             n_rot, rope_type, n_ctx_orig, freq_base, freq_scale,
@@ -7061,6 +7076,13 @@ struct llm_build_bert : public llm_graph_context {
                         0.0f,
                         LLAMA_EXPERT_GATING_FUNC_TYPE_SOFTMAX, il);
                 cb(cur, "ffn_moe_out", il);
+            } else if (model.arch == LLM_ARCH_NEW) {
+                cur = build_ffn(cur,
+                        model.layers[il].ffn_up,   model.layers[il].ffn_up_b,   NULL,
+                        NULL,                      NULL,                        NULL,
+                        model.layers[il].ffn_down, model.layers[il].ffn_down_b, NULL,
+                        NULL,
+                        LLM_FFN_SWIGLU, LLM_FFN_SEQ, il);
             } else if (model.arch == LLM_ARCH_BERT || model.arch == LLM_ARCH_NOMIC_BERT_MOE) {
                 cur = build_ffn(cur,
                         model.layers[il].ffn_up,   model.layers[il].ffn_up_b,   NULL,
@@ -16999,6 +17021,7 @@ llama_memory_i * llama_model::create_memory(const llama_memory_params & params,
     switch (arch) {
         // Models that need specific instantiation should be handled in the
         // switch statement
+        case LLM_ARCH_NEW:
         case LLM_ARCH_BERT:
         case LLM_ARCH_JINA_BERT_V2:
         case LLM_ARCH_NOMIC_BERT:
@@ -17595,6 +17618,7 @@ llama_rope_type llama_model_rope_type(const llama_model * model) {
         case LLM_ARCH_FALCON_H1:
         case LLM_ARCH_GROK:
         case LLM_ARCH_DBRX:
+        case LLM_ARCH_NEW:
         case LLM_ARCH_BERT:
         case LLM_ARCH_NOMIC_BERT:
         case LLM_ARCH_NOMIC_BERT_MOE:
