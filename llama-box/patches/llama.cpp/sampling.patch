diff --git a/common/sampling.cpp b/common/sampling.cpp
index 9c04d35fd..4a28bb026 100644
--- a/common/sampling.cpp
+++ b/common/sampling.cpp
@@ -232,7 +232,7 @@ struct common_sampler * common_sampler_init(const struct llama_model * model, co
         for (const auto & cnstr : params.samplers) {
             switch (cnstr) {
                 case COMMON_SAMPLER_TYPE_DRY:
-                    {
+                    if (params.dry_multiplier != 0.0f) {
                         std::vector<const char *> c_breakers;
                         c_breakers.reserve(params.dry_sequence_breakers.size());
                         for (const auto & str : params.dry_sequence_breakers) {
@@ -243,31 +243,31 @@ struct common_sampler * common_sampler_init(const struct llama_model * model, co
                     }
                     break;
                 case COMMON_SAMPLER_TYPE_TOP_K:
-                    llama_sampler_chain_add(result->chain, llama_sampler_init_top_k       (params.top_k));
+                    params.top_k > 0.0f ? llama_sampler_chain_add(result->chain, llama_sampler_init_top_k       (params.top_k)) : void();
                     break;
                 case COMMON_SAMPLER_TYPE_TOP_P:
-                    llama_sampler_chain_add(result->chain, llama_sampler_init_top_p       (params.top_p, params.min_keep));
+                    params.top_p != 1.0f ? llama_sampler_chain_add(result->chain, llama_sampler_init_top_p       (params.top_p, params.min_keep)) : void();
                     break;
                 case COMMON_SAMPLER_TYPE_TOP_N_SIGMA:
-                    llama_sampler_chain_add(result->chain, llama_sampler_init_top_n_sigma (params.top_n_sigma));
+                    params.top_n_sigma != -1.0f ? llama_sampler_chain_add(result->chain, llama_sampler_init_top_n_sigma (params.top_n_sigma)) : void();
                     break;
                 case COMMON_SAMPLER_TYPE_MIN_P:
-                    llama_sampler_chain_add(result->chain, llama_sampler_init_min_p       (params.min_p, params.min_keep));
+                    params.min_p != 0.0f ? llama_sampler_chain_add(result->chain, llama_sampler_init_min_p       (params.min_p, params.min_keep)) : void();
                     break;
                 case COMMON_SAMPLER_TYPE_XTC:
-                    llama_sampler_chain_add(result->chain, llama_sampler_init_xtc         (params.xtc_probability, params.xtc_threshold, params.min_keep, params.seed));
+                    params.xtc_probability != 0.0f && params.xtc_threshold <= 0.5 ? llama_sampler_chain_add(result->chain, llama_sampler_init_xtc         (params.xtc_probability, params.xtc_threshold, params.min_keep, params.seed)) : void();
                     break;
                 case COMMON_SAMPLER_TYPE_TYPICAL_P:
-                    llama_sampler_chain_add(result->chain, llama_sampler_init_typical     (params.typ_p, params.min_keep));
+                    params.typ_p != 1.0f ? llama_sampler_chain_add(result->chain, llama_sampler_init_typical     (params.typ_p, params.min_keep)) : void();
                     break;
                 case COMMON_SAMPLER_TYPE_TEMPERATURE:
-                    llama_sampler_chain_add(result->chain, llama_sampler_init_temp_ext    (params.temp, params.dynatemp_range, params.dynatemp_exponent));
+                    params.temp != 1.0f ? llama_sampler_chain_add(result->chain, llama_sampler_init_temp_ext    (params.temp, params.dynatemp_range, params.dynatemp_exponent)) : void();
                     break;
                 case COMMON_SAMPLER_TYPE_INFILL:
                     llama_sampler_chain_add(result->chain, llama_sampler_init_infill      (vocab));
                     break;
                 case COMMON_SAMPLER_TYPE_PENALTIES:
-                    llama_sampler_chain_add(result->chain, llama_sampler_init_penalties   (params.penalty_last_n, params.penalty_repeat, params.penalty_freq, params.penalty_present));
+                    params.penalty_last_n != 0.0f && params.penalty_repeat != 1.0f ? llama_sampler_chain_add(result->chain, llama_sampler_init_penalties   (params.penalty_last_n, params.penalty_repeat, params.penalty_freq, params.penalty_present)) : void();
                     break;
                 default:
                     GGML_ASSERT(false && "unknown sampler type");
@@ -335,6 +335,31 @@ void common_perf_print(const struct llama_context * ctx, const struct common_sam
     }
 }
 
+llama_token common_sampler_sample2(struct common_sampler *gsmpl, struct llama_context * ctx, int idx) {
+    gsmpl->set_logits(ctx, idx);
+
+    auto & grmr  = gsmpl->grmr;
+    auto & chain = gsmpl->chain;
+    auto & cur_p = gsmpl->cur_p;  // initialized by set_logits
+
+    if (!llama_sampler_grammar_is_inflight(grmr)) {
+        llama_sampler_apply(grmr, &cur_p);
+    }
+    llama_sampler_apply(chain, &cur_p);
+
+    if (cur_p.selected == -1) {
+        const llama_model * model = llama_get_model(ctx);
+        const llama_vocab * vocab = llama_model_get_vocab(model);
+        cur_p.selected            = 0;
+        cur_p.data[0].id          = llama_vocab_eos(vocab);
+        cur_p.data[0].logit       = +INFINITY;
+        cur_p.data[0].p           = 1.0f;
+    }
+
+    const auto & selected = cur_p.data[cur_p.selected];
+    return selected.id;
+}
+
 llama_token common_sampler_sample(struct common_sampler * gsmpl, struct llama_context * ctx, int idx, bool grammar_first) {
     gsmpl->set_logits(ctx, idx);
 
diff --git a/common/sampling.h b/common/sampling.h
index 2064421db..8e9006635 100644
--- a/common/sampling.h
+++ b/common/sampling.h
@@ -48,6 +48,8 @@ struct common_sampler * common_sampler_clone (struct common_sampler * gsmpl);
 // arguments can be nullptr to skip printing
 void common_perf_print(const struct llama_context * ctx, const struct common_sampler * gsmpl);
 
+llama_token common_sampler_sample2(struct common_sampler * gsmpl, struct llama_context * ctx, int idx);
+
 // extended sampling implementation:
 //
 // - set logits
diff --git a/include/llama.h b/include/llama.h
index 2cbe18d8c..9fde969fe 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -1268,6 +1268,8 @@ extern "C" {
                                float   tau,
                                float   eta);
 
+    LLAMA_API bool llama_sampler_grammar_is_inflight(const struct llama_sampler * smpl);
+
     /// @details Intializes a GBNF grammar, see grammars/README.md for details.
     /// @param vocab The vocabulary that this grammar will be used with.
     /// @param grammar_str The production rules for the grammar, encoded as a string. Returns an empty grammar if empty. Returns NULL if parsing of grammar_str fails.
diff --git a/src/llama-sampling.cpp b/src/llama-sampling.cpp
index bfbf5fa23..abec33bf2 100644
--- a/src/llama-sampling.cpp
+++ b/src/llama-sampling.cpp
@@ -1569,6 +1569,11 @@ static struct llama_sampler * llama_sampler_init_grammar_impl(
     );
 }
 
+bool llama_sampler_grammar_is_inflight(const struct llama_sampler * smpl) {
+    const auto * ctx = (const llama_sampler_grammar *) smpl->ctx;
+    return ctx->grammar == nullptr;
+}
+
 struct llama_sampler * llama_sampler_init_grammar(
         const struct llama_vocab * vocab,
                       const char * grammar_str,
