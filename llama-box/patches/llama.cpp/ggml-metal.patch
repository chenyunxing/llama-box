diff --git a/ggml/include/ggml-metal.h b/ggml/include/ggml-metal.h
index a6106944..40ecffea 100644
--- a/ggml/include/ggml-metal.h
+++ b/ggml/include/ggml-metal.h
@@ -59,6 +59,8 @@ GGML_BACKEND_API bool ggml_backend_metal_supports_family(ggml_backend_t backend,
 // capture all command buffers committed the next time `ggml_backend_graph_compute` is called
 GGML_BACKEND_API void ggml_backend_metal_capture_next_compute(ggml_backend_t backend);
 
+GGML_BACKEND_API void ggml_backend_metal_get_device_memory(ggml_backend_t backend, size_t * free_mem, size_t * total_mem);
+
 GGML_BACKEND_API ggml_backend_reg_t ggml_backend_metal_reg(void);
 
 #ifdef __cplusplus
diff --git a/ggml/src/ggml-metal/ggml-metal.m b/ggml/src/ggml-metal/ggml-metal.m
index f78e7eee..64e618b9 100644
--- a/ggml/src/ggml-metal/ggml-metal.m
+++ b/ggml/src/ggml-metal/ggml-metal.m
@@ -1539,6 +1539,7 @@ static bool ggml_backend_metal_buffer_rset_init(
 
         [ctx->rset commit];
         [ctx->rset requestResidency];
+        GGML_LOG_INFO("%s: residency set %p, n_buffers = %d, all_size = %zu\n", __func__, ctx->rset, ctx->n_buffers, ctx->all_size);
 
         return true;
     }
@@ -1555,8 +1556,10 @@ static void ggml_backend_metal_buffer_rset_free(struct ggml_backend_metal_buffer
 #if defined(GGML_METAL_HAS_RESIDENCY_SETS)
     if (@available(macOS 15.0, iOS 18.0, tvOS 18.0, visionOS 2.0, *)) {
         if (ctx->rset) {
+            GGML_LOG_INFO("%s: residency set %p, n_buffers = %d, all_size = %zu\n", __func__, ctx->rset, ctx->n_buffers, ctx->all_size);
             [ctx->rset endResidency];
             [ctx->rset removeAllAllocations];
+            [ctx->rset commit];
             [ctx->rset release];
         }
     }
@@ -1813,11 +1816,6 @@ static bool ggml_metal_encode_node(
             } break;
     }
 
-    if (!ggml_metal_supports_op(ctx_dev, dst)) {
-        GGML_LOG_ERROR("%s: error: unsupported op '%s'\n", __func__, ggml_op_desc(dst));
-        GGML_ABORT("unsupported op");
-    }
-
     ggml_metal_mem_pool_clear(mem_pool);
 
     const int64_t  ne00 = src0 ? src0->ne[0] : 0;
@@ -2800,7 +2798,21 @@ static bool ggml_metal_encode_node(
 
                 // find the break-even point where the matrix-matrix kernel becomes more efficient compared
                 // to the matrix-vector kernel
-                const int ne11_mm_min = 4;
+                int ne11_mm_min = 4;
+                switch (src0t) {
+                    case GGML_TYPE_F16:  ne11_mm_min = 4;  break;
+                    case GGML_TYPE_Q8_0: ne11_mm_min = 7;  break;
+                    case GGML_TYPE_Q2_K: ne11_mm_min = 15; break;
+                    case GGML_TYPE_Q3_K: ne11_mm_min = 7;  break;
+                    case GGML_TYPE_Q4_0:
+                    case GGML_TYPE_Q4_1: ne11_mm_min = 15; break;
+                    case GGML_TYPE_Q4_K: ne11_mm_min = 11; break;
+                    case GGML_TYPE_Q5_0:                          // not tested yet
+                    case GGML_TYPE_Q5_1: ne11_mm_min = 13; break; // not tested yet
+                    case GGML_TYPE_Q5_K: ne11_mm_min = 7;  break;
+                    case GGML_TYPE_Q6_K: ne11_mm_min = 7;  break;
+                    default:             ne11_mm_min = 4;  break;
+                }
 
                 // first try to use small-batch mat-mv kernels
                 // these should be efficient for BS [2, ~8]
@@ -2809,20 +2821,8 @@ static bool ggml_metal_encode_node(
                      (
                       (
                        src0t == GGML_TYPE_F16  || // TODO: helper function
-                       src0t == GGML_TYPE_Q4_0 ||
-                       src0t == GGML_TYPE_Q4_1 ||
-                       src0t == GGML_TYPE_Q5_0 ||
-                       src0t == GGML_TYPE_Q5_1 ||
-                       src0t == GGML_TYPE_Q8_0 ||
                        src0t == GGML_TYPE_IQ4_NL ||
-                       false) && (ne11 >= 2 && ne11 <= 8)
-                     ) ||
-                     (
-                      (
-                       src0t == GGML_TYPE_Q4_K ||
-                       src0t == GGML_TYPE_Q5_K ||
-                       src0t == GGML_TYPE_Q6_K ||
-                       false) && (ne11 >= 4 && ne11 <= 8)
+                       false) && (ne11 >= 2 && ne11 <= 5)
                      )
                     )
                    ) {
@@ -2834,7 +2834,7 @@ static bool ggml_metal_encode_node(
                     //       my current hypothesis is that the work grid is not evenly divisible for different nsg
                     //       values and there can be some tail effects when nsg is high. need to confirm this
                     //
-                    const int nsg    = 2;                 // num simdgroups per threadgroup
+                    const int nsg    = ne11 < 3 ?  2 : 4; // num simdgroups per threadgroup
                     const int nxpsg  = ne11 < 3 ? 16 : 8; // num threads along row per simdgroup
                     const int nypsg  = 32/nxpsg;          // num threads along col per simdgroup (i.e. a simdgroup processes that many src0 rows at a time)
                     const int r0ptg  = nypsg*nsg;         // num src0 rows per threadgroup
@@ -3719,7 +3719,7 @@ static bool ggml_metal_encode_node(
             } break;
         case GGML_OP_RMS_NORM:
             {
-                GGML_ASSERT(ne00 % 4 == 0);
+                // GGML_ASSERT(ne00 % 4 == 0);
                 GGML_ASSERT(ggml_is_contiguous_1(src0));
 
                 float eps;
@@ -5976,6 +5976,10 @@ static ggml_backend_dev_t ggml_backend_metal_reg_device_get(ggml_backend_reg_t r
     /* .get_proc_address = */ ggml_backend_metal_get_proc_address,
 };
 
+void ggml_backend_metal_get_device_memory(ggml_backend_t backend, size_t *free, size_t *total) {
+    ggml_backend_metal_device_get_memory(backend->device, free, total);
+}
+
 ggml_backend_reg_t ggml_backend_metal_reg(void) {
     // TODO: make this thread-safe somehow?
     {
