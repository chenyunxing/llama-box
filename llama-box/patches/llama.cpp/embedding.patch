diff --git a/include/llama.h b/include/llama.h
index 2cbe18d8c..9338aac64 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -452,6 +452,7 @@ extern "C" {
     LLAMA_API bool llama_supports_gpu_offload(void);
     LLAMA_API bool llama_supports_rpc        (void);
 
+    LLAMA_API bool llama_causal_attn    (const struct llama_context * ctx);
     LLAMA_API uint32_t llama_n_ctx      (const struct llama_context * ctx);
     LLAMA_API uint32_t llama_n_batch    (const struct llama_context * ctx);
     LLAMA_API uint32_t llama_n_ubatch   (const struct llama_context * ctx);
diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index 958bcc047..6aa9bd390 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -122,6 +122,20 @@ llama_context::llama_context(
         }
     }
 
+    if (!cparams.causal_attn) {
+        bool is_rope_customized = rope_scaling_type != hparams.rope_scaling_type_train ||
+                                  cparams.rope_freq_base != hparams.rope_freq_base_train ||
+                                  cparams.rope_freq_scale != hparams.rope_freq_scale_train ||
+                                  (rope_scaling_type == LLAMA_ROPE_SCALING_TYPE_YARN && params.yarn_orig_ctx != 0 && cparams.n_ctx_orig_yarn != hparams.n_ctx_orig_yarn);
+        if (!is_rope_customized) {
+            LLAMA_LOG_WARN("%s: adjust n_ctx of the none causal attention model to the minimum value between given n_ctx(%d) and n_ctx_train(%d), you might change RoPE to extend this limitation\n", __func__, cparams.n_ctx, hparams.n_ctx_train);
+            cparams.n_ctx        = std::min(cparams.n_ctx, hparams.n_ctx_train);
+        }
+        LLAMA_LOG_WARN("%s: adjust n_batch(%d) and n_ubatch(%d) of the none causal attention model to n_ctx(%d), avoid calculation errors\n", __func__, cparams.n_batch, cparams.n_ubatch, cparams.n_ctx);
+        cparams.n_batch      = cparams.n_ctx;
+        cparams.n_ubatch     = cparams.n_ctx;
+    }
+
     const uint32_t n_ctx_per_seq = cparams.n_ctx / cparams.n_seq_max;
 
     LLAMA_LOG_INFO("%s: n_seq_max     = %u\n",   __func__, cparams.n_seq_max);
@@ -412,6 +426,10 @@ ggml_backend_sched_t llama_context::get_sched() const {
     return sched.get();
 }
 
+bool llama_context::causal_attn() const {
+    return cparams.causal_attn;
+}
+
 uint32_t llama_context::n_ctx() const {
     return cparams.n_ctx;
 }
@@ -1111,7 +1129,7 @@ int llama_context::decode(const llama_batch & batch_inp) {
         //    ggml_graph_dump_dot(gf, NULL, "llama.dot");
         //}
 
-        auto * t_logits = res->get_logits();
+        auto * t_logits = cparams.causal_attn ? res->get_logits() : nullptr;
         auto * t_embd   = cparams.embeddings ? res->get_embd() : nullptr;
 
         if (t_embd && res->get_embd_pooled()) {
@@ -1267,7 +1285,7 @@ uint32_t llama_context::output_reserve(int32_t n_outputs) {
     const auto n_vocab = vocab.n_tokens();
     const auto n_embd  = hparams.n_embd;
 
-    bool has_logits = true;
+    bool has_logits = cparams.causal_attn;
     bool has_embd   = cparams.embeddings;
 
     // TODO: hacky enc-dec support
@@ -2323,6 +2341,10 @@ void llama_free(llama_context * ctx) {
     delete ctx;
 }
 
+bool llama_causal_attn(const llama_context * ctx) {
+    return ctx->causal_attn();
+}
+
 uint32_t llama_n_ctx(const llama_context * ctx) {
     return ctx->n_ctx();
 }
diff --git a/src/llama-context.h b/src/llama-context.h
index 25c143d56..f6d1f43c2 100644
--- a/src/llama-context.h
+++ b/src/llama-context.h
@@ -35,6 +35,7 @@ struct llama_context {
 
     ggml_backend_sched_t get_sched() const;
 
+    bool     causal_attn()   const;
     uint32_t n_ctx()         const;
     uint32_t n_ctx_per_seq() const;
     uint32_t n_batch()       const;
