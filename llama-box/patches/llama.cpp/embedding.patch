diff --git a/include/llama.h b/include/llama.h
index 06c56395..1f023acb 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -471,6 +471,7 @@ extern "C" {
     LLAMA_API bool llama_supports_gpu_offload(void);
     LLAMA_API bool llama_supports_rpc        (void);
 
+    LLAMA_API bool llama_causal_attn    (const struct llama_context * ctx);
     LLAMA_API uint32_t llama_n_ctx      (const struct llama_context * ctx);
     LLAMA_API uint32_t llama_n_batch    (const struct llama_context * ctx);
     LLAMA_API uint32_t llama_n_ubatch   (const struct llama_context * ctx);
diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index 45591be9..f1d7a21d 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -94,6 +94,20 @@ llama_context::llama_context(
 
     cparams.n_ubatch = std::min(cparams.n_batch, params.n_ubatch == 0 ? params.n_batch : params.n_ubatch);
 
+    if (!cparams.causal_attn) {
+        bool is_rope_customized = rope_scaling_type != hparams.rope_scaling_type_train ||
+                                  cparams.rope_freq_base != hparams.rope_freq_base_train ||
+                                  cparams.rope_freq_scale != hparams.rope_freq_scale_train ||
+                                  (rope_scaling_type == LLAMA_ROPE_SCALING_TYPE_YARN && params.yarn_orig_ctx != 0 && cparams.n_ctx_orig_yarn != hparams.n_ctx_orig_yarn);
+        if (!is_rope_customized) {
+            LLAMA_LOG_WARN("%s: adjust n_ctx of the none causal attention model to the minimum value between given n_ctx(%d) and n_ctx_train(%d), you might change RoPE to extend this limitation\n", __func__, cparams.n_ctx, hparams.n_ctx_train);
+            cparams.n_ctx        = std::min(cparams.n_ctx, hparams.n_ctx_train);
+        }
+        LLAMA_LOG_WARN("%s: adjust n_batch(%d) and n_ubatch(%d) of the none causal attention model to n_ctx(%d), avoid calculation errors\n", __func__, cparams.n_batch, cparams.n_ubatch, cparams.n_ctx);
+        cparams.n_batch      = cparams.n_ctx;
+        cparams.n_ubatch     = cparams.n_ctx;
+    }
+
     const uint32_t n_ctx_per_seq = cparams.n_ctx / cparams.n_seq_max;
 
     LLAMA_LOG_INFO("%s: n_seq_max     = %u\n",   __func__, cparams.n_seq_max);
@@ -408,6 +422,10 @@ ggml_context * llama_context::get_ctx_compute() const {
     return ctx_compute.get();
 }
 
+bool llama_context::causal_attn() const {
+    return cparams.causal_attn;
+}
+
 uint32_t llama_context::n_ctx() const {
     return cparams.n_ctx;
 }
@@ -1914,6 +1932,10 @@ void llama_free(llama_context * ctx) {
     delete ctx;
 }
 
+bool llama_causal_attn(const struct llama_context * ctx) {
+    return ctx->causal_attn();
+}
+
 uint32_t llama_n_ctx(const llama_context * ctx) {
     return ctx->n_ctx();
 }
diff --git a/src/llama-context.h b/src/llama-context.h
index cf41ac57..cf649196 100644
--- a/src/llama-context.h
+++ b/src/llama-context.h
@@ -34,6 +34,7 @@ struct llama_context {
 
     ggml_context * get_ctx_compute() const;
 
+    bool     causal_attn()   const;
     uint32_t n_ctx()         const;
     uint32_t n_ctx_per_seq() const;
     uint32_t n_batch()       const;
