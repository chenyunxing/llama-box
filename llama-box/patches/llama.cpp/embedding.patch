diff --git a/include/llama.h b/include/llama.h
index ee6e7391..42e53987 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -455,6 +455,7 @@ extern "C" {
     LLAMA_API bool llama_supports_mlock      (void);
     LLAMA_API bool llama_supports_gpu_offload(void);
     LLAMA_API bool llama_supports_rpc        (void);
+    LLAMA_API bool llama_supports_embedding_only (const struct llama_context * ctx);
 
     LLAMA_API uint32_t llama_n_ctx      (const struct llama_context * ctx);
     LLAMA_API uint32_t llama_n_batch    (const struct llama_context * ctx);
diff --git a/src/llama.cpp b/src/llama.cpp
index 607f2786..95e5b44d 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -9395,6 +9395,10 @@ bool llama_supports_rpc(void) {
     return ggml_backend_reg_by_name("RPC") != nullptr;
 }
 
+bool llama_supports_embedding_only(const struct llama_context * ctx) {
+    return !ctx->cparams.causal_attn;
+}
+
 void llama_backend_init(void) {
     ggml_time_init();
 
@@ -9653,6 +9657,20 @@ struct llama_context * llama_init_from_model(
         cparams.causal_attn = params.attention_type == LLAMA_ATTENTION_TYPE_CAUSAL;
     }
 
+    if (!cparams.causal_attn) {
+        bool is_rope_customized = rope_scaling_type != hparams.rope_scaling_type_train ||
+            cparams.rope_freq_base != hparams.rope_freq_base_train ||
+            cparams.rope_freq_scale != hparams.rope_freq_scale_train ||
+           (rope_scaling_type == LLAMA_ROPE_SCALING_TYPE_YARN && params.yarn_orig_ctx != 0 && cparams.n_ctx_orig_yarn != hparams.n_ctx_orig_yarn);
+        if (!is_rope_customized) {
+            LLAMA_LOG_WARN("%s: adjust n_ctx of the none causal attention model to the minimum value between given n_ctx(%d) and n_ctx_train(%d), you might change RoPE to extend this limitation\n", __func__, cparams.n_ctx, hparams.n_ctx_train);
+            cparams.n_ctx        = std::min(cparams.n_ctx, hparams.n_ctx_train);
+        }
+        LLAMA_LOG_WARN("%s: adjust n_batch(%d) and n_ubatch(%d) of the none causal attention model to n_ctx(%d), avoid calculation errors\n", __func__, cparams.n_batch, cparams.n_ubatch, cparams.n_ctx);
+        cparams.n_batch      = cparams.n_ctx;
+        cparams.n_ubatch     = cparams.n_ctx;
+    }
+
     const uint32_t n_ctx_per_seq = cparams.n_ctx / cparams.n_seq_max;
 
     LLAMA_LOG_INFO("%s: n_seq_max     = %u\n",   __func__, cparams.n_seq_max);
