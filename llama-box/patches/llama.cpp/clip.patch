diff --git a/examples/llava/clip-impl.h b/examples/llava/clip-impl.h
index 53ac3813..367a3bb7 100644
--- a/examples/llava/clip-impl.h
+++ b/examples/llava/clip-impl.h
@@ -189,17 +189,7 @@ static void clip_log_internal(enum ggml_log_level level, const char * format, ..
     va_end(args);
 }
 
-#define LOG_TMPL(level, ...) \
-    do { \
-        if ((level) >= g_logger_state.verbosity_thold) { \
-            clip_log_internal((level), __VA_ARGS__); \
-        } \
-    } while (0)
-#define LOG_INF(...) LOG_TMPL(GGML_LOG_LEVEL_INFO,  __VA_ARGS__)
-#define LOG_WRN(...) LOG_TMPL(GGML_LOG_LEVEL_WARN,  __VA_ARGS__)
-#define LOG_ERR(...) LOG_TMPL(GGML_LOG_LEVEL_ERROR, __VA_ARGS__)
-#define LOG_DBG(...) LOG_TMPL(GGML_LOG_LEVEL_DEBUG, __VA_ARGS__)
-#define LOG_CNT(...) LOG_TMPL(GGML_LOG_LEVEL_CONT,  __VA_ARGS__)
+#include "common/log.h"
 
 //
 // cpp wrappers
diff --git a/examples/llava/clip.cpp b/examples/llava/clip.cpp
index da8a590f..23997a39 100644
--- a/examples/llava/clip.cpp
+++ b/examples/llava/clip.cpp
@@ -342,11 +342,14 @@ struct clip_ctx {
 
     clip_image_size load_image_size;
 
+    int32_t max_image_size = 0;
+
     clip_ctx(clip_context_params & ctx_params) {
         backend_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
         backend     = ctx_params.use_gpu
                         ? ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_GPU, nullptr)
                         : nullptr;
+        max_image_size = ctx_params.max_image_size;
 
         if (backend) {
             LOG_INF("%s: CLIP using %s backend\n", __func__, ggml_backend_name(backend));
@@ -1445,6 +1448,18 @@ struct clip_model_loader {
             get_bool(KEY_HAS_QWEN2VL_MERGER, ctx_clip.has_qwen2vl_merger, false);
             // !!! do NOT extend the list above, use KEY_PROJ_TYPE instead
 
+#if (!defined GGML_USE_CUDA) && (!defined GGML_USE_METAL)
+            if (ctx_clip.has_qwen2vl_merger && !ggml_backend_is_cpu(ctx_clip.backend)) {
+                LOG_WRN("%s: Qwen2VL merger is not supported on current backend, fallback to CPU backend\n", __func__);
+                ggml_backend_free(ctx_clip.backend);
+                ctx_clip.backend = ctx_clip.backend_cpu;
+                ctx_clip.backend_ptrs.erase(ctx_clip.backend_ptrs.begin());
+                ctx_clip.backend_buft.erase(ctx_clip.backend_buft.begin());
+                ctx_clip.sched.reset(
+                    ggml_backend_sched_new(ctx_clip.backend_ptrs.data(), ctx_clip.backend_buft.data(), ctx_clip.backend_ptrs.size(), 8192, false)
+                );
+            }
+#endif
             get_bool(KEY_USE_GELU, ctx_clip.use_gelu, false);
             get_bool(KEY_USE_SILU, ctx_clip.use_silu, false);
 
@@ -1773,14 +1788,28 @@ struct clip_model_loader {
 
         // create a fake batch
         clip_image_f32_batch batch;
-        clip_image_f32_ptr img(clip_image_f32_init());
         clip_image_size image_size;
-        image_size.width  = ctx_clip.vision_model.hparams.image_size;
-        image_size.height = ctx_clip.vision_model.hparams.image_size;
-        img->nx = image_size.width;
-        img->ny = image_size.height;
-        img->buf.resize(image_size.width * image_size.height * 3);
-        batch.entries.push_back(std::move(img));
+        if (ctx_clip.max_image_size > 0) {
+            clip_image_u8 * img = new clip_image_u8();
+            img->nx = ctx_clip.max_image_size;
+            img->ny = ctx_clip.max_image_size;
+            img->buf.resize(ctx_clip.max_image_size * ctx_clip.max_image_size * 3);
+            bool processed = clip_image_preprocess(&ctx_clip, img, &batch);
+            clip_image_u8_free(img);
+            if (!processed) {
+                throw std::runtime_error("unable to preprocess image");
+            }
+            image_size.width = batch.entries[0]->nx;
+            image_size.height = batch.entries[0]->ny;
+        } else {
+            clip_image_f32_ptr img(clip_image_f32_init());
+            image_size.width  = ctx_clip.vision_model.hparams.image_size;
+            image_size.height = ctx_clip.vision_model.hparams.image_size;
+            img->nx = image_size.width;
+            img->ny = image_size.height;
+            img->buf.resize(image_size.width * image_size.height * 3);
+            batch.entries.push_back(std::move(img));
+        }
 
         ggml_cgraph * gf = clip_image_build_graph(&ctx_clip, batch, image_size, false);
         ggml_backend_sched_reserve(ctx_clip.sched.get(), gf);
@@ -2323,7 +2352,7 @@ struct llava_uhd {
 
         // resize to overview size
         clip_image_u8_ptr resized_img(clip_image_u8_init());
-        image_manipulation::bicubic_resize(*img, *resized_img, inst.overview_size.width, inst.overview_size.height);
+        image_manipulation::resize_and_pad_image(*img, *resized_img, {inst.overview_size.width, inst.overview_size.height});
         output.push_back(std::move(resized_img));
         if (inst.slices.empty()) {
             // no slices, just return the resized image
@@ -3214,6 +3243,14 @@ bool clip_is_gemma3(const struct clip_ctx * ctx) {
     return ctx->proj_type == PROJECTOR_TYPE_GEMMA3;
 }
 
+bool clip_is_smolvlm(const struct clip_ctx * ctx) {
+    return ctx->proj_type == PROJECTOR_TYPE_IDEFICS3;
+}
+
+bool clip_is_pixtral(const struct clip_ctx * ctx) {
+    return ctx->proj_type == PROJECTOR_TYPE_PIXTRAL;
+}
+
 // Determine the number of encoder layers to iterate over
 int get_deepest_feature_layer(const struct clip_ctx * ctx) {
     // Get the index of the second to last layer; this is the
diff --git a/examples/llava/clip.h b/examples/llava/clip.h
index 5fc45d3e..be1ae9c8 100644
--- a/examples/llava/clip.h
+++ b/examples/llava/clip.h
@@ -37,6 +37,7 @@ struct clip_image_f32_batch;
 struct clip_context_params {
     bool use_gpu;
     enum ggml_log_level verbosity;
+    int32_t max_image_size = 0;
 };
 
 // deprecated, use clip_init
@@ -113,6 +114,8 @@ CLIP_API bool clip_is_glm(const struct clip_ctx * ctx);
 CLIP_API bool clip_is_qwen2vl(const struct clip_ctx * ctx);
 CLIP_API bool clip_is_llava(const struct clip_ctx * ctx);
 CLIP_API bool clip_is_gemma3(const struct clip_ctx * ctx);
+CLIP_API bool clip_is_smolvlm(const struct clip_ctx * ctx);
+CLIP_API bool clip_is_pixtral(const struct clip_ctx * ctx);
 
 CLIP_API int get_deepest_feature_layer(const struct clip_ctx * ctx);
 
diff --git a/examples/llava/llava.cpp b/examples/llava/llava.cpp
index 03a22cbb..a4f48e61 100644
--- a/examples/llava/llava.cpp
+++ b/examples/llava/llava.cpp
@@ -12,17 +12,7 @@
 #include <vector>
 #include <memory>
 
-#if defined(LLAVA_LOG_OFF)
-#   define LOG_INF(...)
-#   define LOG_WRN(...)
-#   define LOG_ERR(...)
-#   define LOG_DBG(...)
-#else // defined(LLAVA_LOG_OFF)
-#   define LOG_INF(...) do { fprintf(stdout, __VA_ARGS__); } while (0)
-#   define LOG_WRN(...) do { fprintf(stderr, __VA_ARGS__); } while (0)
-#   define LOG_ERR(...) do { fprintf(stderr, __VA_ARGS__); } while (0)
-#   define LOG_DBG(...) do { fprintf(stdout, __VA_ARGS__); } while (0)
-#endif // defined(LLAVA_LOG_OFF)
+#include "common/log.h"
 
 // RGB uint8 image
 struct clip_image_u8 {
@@ -264,8 +254,6 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
         return false;
     }
 
-    const int64_t t_img_enc_start_us = ggml_time_us();
-
     const char * mm_patch_merge_type = clip_patch_merge_type(ctx_clip);
 
     const size_t n_imgs = clip_image_f32_batch_n_images(img_res_v.get());
@@ -295,14 +283,11 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
             }
 
             if (!encoded) {
-                LOG_ERR("Unable to encode image - spatial_unpad - subimage %d of %d\n", (int) i+1, (int) n_imgs);
                 return false;
             }
             const int64_t t_img_enc_steop_batch_us = ggml_time_us();
             LOG_INF("%s: step %d of %d encoded in %8.2f ms\n", __func__, (int)i+1, (int)n_imgs, (t_img_enc_steop_batch_us - t_img_enc_step_start_us) / 1000.0);
         }
-        const int64_t t_img_enc_batch_us = ggml_time_us();
-        LOG_INF("%s: all %d segments encoded in %8.2f ms\n", __func__, (int)n_imgs, (t_img_enc_batch_us - t_img_enc_start_us) / 1000.0);
 
         int n_img_pos_out = 0;
         for (size_t i = 0; i < image_embd_v.size(); i++) {
@@ -323,9 +308,10 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
         load_image_size.width = img->nx;
         load_image_size.height = img->ny;
         clip_add_load_image_size(ctx_clip, &load_image_size);
-        LOG_INF("%s: load_image_size %d %d\n", __func__, load_image_size.width, load_image_size.height);
     }
     else if (clip_is_glm(ctx_clip)){
+        const int64_t t_img_enc_step_start_us = ggml_time_us();
+
         struct clip_image_size * load_image_size = clip_image_size_init();
         load_image_size->width  = clip_image_f32_batch_nx(img_res_v.get(), 0);
         load_image_size->height = clip_image_f32_batch_ny(img_res_v.get(), 0);
@@ -339,17 +325,24 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
             LOG_ERR("Unable to encode image \n");
             return false;
         }
+
+        const int64_t t_img_enc_steop_batch_us = ggml_time_us();
+        LOG_INF("%s: step 1 of 1 encoded in %8.2f ms\n", __func__, (t_img_enc_steop_batch_us - t_img_enc_step_start_us) / 1000.0);
     }
-    else if (strcmp(mm_patch_merge_type, "spatial_unpad") != 0) {
+    else if (strcmp(mm_patch_merge_type, "spatial_unpad") != 0 || clip_is_smolvlm(ctx_clip) || clip_is_pixtral(ctx_clip)) {
+        const int64_t t_img_enc_step_start_us = ggml_time_us();
+
         // flat / default llava-1.5 type embedding
-        *n_img_pos = clip_n_patches(ctx_clip);
         clip_image_f32 * img_res = clip_image_f32_get_img(img_res_v.get(), 0);
         bool encoded = clip_image_encode(ctx_clip, n_threads, img_res, image_embd); // image_embd shape is 576 x 4096
         if (!encoded) {
             LOG_ERR("Unable to encode image\n");
-
             return false;
         }
+        *n_img_pos = clip_n_patches_by_img(ctx_clip, img_res);
+
+        const int64_t t_img_enc_steop_batch_us = ggml_time_us();
+        LOG_INF("%s: step 1 of 1 encoded in %8.2f ms\n", __func__, (t_img_enc_steop_batch_us - t_img_enc_step_start_us) / 1000.0);
     }
     else {
         // spatial_unpad llava-1.6 type embedding
@@ -357,6 +350,8 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
         std::vector<float *> image_embd_v;
         image_embd_v.resize(n_imgs);
         for (size_t i = 0; i < n_imgs; i++) {
+            const int64_t t_img_enc_step_start_us = ggml_time_us();
+
             clip_image_f32 * img_res = clip_image_f32_get_img(img_res_v.get(), i);
             image_embd_v[i] = (float *)malloc(clip_embd_nbytes(ctx_clip)); // 576 patches * 4096 embeddings * 4 bytes = 9437184
             const bool encoded = clip_image_encode(ctx_clip, n_threads, img_res, image_embd_v[i]); // image data is in 3x336x336 format and will be converted to 336x336x3 inside
@@ -364,9 +359,10 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
                 LOG_ERR("Unable to encode image - spatial_unpad - subimage %d of %d\n", (int) i+1, (int) n_imgs);
                 return false;
             }
+
+            const int64_t t_img_enc_step_batch_us = ggml_time_us();
+            LOG_INF("%s: step %d of %d encoded in %8.2f ms\n", __func__, (int)i+1, (int)n_imgs, (t_img_enc_step_batch_us - t_img_enc_step_start_us) / 1000.0);
         }
-        const int64_t t_img_enc_batch_us = ggml_time_us();
-        LOG_INF("%s: %d segments encoded in %8.2f ms\n", __func__, (int)n_imgs, (t_img_enc_batch_us - t_img_enc_start_us) / 1000.0);
 
         const int32_t * image_grid = clip_image_grid(ctx_clip);
         const size_t num_gridpoints = get_clip_image_grid_size(ctx_clip);
@@ -397,11 +393,6 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
 
     LOG_INF("%s: image embedding created: %d tokens\n", __func__, *n_img_pos);
 
-    const int64_t t_img_enc_end_us = ggml_time_us();
-    float t_img_enc_ms = (t_img_enc_end_us - t_img_enc_start_us) / 1000.0;
-
-    LOG_INF("\n%s: image encoded in %8.2f ms by CLIP (%8.2f ms per image patch)\n", __func__, t_img_enc_ms, t_img_enc_ms / *n_img_pos);
-
     return true;
 }
 
@@ -416,6 +407,18 @@ bool llava_validate_embed_size(const llama_context * ctx_llama, const clip_ctx *
     return true;
 }
 
+bool llava_image_embed_make_with_clip_img_c(clip_ctx * ctx_clip, int n_threads, const clip_image_u8_c * img_c, float ** image_embd_out, int * n_img_pos_out) {
+    clip_image_u8 *img = clip_image_u8_init();
+    img->nx = img_c->nx;
+    img->ny = img_c->ny;
+    img->buf = std::vector<uint8_t>(img_c->buf_data, img_c->buf_data + img_c->buf_size);
+
+    bool r =  llava_image_embed_make_with_clip_img(ctx_clip, n_threads, img, image_embd_out, n_img_pos_out);
+    clip_image_u8_free(img);
+
+    return r;
+}
+
 bool llava_image_embed_make_with_clip_img(clip_ctx * ctx_clip, int n_threads, const clip_image_u8 * img, float ** image_embd_out, int * n_img_pos_out) {
     // Granite vision uses up to 10 patches + base patch
     int num_max_patches = 11;
@@ -525,6 +528,30 @@ struct llava_image_embed * llava_image_embed_make_with_bytes(struct clip_ctx * c
     return result;
 }
 
+struct llava_image_embed * llava_image_embed_make_with_data(struct clip_ctx * ctx_clip, int n_threads, const unsigned char * data, int nx, int ny, int nc) {
+    clip_image_u8 * img = clip_image_u8_init();
+    img->nx = nx;
+    img->ny = ny;
+    img->buf.resize(nx * ny * nc);
+    LOG_DBG("%s: image size: %.2fKiB, nx: %d, ny: %d, nc: %d\n", __func__, float(img->buf.size())/1024, nx, ny, nc);
+    memcpy(img->buf.data(), data, img->buf.size());
+
+    float* image_embed = NULL;
+    int n_image_pos = 0;
+    bool image_embed_result = llava_image_embed_make_with_clip_img(ctx_clip, n_threads, img, &image_embed, &n_image_pos);
+    if (!image_embed_result) {
+        clip_image_u8_free(img);
+        LOG_ERR("%s: couldn't embed the image\n", __func__);
+        return NULL;
+    }
+
+    clip_image_u8_free(img);
+    auto *result = (llava_image_embed*)malloc(sizeof(llava_image_embed));
+    result->embed = image_embed;
+    result->n_image_pos = n_image_pos;
+    return result;
+}
+
 static bool load_file_to_bytes(const char* path, unsigned char** bytesOut, long *sizeOut) {
     auto file = fopen(path, "rb");
     if (file == NULL) {
diff --git a/examples/llava/llava.h b/examples/llava/llava.h
index b6feb302..c06148c4 100644
--- a/examples/llava/llava.h
+++ b/examples/llava/llava.h
@@ -27,13 +27,23 @@ struct llava_image_embed {
     int n_image_pos;
 };
 
+struct clip_image_u8_c {
+    int nx;
+    int ny;
+    uint8_t * buf_data;
+    size_t    buf_size;
+};
+
 /** sanity check for clip <-> llava embed size match */
 LLAVA_API bool llava_validate_embed_size(const struct llama_context * ctx_llama, const struct clip_ctx * ctx_clip);
 
+LLAVA_API bool llava_image_embed_make_with_clip_img_c(struct clip_ctx * ctx_clip, int n_threads, const struct clip_image_u8_c * img, float ** image_embd_out, int * n_img_pos_out);
 LLAVA_API bool llava_image_embed_make_with_clip_img(struct clip_ctx * ctx_clip, int n_threads, const struct clip_image_u8 * img, float ** image_embd_out, int * n_img_pos_out);
 
 /** build an image embed from image file bytes */
 LLAVA_API struct llava_image_embed * llava_image_embed_make_with_bytes(struct clip_ctx * ctx_clip, int n_threads, const unsigned char * image_bytes, int image_bytes_length);
+/** build an image embed from image data */
+LLAVA_API struct llava_image_embed * llava_image_embed_make_with_data(struct clip_ctx * ctx_clip, int n_threads, const unsigned char * data, int nx, int ny, int nc);
 /** build an image embed from a path to an image filename */
 LLAVA_API struct llava_image_embed * llava_image_embed_make_with_filename(struct clip_ctx * ctx_clip, int n_threads, const char * image_path);
 /** free an embedding made with llava_image_embed_make_* */
