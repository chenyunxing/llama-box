diff --git a/examples/llava/clip-impl.h b/examples/llava/clip-impl.h
index 4d7340a5..dc9012d1 100644
--- a/examples/llava/clip-impl.h
+++ b/examples/llava/clip-impl.h
@@ -187,17 +187,7 @@ static void clip_log_internal(enum ggml_log_level level, const char * format, ..
     va_end(args);
 }
 
-#define LOG_TMPL(level, ...) \
-    do { \
-        if ((level) >= g_logger_state.verbosity_thold) { \
-            clip_log_internal((level), __VA_ARGS__); \
-        } \
-    } while (0)
-#define LOG_INF(...) LOG_TMPL(GGML_LOG_LEVEL_INFO,  __VA_ARGS__)
-#define LOG_WRN(...) LOG_TMPL(GGML_LOG_LEVEL_WARN,  __VA_ARGS__)
-#define LOG_ERR(...) LOG_TMPL(GGML_LOG_LEVEL_ERROR, __VA_ARGS__)
-#define LOG_DBG(...) LOG_TMPL(GGML_LOG_LEVEL_DEBUG, __VA_ARGS__)
-#define LOG_CNT(...) LOG_TMPL(GGML_LOG_LEVEL_CONT,  __VA_ARGS__)
+#include "common/log.h"
 
 //
 // cpp wrappers
diff --git a/examples/llava/clip.cpp b/examples/llava/clip.cpp
index 49c90b75..e08b32ee 100644
--- a/examples/llava/clip.cpp
+++ b/examples/llava/clip.cpp
@@ -331,12 +331,16 @@ struct clip_ctx {
 
     clip_image_size load_image_size;
 
+    int32_t max_image_size = 0;
+
     clip_ctx(clip_context_params & ctx_params) {
         backend_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
         backend     = ctx_params.use_gpu
                         ? ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_GPU, nullptr)
                         : nullptr;
 
+        max_image_size = ctx_params.max_image_size;
+
         if (backend) {
             LOG_INF("%s: CLIP using %s backend\n", __func__, ggml_backend_name(backend));
             backend_ptrs.push_back(backend);
@@ -1173,6 +1177,19 @@ struct clip_model_loader {
             get_bool(KEY_HAS_QWEN2VL_MERGER, ctx_clip.has_qwen2vl_merger, false);
             // !!! do NOT extend the list above, use KEY_PROJ_TYPE instead
 
+#if (!defined GGML_USE_CUDA) && (!defined GGML_USE_METAL)
+            if (ctx_clip.has_qwen2vl_merger && !ggml_backend_is_cpu(ctx_clip.backend)) {
+                LOG_WRN("%s: Qwen2VL merger is not supported on current backend, fallback to CPU backend\n", __func__);
+                ggml_backend_free(ctx_clip.backend);
+                ctx_clip.backend = ctx_clip.backend_cpu;
+                ctx_clip.backend_ptrs.erase(ctx_clip.backend_ptrs.begin());
+                ctx_clip.backend_buft.erase(ctx_clip.backend_buft.begin());
+                ctx_clip.sched.reset(
+                    ggml_backend_sched_new(ctx_clip.backend_ptrs.data(), ctx_clip.backend_buft.data(), ctx_clip.backend_ptrs.size(), 8192, false)
+                );
+            }
+#endif
+
             get_bool(KEY_USE_GELU, ctx_clip.use_gelu, false);
             get_bool(KEY_USE_SILU, ctx_clip.use_silu, false);
 
@@ -1467,15 +1484,29 @@ struct clip_model_loader {
 
         // create a fake batch
         clip_image_f32_batch batch;
-        clip_image_f32_ptr img(clip_image_f32_init());
         clip_image_size image_size;
-        image_size.width  = clip_get_image_size(&ctx_clip);
-        image_size.height = clip_get_image_size(&ctx_clip);
-        int n_patches = clip_get_image_size(&ctx_clip) / image_size.width;
-        img->nx = n_patches;
-        img->ny = n_patches;
-        img->buf.resize(n_patches * image_size.width * image_size.height * 3);
-        batch.entries.push_back(std::move(img));
+        if (ctx_clip.max_image_size > 0) {
+            clip_image_u8 * img = new clip_image_u8();
+            img->nx = ctx_clip.max_image_size;
+            img->ny = ctx_clip.max_image_size;
+            img->buf.resize(3 * ctx_clip.max_image_size * ctx_clip.max_image_size);
+            bool processed = clip_image_preprocess(&ctx_clip, img, &batch);
+            clip_image_u8_free(img);
+            if (!processed) {
+                throw std::runtime_error("unable to preprocess image");
+            }
+            image_size.width = batch.entries[0]->nx;
+            image_size.height = batch.entries[0]->ny;
+        } else {
+            clip_image_f32_ptr img(clip_image_f32_init());
+            image_size.width  = clip_get_image_size(&ctx_clip);
+            image_size.height = clip_get_image_size(&ctx_clip);
+            int n_patches = clip_get_image_size(&ctx_clip) / image_size.width;
+            img->nx = n_patches;
+            img->ny = n_patches;
+            img->buf.resize(n_patches * image_size.width * image_size.height * 3);
+            batch.entries.push_back(std::move(img));
+        }
 
         ggml_cgraph * gf = clip_image_build_graph(&ctx_clip, batch, image_size, false);
         ggml_backend_sched_reserve(ctx_clip.sched.get(), gf);
@@ -2057,7 +2088,6 @@ bool clip_image_preprocess(struct clip_ctx * ctx, const clip_image_u8 * img, str
         std::vector<std::vector<clip_image_u8_ptr>> imgs = uhd_slice_image(img, max_slice_nums);
         for (size_t i = 0; i < imgs.size(); ++i) {
             for (size_t j = 0; j < imgs[i].size(); ++j) {
-                LOG_DBG("%s: %d %d\n", __func__,imgs[i][j]->nx,imgs[i][j]->ny);
                 clip_image_f32_ptr res(clip_image_f32_init());
                 normalize_image_u8_to_f32(*imgs[i][j], *res, ctx->image_mean, ctx->image_std);
                 res_imgs->entries.push_back(std::move(res));
@@ -2154,7 +2184,7 @@ bool clip_image_preprocess(struct clip_ctx * ctx, const clip_image_u8 * img, str
 
             clip_image_u8_ptr image_original_resize(clip_image_u8_init());
             // bilinear_resize(*img, *image_original_resize, params.image_size, params.image_size); // in python this is "shortest_edge", but all CLIP are square
-            bicubic_resize(*img, *image_original_resize, params.image_size, params.image_size); // in python this is "shortest_edge", but all CLIP are square
+            resize_and_pad_image(*img, *image_original_resize, {params.image_size, params.image_size}); // in python this is "shortest_edge", but all CLIP are square
             patches.insert(patches.begin(), std::move(image_original_resize));
             for (auto & patch : patches) {
                 clip_image_f32_ptr res(clip_image_f32_init());
@@ -2187,48 +2217,8 @@ bool clip_image_preprocess(struct clip_ctx * ctx, const clip_image_u8 * img, str
     const int nx3 = int(nx / scale + 0.5f);
     const int ny3 = int(ny / scale + 0.5f);
 
-    const auto & m3 = ctx->image_mean; // {0.48145466f, 0.4578275f, 0.40821073f};
-    const auto & s3 = ctx->image_std;  // {0.26862954f, 0.26130258f, 0.27577711f};
-
-    for (int y = 0; y < ny3; y++) {
-        for (int x = 0; x < nx3; x++) {
-            for (int c = 0; c < 3; c++) {
-                // linear interpolation
-                const float sx = (x + 0.5f) * scale - 0.5f;
-                const float sy = (y + 0.5f) * scale - 0.5f;
-
-                const int x0 = std::max(0, (int)std::floor(sx));
-                const int y0 = std::max(0, (int)std::floor(sy));
-
-                const int x1 = std::min(x0 + 1, nx - 1);
-                const int y1 = std::min(y0 + 1, ny - 1);
-
-                const float dx = sx - x0;
-                const float dy = sy - y0;
-
-                const int j00 = 3 * (y0 * nx + x0) + c;
-                const int j01 = 3 * (y0 * nx + x1) + c;
-                const int j10 = 3 * (y1 * nx + x0) + c;
-                const int j11 = 3 * (y1 * nx + x1) + c;
-
-                const float v00 = temp->buf[j00];
-                const float v01 = temp->buf[j01];
-                const float v10 = temp->buf[j10];
-                const float v11 = temp->buf[j11];
-
-                const float v0 = v00 * (1.0f - dx) + v01 * dx;
-                const float v1 = v10 * (1.0f - dx) + v11 * dx;
-
-                const float v = v0 * (1.0f - dy) + v1 * dy;
-
-                const uint8_t v2 = std::min(std::max(std::round(v), 0.0f), 255.0f);
-
-                const int i = 3 * (y * nx3 + x) + c;
-
-                res->buf[i] = ((float(v2) / 255.0f) - m3[c]) / s3[c];
-            }
-        }
-    }
+    resize_and_pad_image(*img, *temp, {nx3, ny3});
+    normalize_image_u8_to_f32(*temp, *res, ctx->image_mean, ctx->image_std);
 
     // {
     //     clip_image_u8 * temp2 = clip_image_u8_init();
diff --git a/examples/llava/clip.h b/examples/llava/clip.h
index cc133a58..ceb2411d 100644
--- a/examples/llava/clip.h
+++ b/examples/llava/clip.h
@@ -36,6 +36,7 @@ struct clip_image_f32_batch;
 struct clip_context_params {
     bool use_gpu;
     ggml_log_level verbosity;
+    int32_t max_image_size = 0;
 };
 
 // deprecated, use clip_init
diff --git a/examples/llava/llava.cpp b/examples/llava/llava.cpp
index 03a22cbb..1fa988ca 100644
--- a/examples/llava/llava.cpp
+++ b/examples/llava/llava.cpp
@@ -12,17 +12,7 @@
 #include <vector>
 #include <memory>
 
-#if defined(LLAVA_LOG_OFF)
-#   define LOG_INF(...)
-#   define LOG_WRN(...)
-#   define LOG_ERR(...)
-#   define LOG_DBG(...)
-#else // defined(LLAVA_LOG_OFF)
-#   define LOG_INF(...) do { fprintf(stdout, __VA_ARGS__); } while (0)
-#   define LOG_WRN(...) do { fprintf(stderr, __VA_ARGS__); } while (0)
-#   define LOG_ERR(...) do { fprintf(stderr, __VA_ARGS__); } while (0)
-#   define LOG_DBG(...) do { fprintf(stdout, __VA_ARGS__); } while (0)
-#endif // defined(LLAVA_LOG_OFF)
+#include "common/log.h"
 
 // RGB uint8 image
 struct clip_image_u8 {
@@ -264,8 +254,6 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
         return false;
     }
 
-    const int64_t t_img_enc_start_us = ggml_time_us();
-
     const char * mm_patch_merge_type = clip_patch_merge_type(ctx_clip);
 
     const size_t n_imgs = clip_image_f32_batch_n_images(img_res_v.get());
@@ -295,14 +283,11 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
             }
 
             if (!encoded) {
-                LOG_ERR("Unable to encode image - spatial_unpad - subimage %d of %d\n", (int) i+1, (int) n_imgs);
                 return false;
             }
             const int64_t t_img_enc_steop_batch_us = ggml_time_us();
             LOG_INF("%s: step %d of %d encoded in %8.2f ms\n", __func__, (int)i+1, (int)n_imgs, (t_img_enc_steop_batch_us - t_img_enc_step_start_us) / 1000.0);
         }
-        const int64_t t_img_enc_batch_us = ggml_time_us();
-        LOG_INF("%s: all %d segments encoded in %8.2f ms\n", __func__, (int)n_imgs, (t_img_enc_batch_us - t_img_enc_start_us) / 1000.0);
 
         int n_img_pos_out = 0;
         for (size_t i = 0; i < image_embd_v.size(); i++) {
@@ -323,9 +308,10 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
         load_image_size.width = img->nx;
         load_image_size.height = img->ny;
         clip_add_load_image_size(ctx_clip, &load_image_size);
-        LOG_INF("%s: load_image_size %d %d\n", __func__, load_image_size.width, load_image_size.height);
     }
     else if (clip_is_glm(ctx_clip)){
+        const int64_t t_img_enc_step_start_us = ggml_time_us();
+
         struct clip_image_size * load_image_size = clip_image_size_init();
         load_image_size->width  = clip_image_f32_batch_nx(img_res_v.get(), 0);
         load_image_size->height = clip_image_f32_batch_ny(img_res_v.get(), 0);
@@ -339,8 +325,13 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
             LOG_ERR("Unable to encode image \n");
             return false;
         }
+
+        const int64_t t_img_enc_steop_batch_us = ggml_time_us();
+        LOG_INF("%s: step 1 of 1 encoded in %8.2f ms\n", __func__, (t_img_enc_steop_batch_us - t_img_enc_step_start_us) / 1000.0);
     }
     else if (strcmp(mm_patch_merge_type, "spatial_unpad") != 0) {
+        const int64_t t_img_enc_step_start_us = ggml_time_us();
+
         // flat / default llava-1.5 type embedding
         *n_img_pos = clip_n_patches(ctx_clip);
         clip_image_f32 * img_res = clip_image_f32_get_img(img_res_v.get(), 0);
@@ -350,6 +341,9 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
 
             return false;
         }
+
+        const int64_t t_img_enc_steop_batch_us = ggml_time_us();
+        LOG_INF("%s: step 1 of 1 encoded in %8.2f ms\n", __func__, (t_img_enc_steop_batch_us - t_img_enc_step_start_us) / 1000.0);
     }
     else {
         // spatial_unpad llava-1.6 type embedding
@@ -357,6 +351,8 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
         std::vector<float *> image_embd_v;
         image_embd_v.resize(n_imgs);
         for (size_t i = 0; i < n_imgs; i++) {
+            const int64_t t_img_enc_step_start_us = ggml_time_us();
+
             clip_image_f32 * img_res = clip_image_f32_get_img(img_res_v.get(), i);
             image_embd_v[i] = (float *)malloc(clip_embd_nbytes(ctx_clip)); // 576 patches * 4096 embeddings * 4 bytes = 9437184
             const bool encoded = clip_image_encode(ctx_clip, n_threads, img_res, image_embd_v[i]); // image data is in 3x336x336 format and will be converted to 336x336x3 inside
@@ -364,9 +360,10 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
                 LOG_ERR("Unable to encode image - spatial_unpad - subimage %d of %d\n", (int) i+1, (int) n_imgs);
                 return false;
             }
+
+            const int64_t t_img_enc_step_batch_us = ggml_time_us();
+            LOG_INF("%s: step %d of %d encoded in %8.2f ms\n", __func__, (int)i+1, (int)n_imgs, (t_img_enc_step_batch_us - t_img_enc_step_start_us) / 1000.0);
         }
-        const int64_t t_img_enc_batch_us = ggml_time_us();
-        LOG_INF("%s: %d segments encoded in %8.2f ms\n", __func__, (int)n_imgs, (t_img_enc_batch_us - t_img_enc_start_us) / 1000.0);
 
         const int32_t * image_grid = clip_image_grid(ctx_clip);
         const size_t num_gridpoints = get_clip_image_grid_size(ctx_clip);
@@ -397,11 +394,6 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
 
     LOG_INF("%s: image embedding created: %d tokens\n", __func__, *n_img_pos);
 
-    const int64_t t_img_enc_end_us = ggml_time_us();
-    float t_img_enc_ms = (t_img_enc_end_us - t_img_enc_start_us) / 1000.0;
-
-    LOG_INF("\n%s: image encoded in %8.2f ms by CLIP (%8.2f ms per image patch)\n", __func__, t_img_enc_ms, t_img_enc_ms / *n_img_pos);
-
     return true;
 }
 
@@ -416,6 +408,18 @@ bool llava_validate_embed_size(const llama_context * ctx_llama, const clip_ctx *
     return true;
 }
 
+bool llava_image_embed_make_with_clip_img_c(clip_ctx * ctx_clip, int n_threads, const clip_image_u8_c * img_c, float ** image_embd_out, int * n_img_pos_out) {
+    clip_image_u8 *img = clip_image_u8_init();
+    img->nx = img_c->nx;
+    img->ny = img_c->ny;
+    img->buf = std::vector<uint8_t>(img_c->buf_data, img_c->buf_data + img_c->buf_size);
+
+    bool r =  llava_image_embed_make_with_clip_img(ctx_clip, n_threads, img, image_embd_out, n_img_pos_out);
+    clip_image_u8_free(img);
+
+    return r;
+}
+
 bool llava_image_embed_make_with_clip_img(clip_ctx * ctx_clip, int n_threads, const clip_image_u8 * img, float ** image_embd_out, int * n_img_pos_out) {
     // Granite vision uses up to 10 patches + base patch
     int num_max_patches = 11;
@@ -525,6 +529,30 @@ struct llava_image_embed * llava_image_embed_make_with_bytes(struct clip_ctx * c
     return result;
 }
 
+struct llava_image_embed * llava_image_embed_make_with_data(struct clip_ctx * ctx_clip, int n_threads, const unsigned char * data, int nx, int ny, int nc) {
+    clip_image_u8 * img = clip_image_u8_init();
+    img->nx = nx;
+    img->ny = ny;
+    img->buf.resize(nx * ny * nc);
+    LOG_DBG("%s: image size: %.2fKiB, nx: %d, ny: %d, nc: %d\n", __func__, float(img->buf.size())/1024, nx, ny, nc);
+    memcpy(img->buf.data(), data, img->buf.size());
+
+    float* image_embed = NULL;
+    int n_image_pos = 0;
+    bool image_embed_result = llava_image_embed_make_with_clip_img(ctx_clip, n_threads, img, &image_embed, &n_image_pos);
+    if (!image_embed_result) {
+        clip_image_u8_free(img);
+        LOG_ERR("%s: couldn't embed the image\n", __func__);
+        return NULL;
+    }
+
+    clip_image_u8_free(img);
+    auto *result = (llava_image_embed*)malloc(sizeof(llava_image_embed));
+    result->embed = image_embed;
+    result->n_image_pos = n_image_pos;
+    return result;
+}
+
 static bool load_file_to_bytes(const char* path, unsigned char** bytesOut, long *sizeOut) {
     auto file = fopen(path, "rb");
     if (file == NULL) {
diff --git a/examples/llava/llava.h b/examples/llava/llava.h
index b6feb302..c06148c4 100644
--- a/examples/llava/llava.h
+++ b/examples/llava/llava.h
@@ -27,13 +27,23 @@ struct llava_image_embed {
     int n_image_pos;
 };
 
+struct clip_image_u8_c {
+    int nx;
+    int ny;
+    uint8_t * buf_data;
+    size_t    buf_size;
+};
+
 /** sanity check for clip <-> llava embed size match */
 LLAVA_API bool llava_validate_embed_size(const struct llama_context * ctx_llama, const struct clip_ctx * ctx_clip);
 
+LLAVA_API bool llava_image_embed_make_with_clip_img_c(struct clip_ctx * ctx_clip, int n_threads, const struct clip_image_u8_c * img, float ** image_embd_out, int * n_img_pos_out);
 LLAVA_API bool llava_image_embed_make_with_clip_img(struct clip_ctx * ctx_clip, int n_threads, const struct clip_image_u8 * img, float ** image_embd_out, int * n_img_pos_out);
 
 /** build an image embed from image file bytes */
 LLAVA_API struct llava_image_embed * llava_image_embed_make_with_bytes(struct clip_ctx * ctx_clip, int n_threads, const unsigned char * image_bytes, int image_bytes_length);
+/** build an image embed from image data */
+LLAVA_API struct llava_image_embed * llava_image_embed_make_with_data(struct clip_ctx * ctx_clip, int n_threads, const unsigned char * data, int nx, int ny, int nc);
 /** build an image embed from a path to an image filename */
 LLAVA_API struct llava_image_embed * llava_image_embed_make_with_filename(struct clip_ctx * ctx_clip, int n_threads, const char * image_path);
 /** free an embedding made with llava_image_embed_make_* */
