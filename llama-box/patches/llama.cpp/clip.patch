diff --git a/examples/llava/clip.cpp b/examples/llava/clip.cpp
index e2150c06..d9abd47f 100644
--- a/examples/llava/clip.cpp
+++ b/examples/llava/clip.cpp
@@ -9,25 +9,25 @@
 #include "ggml-backend.h"
 #include "gguf.h"
 
-//#ifdef GGML_USE_CUDA
-//#include "ggml-cuda.h"
-//#endif
-//
-//#ifdef GGML_USE_SYCL
-//#include "ggml-sycl.h"
-//#endif
-//
-//#ifdef GGML_USE_METAL
-//#include "ggml-metal.h"
-//#endif
-//
-//#ifdef GGML_USE_CANN
-//#include "ggml-cann.h"
-//#endif
-//
-//#ifdef GGML_USE_VULKAN
-//#include "ggml-vulkan.h"
-//#endif
+#ifdef GGML_USE_CUDA
+#include "ggml-cuda.h"
+#endif
+
+#ifdef GGML_USE_SYCL
+#include "ggml-sycl.h"
+#endif
+
+#ifdef GGML_USE_METAL
+#include "ggml-metal.h"
+#endif
+
+#ifdef GGML_USE_CANN
+#include "ggml-cann.h"
+#endif
+
+#ifdef GGML_USE_VULKAN
+#include "ggml-vulkan.h"
+#endif
 
 #define STB_IMAGE_IMPLEMENTATION
 #include "stb_image.h"
@@ -45,17 +45,7 @@
 #include <cinttypes>
 #include <limits>
 
-#if defined(LLAVA_LOG_OFF)
-#   define LOG_INF(...)
-#   define LOG_WRN(...)
-#   define LOG_ERR(...)
-#   define LOG_DBG(...)
-#else // defined(LLAVA_LOG_OFF)
-#   define LOG_INF(...) do { fprintf(stdout, __VA_ARGS__); } while (0)
-#   define LOG_WRN(...) do { fprintf(stderr, __VA_ARGS__); } while (0)
-#   define LOG_ERR(...) do { fprintf(stderr, __VA_ARGS__); } while (0)
-#   define LOG_DBG(...) do { fprintf(stdout, __VA_ARGS__); } while (0)
-#endif // defined(LLAVA_LOG_OFF)
+#include "common/log.h"
 
 //#define CLIP_DEBUG_FUNCTIONS
 
@@ -1162,7 +1152,7 @@ static ggml_cgraph * clip_image_build_graph(clip_ctx * ctx, const clip_image_f32
 }
 
 // read and create ggml_context containing the tensors and their data
-struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
+struct clip_ctx * clip_model_load(const char * fname, const int verbosity, int32_t ngl, int32_t max_image_size) {
     struct ggml_context * meta = NULL;
 
     struct gguf_init_params params = {
@@ -1193,7 +1183,6 @@ struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
         LOG_INF("%s: n_tensors:    %d\n", __func__, n_tensors);
         LOG_INF("%s: n_kv:         %d\n", __func__, n_kv);
         LOG_INF("%s: ftype:        %s\n", __func__, ftype_str.c_str());
-        LOG_INF("\n");
     }
     const int n_tensors = gguf_get_n_tensors(ctx);
 
@@ -1249,7 +1238,7 @@ struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
             struct ggml_tensor * cur = ggml_get_tensor(meta, name);
             size_t tensor_size = ggml_nbytes(cur);
             model_size += tensor_size;
-            if (verbosity >= 3) {
+            if (verbosity > 5) {
                 LOG_INF("%s: tensor[%d]: n_dims = %d, name = %s, tensor_size=%zu, offset=%zu, shape:[%" PRIu64 ", %" PRIu64 ", %" PRIu64 ", %" PRIu64 "], type = %s\n",
                        __func__, i, ggml_n_dims(cur), cur->name, tensor_size, offset, cur->ne[0], cur->ne[1], cur->ne[2], cur->ne[3], ggml_type_name(type));
             }
@@ -1275,30 +1264,40 @@ struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
         }
     }
 
-//#ifdef GGML_USE_CUDA
-//    new_clip->backend = ggml_backend_cuda_init(0);
-//    LOG_INF("%s: CLIP using CUDA backend\n", __func__);
-//#endif
-//
-//#ifdef GGML_USE_METAL
-//    new_clip->backend = ggml_backend_metal_init();
-//    LOG_INF("%s: CLIP using Metal backend\n", __func__);
-//#endif
-//
-//#ifdef GGML_USE_CANN
-//    new_clip->backend = ggml_backend_cann_init(0);
-//    LOG_INF("%s: CLIP using CANN backend\n", __func__);
-//#endif
-//
-//#ifdef GGML_USE_VULKAN
-//    new_clip->backend = ggml_backend_vk_init(0);
-//    LOG_INF("%s: CLIP using Vulkan backend\n", __func__);
-//#endif
-//
-//#ifdef GGML_USE_SYCL
-//    new_clip->backend = ggml_backend_sycl_init(0);
-//    LOG_INF("%s: CLIP using SYCL backend\n", __func__);
-//#endif
+#ifdef GGML_USE_CUDA
+    if (ngl != 0) {
+        new_clip->backend = ggml_backend_cuda_init(0);
+        LOG_INF("%s: CLIP using CUDA backend\n", __func__);
+    }
+#endif
+
+#ifdef GGML_USE_METAL
+    if (ngl != 0) {
+        new_clip->backend = ggml_backend_metal_init();
+        LOG_INF("%s: CLIP using Metal backend\n", __func__);
+    }
+#endif
+
+#ifdef GGML_USE_CANN
+    if (ngl != 0) {
+        new_clip->backend = ggml_backend_cann_init(0);
+        LOG_INF("%s: CLIP using CANN backend\n", __func__);
+    }
+#endif
+
+#ifdef GGML_USE_VULKAN
+    if (ngl != 0) {
+        new_clip->backend = ggml_backend_vk_init(0);
+        LOG_INF("%s: CLIP using Vulkan backend\n", __func__);
+    }
+#endif
+
+#ifdef GGML_USE_SYCL
+    if (ngl != 0) {
+        new_clip->backend = ggml_backend_sycl_init(0);
+        LOG_INF("%s: CLIP using SYCL backend\n", __func__);
+    }
+#endif
 
     if (!new_clip->backend) {
         new_clip->backend = ggml_backend_cpu_init();
@@ -1336,6 +1335,13 @@ struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
         idx = gguf_find_key(ctx, KEY_HAS_QWEN2VL_MERGER);
         if (idx != -1) {
             new_clip->has_qwen2vl_merger = gguf_get_val_bool(ctx, idx);
+#ifndef GGML_USE_CUDA
+            if (!ggml_backend_is_cpu(new_clip->backend)) {
+                ggml_backend_free(new_clip->backend);
+                new_clip->backend = ggml_backend_cpu_init();
+                LOG_WRN("%s: Qwen2VL merger is not supported on current backend, fallback to CPU backend\n", __func__);
+            }
+#endif
         }
         // GGML_ASSERT(new_clip->has_llava_projector); // see monatis/clip.cpp for image and/or text encoding for semantic search
 
@@ -1357,6 +1363,7 @@ struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
             LOG_INF("%s: vision_encoder: %d\n", __func__, new_clip->has_vision_encoder);
             LOG_INF("%s: llava_projector:  %d\n", __func__, new_clip->has_llava_projector);
             LOG_INF("%s: minicpmv_projector:  %d\n", __func__, new_clip->has_minicpmv_projector);
+            LOG_INF("%s: qwen2vl_merger:  %d\n", __func__, new_clip->has_qwen2vl_merger);
             LOG_INF("%s: glm_projector:  %d\n", __func__, new_clip->has_glm_projector);
             LOG_INF("%s: model size:     %.2f MB\n", __func__, model_size / 1024.0 / 1024.0);
             LOG_INF("%s: metadata size:  %.2f MB\n", __func__, ggml_get_mem_size(meta) / 1024.0 / 1024.0);
@@ -1477,23 +1484,23 @@ struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
         }
 
         if (verbosity >= 2) {
-            LOG_INF("\n%s: vision model hparams\n", __func__);
-            LOG_INF("image_size         %d\n", hparams.image_size);
-            LOG_INF("patch_size         %d\n", hparams.patch_size);
-            LOG_INF("v_hidden_size      %d\n", hparams.hidden_size);
-            LOG_INF("v_n_intermediate   %d\n", hparams.n_intermediate);
-            LOG_INF("v_projection_dim   %d\n", hparams.projection_dim);
-            LOG_INF("v_n_head           %d\n", hparams.n_head);
-            LOG_INF("v_n_layer          %d\n", hparams.n_layer);
-            LOG_INF("v_eps              %f\n", hparams.eps);
-            LOG_INF("v_image_mean       %f %f %f\n", new_clip->image_mean[0], new_clip->image_mean[1], new_clip->image_mean[2]);
-            LOG_INF("v_image_std        %f %f %f\n", new_clip->image_std[0], new_clip->image_std[1], new_clip->image_std[2]);
-            LOG_INF("v_image_grid_pinpoints: ");
+            LOG_INF("%s: vision model hparams\n", __func__);
+            LOG_INF("image_size             %d\n", hparams.image_size);
+            LOG_INF("patch_size             %d\n", hparams.patch_size);
+            LOG_INF("v_hidden_size          %d\n", hparams.hidden_size);
+            LOG_INF("v_n_intermediate       %d\n", hparams.n_intermediate);
+            LOG_INF("v_projection_dim       %d\n", hparams.projection_dim);
+            LOG_INF("v_n_head               %d\n", hparams.n_head);
+            LOG_INF("v_n_layer              %d\n", hparams.n_layer);
+            LOG_INF("v_eps                  %f\n", hparams.eps);
+            LOG_INF("v_image_mean           %f %f %f\n", new_clip->image_mean[0], new_clip->image_mean[1], new_clip->image_mean[2]);
+            LOG_INF("v_image_std            %f %f %f\n", new_clip->image_std[0], new_clip->image_std[1], new_clip->image_std[2]);
+            std::string pinpoints_str;
             for (int i = 0; i < 32 && (hparams.image_grid_pinpoints[i] != 0); ++i) {
-                LOG_INF("%d ", hparams.image_grid_pinpoints[i]);
+                pinpoints_str += std::to_string(hparams.image_grid_pinpoints[i]) + " ";
             }
-            LOG_INF("\n");
-            LOG_INF("v_mm_patch_merge_type: %s\n", hparams.mm_patch_merge_type);
+            LOG_INF("v_image_grid_pinpoints %s\n", pinpoints_str.c_str());
+            LOG_INF("v_mm_patch_merge_type  %s\n", hparams.mm_patch_merge_type);
 
         }
 
@@ -1682,7 +1689,29 @@ struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
         clip_image_f32_batch batch;
         batch.size = 1;
         batch.data = nullptr;
-        ggml_cgraph * gf = clip_image_build_graph(new_clip, &batch, nullptr, false);
+        ggml_cgraph * gf;
+        if (max_image_size > 0) {
+            clip_image_u8 * img = new clip_image_u8();
+            img->nx = max_image_size;
+            img->ny = max_image_size;
+            img->buf.resize(3 * max_image_size * max_image_size);
+            bool processed = clip_image_preprocess(new_clip, img, &batch);
+            clip_image_u8_free(img);
+            if (!processed) {
+                LOG_ERR("%s: unable to preprocess image\n", __func__);
+                delete[] batch.data;
+                clip_free(new_clip);
+                return nullptr;
+            }
+            batch.size = 1;
+            struct clip_image_size * load_image_size = clip_image_size_init();
+            load_image_size->width = batch.data[0].nx;
+            load_image_size->height = batch.data[0].ny;
+            gf = clip_image_build_graph(new_clip, &batch, load_image_size, true);
+            delete[] batch.data;
+        } else {
+            gf = clip_image_build_graph(new_clip, &batch, nullptr, false);
+        }
         ggml_gallocr_reserve(new_clip->compute_alloc, gf);
         size_t compute_memory_buffer_size = ggml_gallocr_get_buffer_size(new_clip->compute_alloc, 0);
         LOG_INF("%s: compute allocated memory: %.2f MB\n", __func__, compute_memory_buffer_size /1024.0/1024.0);
@@ -2060,7 +2089,7 @@ static std::vector<std::vector<clip_image_u8 *>> uhd_slice_image(const clip_imag
     const int multiple = fmin(ceil(ratio), max_slice_nums);
 
     std::vector<std::vector<clip_image_u8 *>> images;
-    LOG_INF("%s: multiple %d\n", __func__, multiple);
+    LOG_INFV(4, "%s: multiple %d\n", __func__, multiple);
     images.push_back(std::vector<clip_image_u8 *>());
 
     if (multiple <= 1) {
@@ -2075,17 +2104,17 @@ static std::vector<std::vector<clip_image_u8 *>> uhd_slice_image(const clip_imag
         clip_image_u8 * source_image = clip_image_u8_init();
         bicubic_resize(*img, *source_image, best_size.first, best_size.second);
         // source_image = image.copy().resize(best_resize, Image.Resampling.BICUBIC)
-        LOG_INF("%s: image_size: %d %d; source_image size: %d %d\n", __func__, img->nx, img->ny, best_size.first, best_size.second);
+        LOG_INFV(4, "%s: image_size: %d %d; source_image size: %d %d\n", __func__, img->nx, img->ny, best_size.first, best_size.second);
         images[images.size()-1].push_back(source_image);
 
         std::pair<int, int> best_grid = uhd_best_grid(max_slice_nums, multiple, log_ratio);
-        LOG_INF("%s: image_size: %d %d; best_grid: %d %d\n", __func__, img->nx, img->ny, best_grid.first, best_grid.second);
+        LOG_INFV(4, "%s: image_size: %d %d; best_grid: %d %d\n", __func__, img->nx, img->ny, best_grid.first, best_grid.second);
 
         auto refine_size = uhd_get_refine_size(original_size, best_grid, scale_resolution, patch_size, true);
         clip_image_u8 * refine_image = clip_image_u8_init();
         bicubic_resize(*img, *refine_image, refine_size.first, refine_size.second);
 
-        LOG_INF("%s: refine_image_size: %d %d; refine_size: %d %d\n", __func__, refine_image->nx, refine_image->ny, refine_size.first, refine_size.second);
+        LOG_INFV(4, "%s: refine_image_size: %d %d; refine_size: %d %d\n", __func__, refine_image->nx, refine_image->ny, refine_size.first, refine_size.second);
 
         // split_to_patches
         int width = refine_image->nx;
@@ -2143,7 +2172,7 @@ bool clip_image_preprocess(struct clip_ctx * ctx, const clip_image_u8 * img, cli
         int idx = 0;
         for (size_t i = 0; i < imgs.size(); ++i) {
             for (size_t j = 0; j < imgs[i].size(); ++j) {
-                LOG_DBG("%s: %d %d\n", __func__,imgs[i][j]->nx,imgs[i][j]->ny);
+                LOG_DBGV(6, "%s: %d %d\n", __func__,imgs[i][j]->nx,imgs[i][j]->ny);
                 clip_image_f32 * res = clip_image_f32_init();
                 normalize_image_u8_to_f32(imgs[i][j], res, ctx->image_mean, ctx->image_std);
                 res_imgs->data[idx++] = *res;
@@ -2168,6 +2197,7 @@ bool clip_image_preprocess(struct clip_ctx * ctx, const clip_image_u8 * img, cli
 
         res_imgs->data = new clip_image_f32[1];
         // clip_image_f32 * res = clip_image_f32_init();
+        LOG_DBGV(6, "%s: %d %d\n", __func__, resized->nx, resized->ny);
         normalize_image_u8_to_f32(resized, res_imgs->data, ctx->image_mean, ctx->image_std);
         // res_imgs->data[0] = *res;
         res_imgs->size = 1;
@@ -2258,13 +2288,14 @@ bool clip_image_preprocess(struct clip_ctx * ctx, const clip_image_u8 * img, cli
 
             clip_image_u8 *image_original_resize = clip_image_u8_init();
             // bilinear_resize(*img, *image_original_resize, params.image_size, params.image_size); // in python this is "shortest_edge", but all CLIP are square
-            bicubic_resize(*img, *image_original_resize, params.image_size, params.image_size); // in python this is "shortest_edge", but all CLIP are square
+            resize_and_pad_image(*img, *image_original_resize, {params.image_size, params.image_size}); // in python this is "shortest_edge", but all CLIP are square
             patches.insert(patches.begin(), image_original_resize);
             // clip_image_f32_batch_init(patches.size());
             res_imgs->size = patches.size();
             res_imgs->data = new clip_image_f32[res_imgs->size];
             int num=0;
             for (auto& patch : patches) {
+                LOG_DBGV(6, "%s: %d %d\n", __func__, patch->nx, patch->ny);
                 normalize_image_u8_to_f32(patch, &res_imgs->data[num], ctx->image_mean, ctx->image_std);
                 num++;
             }
@@ -2300,49 +2331,10 @@ bool clip_image_preprocess(struct clip_ctx * ctx, const clip_image_u8 * img, cli
 
     const int nx3 = int(nx / scale + 0.5f);
     const int ny3 = int(ny / scale + 0.5f);
+    resize_and_pad_image(*img, *temp, {nx3, ny3});
 
-    const auto & m3 = ctx->image_mean; // {0.48145466f, 0.4578275f, 0.40821073f};
-    const auto & s3 = ctx->image_std;  // {0.26862954f, 0.26130258f, 0.27577711f};
-
-    for (int y = 0; y < ny3; y++) {
-        for (int x = 0; x < nx3; x++) {
-            for (int c = 0; c < 3; c++) {
-                // linear interpolation
-                const float sx = (x + 0.5f) * scale - 0.5f;
-                const float sy = (y + 0.5f) * scale - 0.5f;
-
-                const int x0 = std::max(0, (int)std::floor(sx));
-                const int y0 = std::max(0, (int)std::floor(sy));
-
-                const int x1 = std::min(x0 + 1, nx - 1);
-                const int y1 = std::min(y0 + 1, ny - 1);
-
-                const float dx = sx - x0;
-                const float dy = sy - y0;
-
-                const int j00 = 3 * (y0 * nx + x0) + c;
-                const int j01 = 3 * (y0 * nx + x1) + c;
-                const int j10 = 3 * (y1 * nx + x0) + c;
-                const int j11 = 3 * (y1 * nx + x1) + c;
-
-                const float v00 = temp->buf[j00];
-                const float v01 = temp->buf[j01];
-                const float v10 = temp->buf[j10];
-                const float v11 = temp->buf[j11];
-
-                const float v0 = v00 * (1.0f - dx) + v01 * dx;
-                const float v1 = v10 * (1.0f - dx) + v11 * dx;
-
-                const float v = v0 * (1.0f - dy) + v1 * dy;
-
-                const uint8_t v2 = std::min(std::max(std::round(v), 0.0f), 255.0f);
-
-                const int i = 3 * (y * nx3 + x) + c;
-
-                res->buf[i] = ((float(v2) / 255.0f) - m3[c]) / s3[c];
-            }
-        }
-    }
+    LOG_DBGV(6, "%s: %d %d\n", __func__, temp->nx, temp->ny);
+    normalize_image_u8_to_f32(temp, res, ctx->image_mean, ctx->image_std);
     clip_image_u8_free(temp);
 
     // {
diff --git a/examples/llava/clip.h b/examples/llava/clip.h
index f5571220..c37747c5 100644
--- a/examples/llava/clip.h
+++ b/examples/llava/clip.h
@@ -39,7 +39,7 @@ struct clip_image_f32_batch {
     size_t size;
 };
 
-CLIP_API struct clip_ctx * clip_model_load    (const char * fname, int verbosity);
+CLIP_API struct clip_ctx * clip_model_load    (const char * fname, int verbosity = 1, int32_t ngl = 999, int32_t max_image_size = 0);
 CLIP_API struct clip_ctx * clip_model_load_cpu(const char * fname, int verbosity);
 
 CLIP_API void clip_free(struct clip_ctx * ctx);
diff --git a/examples/llava/llava.cpp b/examples/llava/llava.cpp
index 30071404..04c092c1 100644
--- a/examples/llava/llava.cpp
+++ b/examples/llava/llava.cpp
@@ -11,17 +11,7 @@
 #include <limits>
 #include <vector>
 
-#if defined(LLAVA_LOG_OFF)
-#   define LOG_INF(...)
-#   define LOG_WRN(...)
-#   define LOG_ERR(...)
-#   define LOG_DBG(...)
-#else // defined(LLAVA_LOG_OFF)
-#   define LOG_INF(...) do { fprintf(stdout, __VA_ARGS__); } while (0)
-#   define LOG_WRN(...) do { fprintf(stderr, __VA_ARGS__); } while (0)
-#   define LOG_ERR(...) do { fprintf(stderr, __VA_ARGS__); } while (0)
-#   define LOG_DBG(...) do { fprintf(stdout, __VA_ARGS__); } while (0)
-#endif // defined(LLAVA_LOG_OFF)
+#include "common/log.h"
 
 // RGB uint8 image
 struct clip_image_u8 {
@@ -249,14 +239,17 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
     clip_image_f32_batch img_res_v;
     img_res_v.size = 0;
     img_res_v.data = nullptr;
+#define free_img_res_v \
+    delete[] img_res_v.data; \
+    img_res_v.size = 0; \
+    img_res_v.data = nullptr;
+
     if (!clip_image_preprocess(ctx_clip, img, &img_res_v)) {
         LOG_ERR("%s: unable to preprocess image\n", __func__);
-        delete[] img_res_v.data;
+        free_img_res_v;
         return false;
     }
 
-    const int64_t t_img_enc_start_us = ggml_time_us();
-
     const char * mm_patch_merge_type = clip_patch_merge_type(ctx_clip);
 
     if (clip_is_minicpmv(ctx_clip) || clip_is_qwen2vl(ctx_clip)) {
@@ -282,13 +275,16 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
 
             if (!encoded) {
                 LOG_ERR("Unable to encode image - spatial_unpad - subimage %d of %d\n", (int) i+1, (int) img_res_v.size);
+                for (size_t j = 0; j <= i; j++) {
+                    free(image_embd_v[j]);
+                }
+                image_embd_v.clear();
+                free_img_res_v;
                 return false;
             }
             const int64_t t_img_enc_steop_batch_us = ggml_time_us();
             LOG_INF("%s: step %d of %d encoded in %8.2f ms\n", __func__, (int)i+1, (int)img_res_v.size, (t_img_enc_steop_batch_us - t_img_enc_step_start_us) / 1000.0);
         }
-        const int64_t t_img_enc_batch_us = ggml_time_us();
-        LOG_INF("%s: all %d segments encoded in %8.2f ms\n", __func__, (int)img_res_v.size, (t_img_enc_batch_us - t_img_enc_start_us) / 1000.0);
 
         int n_img_pos_out = 0;
         for (size_t i = 0; i < image_embd_v.size(); i++) {
@@ -299,19 +295,18 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
             n_img_pos_out += clip_n_patches_by_img(ctx_clip, &img_res_v.data[i]);
         }
         *n_img_pos = n_img_pos_out;
+        load_image_size->width = img->nx;
+        load_image_size->height = img->ny;
+        clip_add_load_image_size(ctx_clip, load_image_size);
         for (size_t i = 0; i < image_embd_v.size(); i++) {
             free(image_embd_v[i]);
         }
         image_embd_v.clear();
-        load_image_size->width = img->nx;
-        load_image_size->height = img->ny;
-        clip_add_load_image_size(ctx_clip, load_image_size);
-        LOG_INF("%s: load_image_size %d %d\n", __func__, load_image_size->width, load_image_size->height);
-        delete[] img_res_v.data;
-        img_res_v.size = 0;
-        img_res_v.data = nullptr;
+        free_img_res_v;
     }
     else if (clip_is_glm(ctx_clip)){
+        const int64_t t_img_enc_step_start_us = ggml_time_us();
+
         struct clip_image_size * load_image_size = clip_image_size_init();
         load_image_size->width = img_res_v.data[0].nx;
         load_image_size->height = img_res_v.data[0].ny;
@@ -320,37 +315,54 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
         bool encoded = clip_image_encode(ctx_clip, n_threads, &img_res_v.data[0], image_embd);
         int pos = int(load_image_size->width/clip_patch_size(ctx_clip)/2);
         *n_img_pos = (pos * pos + 2);
+        free_img_res_v;
         if (!encoded){
             LOG_ERR("Unable to encode image \n");
             return false;
         }
+
+        const int64_t t_img_enc_steop_batch_us = ggml_time_us();
+        LOG_INF("%s: step 1 of 1 encoded in %8.2f ms\n", __func__, (t_img_enc_steop_batch_us - t_img_enc_step_start_us) / 1000.0);
     }
     else if (strcmp(mm_patch_merge_type, "spatial_unpad") != 0) {
+        const int64_t t_img_enc_step_start_us = ggml_time_us();
+
         // flat / default llava-1.5 type embedding
         *n_img_pos = clip_n_patches(ctx_clip);
         bool encoded = clip_image_encode(ctx_clip, n_threads, &img_res_v.data[0], image_embd); // image_embd shape is 576 x 4096
-        delete[] img_res_v.data;
+        free_img_res_v;
         if (!encoded) {
             LOG_ERR("Unable to encode image\n");
-
             return false;
         }
+
+        const int64_t t_img_enc_steop_batch_us = ggml_time_us();
+        LOG_INF("%s: step 1 of 1 encoded in %8.2f ms\n", __func__, (t_img_enc_steop_batch_us - t_img_enc_step_start_us) / 1000.0);
     }
     else {
         // spatial_unpad llava-1.6 type embedding
         // TODO: CLIP needs batching support - in HF the llm projection is separate after encoding, which might be a solution to quickly get batching working
         std::vector<float *> image_embd_v;
         image_embd_v.resize(img_res_v.size);
-        for (size_t i = 0; i < img_res_v.size; i++) {
+        bool ok = true;
+        for (size_t i = 0; ok && i < img_res_v.size; i++) {
+            const int64_t t_img_enc_step_start_us = ggml_time_us();
+
             image_embd_v[i] = (float *)malloc(clip_embd_nbytes(ctx_clip)); // 576 patches * 4096 embeddings * 4 bytes = 9437184
             const bool encoded = clip_image_encode(ctx_clip, n_threads, &img_res_v.data[i], image_embd_v[i]); // image data is in 3x336x336 format and will be converted to 336x336x3 inside
             if (!encoded) {
-                LOG_ERR("Unable to encode image - spatial_unpad - subimage %d of %d\n", (int) i+1, (int) img_res_v.size);
+                LOG_ERR("unable to encode image - spatial_unpad - subimage %d of %d\n", (int) i+1, (int) img_res_v.size);
+                for (size_t j = 0; j <= i; j++) {
+                    free(image_embd_v[j]);
+                }
+                image_embd_v.clear();
+                free_img_res_v;
                 return false;
             }
+
+            const int64_t t_img_enc_steop_batch_us = ggml_time_us();
+            LOG_INF("%s: step %d of %d encoded in %8.2f ms\n", __func__, (int)i+1, (int)img_res_v.size, (t_img_enc_steop_batch_us - t_img_enc_step_start_us) / 1000.0);
         }
-        const int64_t t_img_enc_batch_us = ggml_time_us();
-        LOG_INF("%s: %d segments encoded in %8.2f ms\n", __func__, (int)img_res_v.size, (t_img_enc_batch_us - t_img_enc_start_us) / 1000.0);
 
         const int32_t * image_grid = clip_image_grid(ctx_clip);
 
@@ -359,11 +371,6 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
             grid_pinpoints.push_back({image_grid[i], image_grid[i+1]});
         }
 
-        // free all img_res_v - not needed anymore
-        delete[] img_res_v.data;
-        img_res_v.size = 0;
-        img_res_v.data = nullptr;
-
         const int32_t image_size = clip_image_size(ctx_clip);
 
         struct clip_image_grid_shape grid_shape = get_anyres_image_grid_shape({img->nx,img->ny}, grid_pinpoints, image_size);
@@ -376,6 +383,7 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
             free(image_embd_v[i]);
         }
         image_embd_v.clear();
+        free_img_res_v;
 
         // debug image/segment/normalization content:
         // clip_image_u8 * tmp = clip_image_u8_init();
@@ -385,11 +393,6 @@ static bool encode_image_with_clip(clip_ctx * ctx_clip, int n_threads, const cli
 
     LOG_INF("%s: image embedding created: %d tokens\n", __func__, *n_img_pos);
 
-    const int64_t t_img_enc_end_us = ggml_time_us();
-    float t_img_enc_ms = (t_img_enc_end_us - t_img_enc_start_us) / 1000.0;
-
-    LOG_INF("\n%s: image encoded in %8.2f ms by CLIP (%8.2f ms per image patch)\n", __func__, t_img_enc_ms, t_img_enc_ms / *n_img_pos);
-
     return true;
 }
 
@@ -512,6 +515,30 @@ struct llava_image_embed * llava_image_embed_make_with_bytes(struct clip_ctx * c
     return result;
 }
 
+struct llava_image_embed * llava_image_embed_make_with_data(struct clip_ctx * ctx_clip, int n_threads, const unsigned char * data, int nx, int ny, int nc) {
+    clip_image_u8 * img = clip_image_u8_init();
+    img->nx = nx;
+    img->ny = ny;
+    img->buf.resize(nx * ny * nc);
+    LOG_INFV(4, "%s: image size: %.2fKiB, nx: %d, ny: %d, nc: %d\n", __func__, float(img->buf.size())/1024, nx, ny, nc);
+    memcpy(img->buf.data(), data, img->buf.size());
+
+    float* image_embed = NULL;
+    int n_image_pos = 0;
+    bool image_embed_result = llava_image_embed_make_with_clip_img(ctx_clip, n_threads, img, &image_embed, &n_image_pos);
+    if (!image_embed_result) {
+        clip_image_u8_free(img);
+        LOG_ERR("%s: couldn't embed the image\n", __func__);
+        return NULL;
+    }
+
+    clip_image_u8_free(img);
+    auto *result = (llava_image_embed*)malloc(sizeof(llava_image_embed));
+    result->embed = image_embed;
+    result->n_image_pos = n_image_pos;
+    return result;
+}
+
 static bool load_file_to_bytes(const char* path, unsigned char** bytesOut, long *sizeOut) {
     auto file = fopen(path, "rb");
     if (file == NULL) {
diff --git a/examples/llava/llava.h b/examples/llava/llava.h
index b6feb302..7b29b7a0 100644
--- a/examples/llava/llava.h
+++ b/examples/llava/llava.h
@@ -34,6 +34,8 @@ LLAVA_API bool llava_image_embed_make_with_clip_img(struct clip_ctx * ctx_clip,
 
 /** build an image embed from image file bytes */
 LLAVA_API struct llava_image_embed * llava_image_embed_make_with_bytes(struct clip_ctx * ctx_clip, int n_threads, const unsigned char * image_bytes, int image_bytes_length);
+/** build an image embed from image data */
+LLAVA_API struct llava_image_embed * llava_image_embed_make_with_data(struct clip_ctx * ctx_clip, int n_threads, const unsigned char * data, int nx, int ny, int nc);
 /** build an image embed from a path to an image filename */
 LLAVA_API struct llava_image_embed * llava_image_embed_make_with_filename(struct clip_ctx * ctx_clip, int n_threads, const char * image_path);
 /** free an embedding made with llava_image_embed_make_* */
