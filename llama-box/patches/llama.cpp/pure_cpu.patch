diff --git a/ggml/src/ggml-backend-impl.h b/ggml/src/ggml-backend-impl.h
index c36c12d6..1b1dc734 100644
--- a/ggml/src/ggml-backend-impl.h
+++ b/ggml/src/ggml-backend-impl.h
@@ -207,6 +207,9 @@ extern "C" {
     };
 
     // Internal backend registry API
+    static int ngl = -1;
+
+    GGML_API void ggml_backend_register_metadata_set(int ngl_);
     GGML_API void ggml_backend_register(ggml_backend_reg_t reg);
 
     // Add backend dynamic loading support to the backend
diff --git a/ggml/src/ggml-backend-reg.cpp b/ggml/src/ggml-backend-reg.cpp
index 2d93771f..1ab62007 100644
--- a/ggml/src/ggml-backend-reg.cpp
+++ b/ggml/src/ggml-backend-reg.cpp
@@ -166,22 +166,34 @@ struct ggml_backend_registry {
 
     ggml_backend_registry() {
 #ifdef GGML_USE_CUDA
-        register_backend(ggml_backend_cuda_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_cuda_reg());
+        }
 #endif
 #ifdef GGML_USE_METAL
-        register_backend(ggml_backend_metal_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_metal_reg());
+        }
 #endif
 #ifdef GGML_USE_SYCL
-        register_backend(ggml_backend_sycl_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_sycl_reg());
+        }
 #endif
 #ifdef GGML_USE_VULKAN
-        register_backend(ggml_backend_vk_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_vk_reg());
+        }
 #endif
 #ifdef GGML_USE_OPENCL
-        register_backend(ggml_backend_opencl_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_opencl_reg());
+        }
 #endif
 #ifdef GGML_USE_CANN
-        register_backend(ggml_backend_cann_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_cann_reg());
+        }
 #endif
 #ifdef GGML_USE_BLAS
         register_backend(ggml_backend_blas_reg());
@@ -190,7 +202,9 @@ struct ggml_backend_registry {
         register_backend(ggml_backend_rpc_reg());
 #endif
 #ifdef GGML_USE_KOMPUTE
-        register_backend(ggml_backend_kompute_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_kompute_reg());
+        }
 #endif
 #ifdef GGML_USE_CPU
         register_backend(ggml_backend_cpu_reg());
@@ -307,6 +321,10 @@ static ggml_backend_registry & get_reg() {
 }
 
 // Internal API
+void ggml_backend_register_metadata_set(int ngl_) {
+    ngl = ngl_;
+}
+
 void ggml_backend_register(ggml_backend_reg_t reg) {
     get_reg().register_backend(reg);
 }
diff --git a/src/llama.cpp b/src/llama.cpp
index 34906cdb..db75b00c 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -197,7 +197,7 @@ static struct llama_model * llama_model_load_from_file_impl(
     }
 
     // if using single GPU mode, remove all except the main GPU
-    if (params.split_mode == LLAMA_SPLIT_MODE_NONE) {
+    if (model->devices.size() > 0 && params.split_mode == LLAMA_SPLIT_MODE_NONE) {
         if (params.main_gpu < 0) {
             model->devices.clear();
         } else {
