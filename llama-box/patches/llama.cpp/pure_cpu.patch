diff --git a/ggml/src/ggml-backend-impl.h b/ggml/src/ggml-backend-impl.h
index 36d72e95..4809627b 100644
--- a/ggml/src/ggml-backend-impl.h
+++ b/ggml/src/ggml-backend-impl.h
@@ -207,6 +207,9 @@ extern "C" {
     };
 
     // Internal backend registry API
+    static int ngl = -1;
+
+    GGML_API void ggml_backend_register_metadata_set(int ngl_);
     GGML_API void ggml_backend_register(ggml_backend_reg_t reg);
     GGML_API void ggml_backend_device_register(ggml_backend_dev_t device);
 
diff --git a/ggml/src/ggml-backend-reg.cpp b/ggml/src/ggml-backend-reg.cpp
index 955ed505..1caffd34 100644
--- a/ggml/src/ggml-backend-reg.cpp
+++ b/ggml/src/ggml-backend-reg.cpp
@@ -154,22 +154,34 @@ struct ggml_backend_registry {
 
     ggml_backend_registry() {
 #ifdef GGML_USE_CUDA
-        register_backend(ggml_backend_cuda_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_cuda_reg());
+        }
 #endif
 #ifdef GGML_USE_METAL
-        register_backend(ggml_backend_metal_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_metal_reg());
+        }
 #endif
 #ifdef GGML_USE_SYCL
-        register_backend(ggml_backend_sycl_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_sycl_reg());
+        }
 #endif
 #ifdef GGML_USE_VULKAN
-        register_backend(ggml_backend_vk_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_vk_reg());
+        }
 #endif
 #ifdef GGML_USE_OPENCL
-        register_backend(ggml_backend_opencl_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_opencl_reg());
+        }
 #endif
 #ifdef GGML_USE_CANN
-        register_backend(ggml_backend_cann_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_cann_reg());
+        }
 #endif
 #ifdef GGML_USE_BLAS
         register_backend(ggml_backend_blas_reg());
@@ -178,7 +190,9 @@ struct ggml_backend_registry {
         register_backend(ggml_backend_rpc_reg());
 #endif
 #ifdef GGML_USE_KOMPUTE
-        register_backend(ggml_backend_kompute_reg());
+        if (ngl != 0) {
+            register_backend(ggml_backend_kompute_reg());
+        }
 #endif
 #ifdef GGML_USE_CPU
         register_backend(ggml_backend_cpu_reg());
@@ -294,6 +308,10 @@ static ggml_backend_registry & get_reg() {
 }
 
 // Internal API
+void ggml_backend_register_metadata_set(int ngl_) {
+    ngl = ngl_;
+}
+
 void ggml_backend_register(ggml_backend_reg_t reg) {
     get_reg().register_backend(reg);
 }
diff --git a/src/llama.cpp b/src/llama.cpp
index 2e391b3b..89f72886 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -9468,7 +9468,7 @@ struct llama_model * llama_model_load_from_file(
     }
 
     // if using single GPU mode, remove all except the main GPU
-    if (params.split_mode == LLAMA_SPLIT_MODE_NONE) {
+    if (model->devices.size() > 0 && params.split_mode == LLAMA_SPLIT_MODE_NONE) {
         if (params.main_gpu < 0 || params.main_gpu >= (int)model->devices.size()) {
             LLAMA_LOG_ERROR("%s: invalid value for main_gpu: %d (available devices: %d)\n", __func__, params.main_gpu, (int)model->devices.size());
             llama_model_free(model);
