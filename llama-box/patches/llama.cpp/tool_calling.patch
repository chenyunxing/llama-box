diff --git a/common/common.cpp b/common/common.cpp
index a6f9252b..ffdc5388 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -1736,13 +1736,15 @@ std::string common_get_builtin_chat_template(const struct llama_model * model) {
 
 bool common_chat_verify_template(const std::string & tmpl) {
     llama_chat_message chat[] = {{"user", "test"}};
-    const int res = llama_chat_apply_template(tmpl.c_str(), chat, 1, true, nullptr, 0);
+    const int res = llama_chat_apply_template(nullptr, tmpl.c_str(), chat, 1, nullptr, 0, false, true, nullptr, 0);
     return res >= 0;
 }
 
 std::string common_chat_apply_template(const struct llama_model * model,
         const std::string & tmpl,
         const std::vector<common_chat_msg> & msgs,
+        const std::vector<common_chat_func> & funcs,
+        bool req_func,
         bool add_ass) {
     int alloc_size = 0;
     bool fallback = false; // indicate if we must fallback to default chatml
@@ -1751,12 +1753,17 @@ std::string common_chat_apply_template(const struct llama_model * model,
         chat.push_back({msg.role.c_str(), msg.content.c_str()});
         alloc_size += (msg.role.size() + msg.content.size()) * 1.25;
     }
+    std::vector<llama_chat_function> func;
+    for (const auto & fn: funcs) {
+        func.push_back({fn.name.c_str(), fn.description.c_str(), fn.parameter.c_str()});
+        alloc_size += (fn.name.size() + fn.description.size() + fn.parameter.size()) * 1.25;
+    }
 
     const char * ptr_tmpl = tmpl.empty() ? llama_model_chat_template(model) : tmpl.c_str();
     std::vector<char> buf(alloc_size);
 
     // run the first time to get the total output length
-    int32_t res = llama_chat_apply_template(ptr_tmpl, chat.data(), chat.size(), add_ass, buf.data(), buf.size());
+    int32_t res = llama_chat_apply_template(model, ptr_tmpl, chat.data(), chat.size(), func.data(), func.size(), req_func, add_ass, buf.data(), buf.size());
 
     // error: chat template is not supported
     if (res < 0) {
@@ -1767,7 +1774,7 @@ std::string common_chat_apply_template(const struct llama_model * model,
         }
 
         // If the built-in template is not supported, we default to chatml
-        res = llama_chat_apply_template("chatml", chat.data(), chat.size(), add_ass, buf.data(), buf.size());
+        res = llama_chat_apply_template(model, "chatml", chat.data(), chat.size(), func.data(), func.size(), req_func, add_ass, buf.data(), buf.size());
         fallback = true;
     }
 
@@ -1775,8 +1782,9 @@ std::string common_chat_apply_template(const struct llama_model * model,
     if ((size_t) res > buf.size()) {
         buf.resize(res);
         res = llama_chat_apply_template(
+            model,
             fallback ? "chatml" : ptr_tmpl,
-            chat.data(), chat.size(), add_ass, buf.data(), buf.size());
+            chat.data(), chat.size(), func.data(), func.size(), req_func, add_ass, buf.data(), buf.size());
     }
 
     std::string formatted_chat(buf.data(), res);
@@ -1789,7 +1797,7 @@ std::string common_chat_format_single(const struct llama_model * model,
         const common_chat_msg & new_msg,
         bool add_ass) {
     std::ostringstream ss;
-    auto fmt_past_msg = past_msg.empty() ? "" : common_chat_apply_template(model, tmpl, past_msg, false);
+    auto fmt_past_msg = past_msg.empty() ? "" : common_chat_apply_template(model, tmpl, past_msg, {}, false, false);
     std::vector<common_chat_msg> chat_new(past_msg);
     // if the past_msg ends with a newline, we must preserve it in the formatted version
     if (add_ass && !fmt_past_msg.empty() && fmt_past_msg.back() == '\n') {
@@ -1797,7 +1805,7 @@ std::string common_chat_format_single(const struct llama_model * model,
     };
     // format chat with new_msg
     chat_new.push_back(new_msg);
-    auto fmt_new_msg = common_chat_apply_template(model, tmpl, chat_new, add_ass);
+    auto fmt_new_msg = common_chat_apply_template(model, tmpl, chat_new, {}, false, add_ass);
     // get the diff part
     ss << fmt_new_msg.substr(fmt_past_msg.size(), fmt_new_msg.size() - fmt_past_msg.size());
     return ss.str();
@@ -1806,12 +1814,16 @@ std::string common_chat_format_single(const struct llama_model * model,
 std::string common_chat_format_example(const struct llama_model * model,
         const std::string & tmpl) {
     std::vector<common_chat_msg> msgs = {
-        {"system",    "You are a helpful assistant"},
-        {"user",      "Hello"},
-        {"assistant", "Hi there"},
-        {"user",      "How are you?"},
+        {"system",    "You are a helpful assistant."},
+        {"user",      "Hello."},
+        {"assistant", "Hi there."},
+        {"user",      "What's the weather like in Paris today?"},
+    };
+    std::vector<common_chat_func> funcs = {
+        {"get_weather", "", "{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\"}}}"},
+        {"get_temperature", "Return the temperature according to the location.", "{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\"}}}"},
     };
-    return common_chat_apply_template(model, tmpl, msgs, true);
+    return common_chat_apply_template(model, tmpl, msgs, funcs, false, true);
 }
 
 //
diff --git a/common/common.h b/common/common.h
index c86a4ef3..14acb5ad 100644
--- a/common/common.h
+++ b/common/common.h
@@ -583,6 +583,13 @@ std::string common_detokenize(
 // Chat template utils
 //
 
+// same with llama_chat_function, but uses std::string
+struct common_chat_func {
+    std::string name;
+    std::string description;
+    std::string parameter;
+};
+
 // same with llama_chat_message, but uses std::string
 struct common_chat_msg {
     std::string role;
@@ -601,6 +608,8 @@ bool common_chat_verify_template(const std::string & tmpl);
 std::string common_chat_apply_template(const struct llama_model * model,
         const std::string & tmpl,
         const std::vector<common_chat_msg> & chat,
+        const std::vector<common_chat_func> & tools,
+        bool req_func,
         bool add_ass);
 
 // Format single message, while taking into account the position of that message in chat history
diff --git a/include/llama.h b/include/llama.h
index a184884c..a519bf58 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -383,6 +383,11 @@ extern "C" {
         const char * role;
         const char * content;
     } llama_chat_message;
+    typedef struct llama_chat_function {
+        const char * name;
+        const char * description;
+        const char * parameters;
+    } llama_chat_function;
 
     // lora adapter
     struct llama_adapter_lora;
@@ -1036,18 +1041,28 @@ extern "C" {
     /// @param tmpl A Jinja template to use for this chat. If this is nullptr, the modelâ€™s default chat template will be used instead.
     /// @param chat Pointer to a list of multiple llama_chat_message
     /// @param n_msg Number of llama_chat_message in this chat
+    /// @param func Pointer to a list of multiple llama_chat_function (In JSON format)
+    /// @param n_func Number of llama_chat_function in this chat
+    /// @param req_func Whether to must call function call.
     /// @param add_ass Whether to end the prompt with the token(s) that indicate the start of an assistant message.
     /// @param buf A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)
     /// @param length The size of the allocated buffer
     /// @return The total number of bytes of the formatted prompt. If is it larger than the size of buffer, you may need to re-alloc it and then re-apply the template.
     LLAMA_API int32_t llama_chat_apply_template(
+              const struct llama_model * model,
                             const char * tmpl,
        const struct llama_chat_message * chat,
                                 size_t   n_msg,
+      const struct llama_chat_function * func,
+                                size_t   n_func,
+                                  bool   req_func,
                                   bool   add_ass,
                                   char * buf,
                                int32_t   length);
 
+    // Get chat template alias
+    LLAMA_API const char * llama_chat_template_alias(const char * tmpl);
+
     // Get list of built-in chat templates
     LLAMA_API int32_t llama_chat_builtin_templates(const char ** output, size_t len);
 
diff --git a/src/llama-chat.cpp b/src/llama-chat.cpp
index 1347ec15..c66070c2 100644
--- a/src/llama-chat.cpp
+++ b/src/llama-chat.cpp
@@ -173,14 +173,114 @@ llm_chat_template llm_chat_detect_template(const std::string & tmpl) {
 // Simple version of "llama_apply_chat_template" that only works with strings
 // This function uses heuristic checks to determine commonly used template. It is not a jinja parser.
 int32_t llm_chat_apply_template(
+    llm_arch arch,
     llm_chat_template tmpl,
     const std::vector<const llama_chat_message *> & chat,
-    std::string & dest, bool add_ass) {
+    const std::vector<const llama_chat_function * > & func,
+    std::string & dest, bool req_func, bool add_ass) {
     // Taken from the research: https://github.com/ggerganov/llama.cpp/issues/5527
     std::stringstream ss;
     if (tmpl == LLM_CHAT_TEMPLATE_CHATML) {
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "<|im_start|>system\n";
+            if (root_msg) {
+                ss << root_msg->content << "\n\n";
+            } else {
+                ss << "You are a helpful assistant.\n\n";
+            }
+            if (arch == LLM_ARCH_QWEN2VL) {
+                ss << "## Tools\n\n";
+            } else {
+                ss << "# Tools\n\n";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions.\n\n";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions.\n\n";
+            }
+            if (arch == LLM_ARCH_QWEN2VL) {
+                ss << "You are provided with following function tools:\n\n";
+                for (const auto & fn : func) {
+                    ss << "### " << fn->name << "\n\n";
+                    ss << fn->name << ": " << fn->description << " Parameters: " << fn->parameters << "Format the arguments as a JSON object.\n\n";
+                }
+                if (req_func) {
+                    ss << "For each function call, return a JSON object with function name and arguments within <tool_call></tool_call> XML tags:\n";
+                } else {
+                    ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                    ss << "Otherwise, for each function call, return a JSON object with function name and arguments within <tool_call></tool_call> XML tags:\n";
+                }
+                ss << "<tool_call>\n";
+                ss << "{\"name\": The name of the function to use, \"arguments\": The input of the function, must be a JSON object in compact format}\n";
+                ss << "</tool_call>\n";
+                ss << "<tool_result>\n";
+                ss << "The function results.\n";
+                ss << "</tool_result>\n";
+                ss << "Reply based on the function results." << "<|im_end|>\n";
+            } else {
+                ss << "You are provided with following function signatures within <tools></tools> XML tags:\n";
+                ss << "<tools>\n";
+                for (const auto & fn : func) {
+                    ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}\n";
+                }
+                ss << "</tools>\n\n";
+                if (req_func) {
+                    ss << "For each function call, return a JSON object with function name and arguments within <tool_call></tool_call> XML tags:\n";
+                } else {
+                    ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                    ss << "Otherwise, for each function call, return a JSON object with function name and arguments within <tool_call></tool_call> XML tags:\n";
+                }
+                ss << "<tool_call>\n";
+                ss << "{\"name\": <function-name>, \"arguments\": <arguments-json-object>}\n";
+                ss << "</tool_call>" << "<|im_end|>\n";
+            }
+        }
+        bool previous_tool_response = false;
         // chatml template
         for (auto message : chat) {
+            if (!func.empty()) {
+                std::string role(message->role);
+                if (role == "assistant_tool_call") {
+                    if (arch == LLM_ARCH_QWEN2VL) {
+                        if (!previous_tool_response) {
+                            ss << "<|im_start|>assistant\n";
+                        }
+                        ss << "<tool_call>\n" << message->content << "\n</tool_call>\n";
+                    } else {
+                        ss << "<|im_start|>assistant\n";
+                        ss << "<tool_call>\n" << message->content << "\n</tool_call>";
+                        ss << "<|im_end|>\n";
+                    }
+                    previous_tool_response = false;
+                    continue;
+                }
+                previous_tool_response = false;
+                if (role == "system") {
+                    continue;
+                }
+                if (role == "tool") {
+                    if (arch == LLM_ARCH_QWEN2VL) {
+                        ss << "<tool_result>\n" << message->content << "\n</tool_result>\n";
+                        add_ass = message != chat.back();
+                    } else {
+                        ss << "<|im_start|>user\n" << message->content << "<|im_end|>\n";
+                    }
+                    previous_tool_response = true;
+                    continue;
+                }
+                if (role == "assistant" && arch == LLM_ARCH_QWEN2VL) {
+                    ss << message->content << "<|im_end|>\n";
+                    continue;
+                }
+            }
             ss << "<|im_start|>" << message->role << "\n" << message->content << "<|im_end|>\n";
         }
         if (add_ass) {
@@ -189,15 +289,59 @@ int32_t llm_chat_apply_template(
     } else if (tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V7) {
         // Official mistral 'v7' template
         // See: https://huggingface.co/mistralai/Mistral-Large-Instruct-2411#basic-instruct-template-v7
+        // See: https://github.com/mistralai/mistral-common/releases/tag/v1.5.0
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "[AVAILABLE_TOOLS] " << "[";
+            for (const auto & fn : func) {
+                ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}";
+                ss << ((fn == func.back()) ? "" : ",");
+            }
+            ss << "]" << "[/AVAILABLE_TOOLS]";
+            if (root_msg) {
+                ss << "[SYSTEM_PROMPT] " << root_msg->content;
+            } else {
+                ss << "[SYSTEM_PROMPT] " << "You are a helpful assistant. ";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions. ";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions. ";
+                ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                ss << "Otherwise, try to call functions to assist with the user query. ";
+            }
+            ss << "[/SYSTEM_PROMPT]";
+        }
         for (auto message : chat) {
             std::string role(message->role);
             std::string content(message->content);
             if (role == "system") {
+                if (!func.empty()) {
+                    continue;
+                }
                 ss << "[SYSTEM_PROMPT] " << content << "[/SYSTEM_PROMPT]";
             } else if (role == "user") {
                 ss << "[INST] " << content << "[/INST]";
             }
             else {
+                if (!func.empty()) {
+                    if (role == "assistant_tool_call") {
+                        ss << "[TOOL_CALLS] ";
+                        ss << "[" << message->content << "]</s>";
+                        continue;
+                    }
+                    if (role == "tool") {
+                        ss << "[TOOL_RESULTS] " << message->content << "[/TOOL_RESULTS]";
+                        continue;
+                    }
+                }
                 ss << " " << content << "</s>";
             }
         }
@@ -206,10 +350,41 @@ int32_t llm_chat_apply_template(
             || tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V3_TEKKEN) {
         // See: https://github.com/mistralai/cookbook/blob/main/concept-deep-dive/tokenization/chat_templates.md
         // See: https://github.com/mistralai/cookbook/blob/main/concept-deep-dive/tokenization/templates.md
+        // See: https://github.com/mistralai/cookbook/blob/main/concept-deep-dive/tokenization/tool_calling.md
         std::string leading_space = tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V1 ? " " : "";
         std::string trailing_space = tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V3_TEKKEN ? "" : " ";
         bool trim_assistant_message = tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V3;
         bool is_inside_turn = false;
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << leading_space << "[AVAILABLE_TOOLS]" << trailing_space << "[";
+            for (const auto & fn : func) {
+                ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}";
+                ss << ((fn == func.back()) ? "" : ",");
+            }
+            ss << "]" << leading_space << "[/AVAILABLE_TOOLS]";
+            ss << leading_space << "[INST]" << trailing_space;
+            is_inside_turn = true;
+            if (root_msg) {
+                ss << root_msg->content << "\n\n";
+            } else {
+                ss << "You are a helpful assistant.\n\n";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions.\n\n";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions. ";
+                ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                ss << "Otherwise, try to call functions to assist with the user query.\n\n";
+            }
+        }
         for (auto message : chat) {
             if (!is_inside_turn) {
                 ss << leading_space << "[INST]" << trailing_space;
@@ -218,10 +393,26 @@ int32_t llm_chat_apply_template(
             std::string role(message->role);
             std::string content(message->content);
             if (role == "system") {
+                if (!func.empty()) {
+                    continue;
+                }
                 ss << content << "\n\n";
             } else if (role == "user") {
                 ss << content << leading_space << "[/INST]";
             } else {
+                if (!func.empty()) {
+                    if (role == "assistant_tool_call") {
+                        ss << leading_space << "[TOOL_CALLS]" << trailing_space;
+                        ss << "[" << message->content << "]</s>";
+                        continue;
+                    }
+                    if (role == "tool") {
+                        ss << leading_space << "[TOOL_RESULTS]" << trailing_space;
+                        ss << message->content;
+                        ss << leading_space << "[/TOOL_RESULTS]";
+                        continue;
+                    }
+                }
                 ss << trailing_space << (trim_assistant_message ? trim(content) : content) << "</s>";
                 is_inside_turn = false;
             }
@@ -413,9 +604,61 @@ int32_t llm_chat_apply_template(
             ss << "<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>";
         }
     } else if (tmpl == LLM_CHAT_TEMPLATE_LLAMA_3) {
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "<|start_header_id|>system<|end_header_id|>\n\n";
+            if (root_msg) {
+                ss << trim(root_msg->content) << "\n\n";
+            } else {
+                ss << "You are a helpful assistant.\n\n";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions.";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions.";
+            }
+            ss << "<|eot_id|>";
+        }
         // Llama 3
         for (auto message : chat) {
             std::string role(message->role);
+            if (!func.empty()) {
+                if (role == "system") {
+                    continue;
+                }
+                if (role == "assistant_tool_call") {
+                    ss << "<|start_header_id|>assistant<|end_header_id|>\n\n" << trim(message->content) << "<|eot_id|>";
+                    continue;
+                }
+                if (role == "tool") {
+                    ss << "<|start_header_id|>ipython<|end_header_id|>\n\n" << trim(message->content) << "<|eot_id|>";
+                    continue;
+                }
+                if (role == "user" && message == chat.back()) {
+                    ss << "<|start_header_id|>user<|end_header_id|>\n\n";
+                    ss << "You are provided with following function signatures within <tools></tools> XML tags:\n";
+                    ss << "<tools>\n";
+                    for (const auto & fn : func) {
+                        ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}\n";
+                    }
+                    ss << "</tools>\n\n";
+                    if (req_func) {
+                        ss << "For each function call, return a JSON object with function name and arguments in the format {\"name\": <function-name>, \"arguments\": <arguments-json-object>}.\n";
+                    } else {
+                        ss << "When you can reply with your internal knowledge, reply directly without any function call. ";
+                        ss << "Otherwise, for each function call, return a JSON object with function name and arguments in the format {\"name\": <function-name>, \"arguments\": <arguments-json-object>}.\n";
+                    }
+                    ss << trim(message->content) << "<|eot_id|>";
+                    continue;
+                }
+            }
             ss << "<|start_header_id|>" << role << "<|end_header_id|>\n\n" << trim(message->content) << "<|eot_id|>";
         }
         if (add_ass) {
@@ -433,12 +676,80 @@ int32_t llm_chat_apply_template(
         }
     } else if (tmpl == LLM_CHAT_TEMPLATE_CHATGML_4) {
         ss << "[gMASK]" << "<sop>";
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "<|system|>\n";
+            if (root_msg) {
+                ss << root_msg -> content << "\n";
+            } else {
+                ss << "You are a helpful assistant.\n";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions.\n";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions.\n";
+            }
+            ss << "# Functions\n";
+            ss << "You are provided with following functions:\n";
+            for (size_t i = 0; i < func.size(); i++) {
+                const llama_chat_function *fn = func[i];
+                ss << "## Function " << i << "\n";
+                ss << "### Name\n" << fn->name << "\n";
+                ss << "### Description\n" << fn->description << "\n";
+                ss << "### Parameters\n" << fn->parameters << "\n";
+            }
+            if (req_func) {
+                ss << "For each function call, just generate an answer, no explanation before or after your answer, and MUST return a JSON object with function name and arguments within <tool_call></tool_call> XML tags:\n";
+            } else {
+                ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                ss << "Otherwise, for each function call, just generate an answer, no explanation before or after your answer, and MUST return a JSON object with function name and arguments within <tool_call></tool_call> XML tags:\n";
+            }
+            ss << "<tool_call>\n";
+            ss << "{\"name\": The name of the function to use, \"arguments\": The input of the function, must be a JSON object in compact format}\n";
+            ss << "</tool_call>\n";
+            ss << "<tool_result>\n";
+            ss << "The function results.\n";
+            ss << "</tool_result>\n";
+            ss << "Reply based on the function results.\n";
+        }
+        bool previous_tool_response = false;
         for (auto message : chat) {
             std::string role(message->role);
+            if (!func.empty()) {
+                if (role == "assistant_tool_call") {
+                    if (!previous_tool_response) {
+                        ss << "<|assistant|>\n";
+                    }
+                    ss << "<tool_call>\n" << message->content << "\n</tool_call>\n";
+                    previous_tool_response = false;
+                    continue;
+                }
+                previous_tool_response = false;
+                if (role == "system") {
+                    continue;
+                }
+                if (role == "tool") {
+                    ss << "<tool_result>\n" << message->content << "\n</tool_result>\n";
+                    add_ass = message != chat.back();
+                    previous_tool_response = true;
+                    continue;
+                }
+                if (role == "assistant") {
+                    ss << "<|assistant|>\n" << message->content;
+                    continue;
+                }
+            }
             ss << "<|" << role << "|>" << "\n" << message->content;
         }
         if (add_ass) {
-            ss << "<|assistant|>";
+            ss << "<|assistant|>\n";
         }
     } else if (tmpl == LLM_CHAT_TEMPLATE_MINICPM) {
         // MiniCPM-3B-OpenHermes-2.5-v2-GGUF
@@ -509,13 +820,59 @@ int32_t llm_chat_apply_template(
             }
         }
     } else if (tmpl == LLM_CHAT_TEMPLATE_GRANITE) {
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "<|start_of_role|>tools<|end_of_role|>[";
+            for (const auto & fn : func) {
+                ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}";
+                ss << ((fn == func.back()) ? "" : ",");
+            }
+            ss << "]<|end_of_text|>\n";
+            ss << "<|start_of_role|>system<|end_of_role|>";
+            if (root_msg) {
+                ss << trim(root_msg->content) << " ";
+            } else {
+                ss << "You are a helpful assistant with tool calling capabilities. ";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more tools to assist with the user query. Do not make assumptions about what values to plug into tools. ";
+            } else {
+                ss << "You CAN call tools to assist with the user query. Do not make assumptions about what values to plug into tools. ";
+            }
+            if (req_func) {
+                ss << "For each tool call, return <|tool_call|><tool_call> followed by a JSON list of tool used as follows: ";
+            } else {
+                ss << "When you can reply with your internal knowledge, reply directly without any tool calls. ";
+                ss << "Otherwise, for each tool call, return <|tool_call|><tool_call> followed by a JSON list of tool used as follows: ";
+            }
+            ss << R"(<|tool_call|><tool_call>[{"name": <function-name>, "arguments": <arguments-json-object>}])";
+            ss << "Write the response to the user's input by strictly aligning with the facts in the provided documents.";
+            ss << "<|end_of_text|>\n";
+        }
         // IBM Granite template
         for (const auto & message : chat) {
             std::string role(message->role);
-            ss << "<|start_of_role|>" << role << "<|end_of_role|>";
-            if (role == "assistant_tool_call") {
-                ss << "<|tool_call|>";
+            if (!func.empty()) {
+                if (role == "system") {
+                    continue;
+                }
+                if (role == "assistant_tool_call") {
+                    ss << "<|start_of_role|>assistant<|start_of_role|><|tool_call|><tool_call>" << message->content << "<|end_of_text|>\n";
+                    continue;
+                }
+                if (role == "tool") {
+                    ss << "<|start_of_role|>tool_response<|end_of_role|>" << message->content << "<|end_of_text|>\n";
+                    continue;
+                }
             }
+            ss << "<|start_of_role|>" << role << "<|end_of_role|>";
             ss << message->content << "<|end_of_text|>\n";
         }
         if (add_ass) {
@@ -567,6 +924,17 @@ int32_t llm_chat_apply_template(
 
 // public interface
 
+const char * llama_chat_template_alias(const char * tmpl) {
+    llm_chat_template t = llm_chat_detect_template(std::string(tmpl));
+    for (const auto & it : LLM_CHAT_TEMPLATES) {
+        if (it.second != t) {
+            continue;
+        }
+        return it.first.c_str();
+    }
+    return "unknown";
+}
+
 int32_t llama_chat_builtin_templates(const char ** output, size_t len) {
     auto it = LLM_CHAT_TEMPLATES.begin();
     for (size_t i = 0; i < std::min(len, LLM_CHAT_TEMPLATES.size()); i++) {
diff --git a/src/llama-chat.h b/src/llama-chat.h
index 3a4d07ce..527e9334 100644
--- a/src/llama-chat.h
+++ b/src/llama-chat.h
@@ -4,6 +4,8 @@
 #include <vector>
 #include <cstdint>
 
+#include "llama-arch.h"
+
 enum llm_chat_template {
     LLM_CHAT_TEMPLATE_CHATML,
     LLM_CHAT_TEMPLATE_LLAMA_2,
@@ -42,11 +44,15 @@ enum llm_chat_template {
 
 struct llama_chat_message;
 
+struct llama_chat_function;
+
 llm_chat_template llm_chat_template_from_str(const std::string & name);
 
 llm_chat_template llm_chat_detect_template(const std::string & tmpl);
 
 int32_t llm_chat_apply_template(
+    llm_arch arch,
     llm_chat_template tmpl,
     const std::vector<const llama_chat_message *> & chat,
-    std::string & dest, bool add_ass);
+    const std::vector<const llama_chat_function * > & func,
+    std::string & dest, bool req_func, bool add_ass);
diff --git a/src/llama.cpp b/src/llama.cpp
index daf1b7c9..5c7fe969 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -9960,9 +9960,13 @@ int32_t llama_decode(
 //
 
 int32_t llama_chat_apply_template(
+                const struct llama_model * model,
                               const char * tmpl,
          const struct llama_chat_message * chat,
                                   size_t   n_msg,
+        const struct llama_chat_function * func,
+                                  size_t   n_func,
+                                    bool   req_func,
                                     bool   add_ass,
                                     char * buf,
                                  int32_t   length) {
@@ -9975,12 +9979,19 @@ int32_t llama_chat_apply_template(
         chat_vec[i] = &chat[i];
     }
 
+    // format the func to string
+    std::vector<const llama_chat_function *> func_vec;
+    func_vec.resize(n_func);
+    for (size_t i = 0; i < n_func; i++) {
+        func_vec[i] = &func[i];
+    }
+
     std::string formatted_chat;
     llm_chat_template detected_tmpl = llm_chat_detect_template(curr_tmpl);
     if (detected_tmpl == LLM_CHAT_TEMPLATE_UNKNOWN) {
         return -1;
     }
-    int32_t res = llm_chat_apply_template(detected_tmpl, chat_vec, formatted_chat, add_ass);
+    int32_t res = llm_chat_apply_template(model ? model->arch : LLM_ARCH_LLAMA, detected_tmpl, chat_vec, func_vec, formatted_chat, req_func, add_ass);
     if (res < 0) {
         return res;
     }
