diff --git a/common/chat.cpp b/common/chat.cpp
index bbc5f087..08207be6 100644
--- a/common/chat.cpp
+++ b/common/chat.cpp
@@ -347,6 +347,13 @@ void common_chat_templates_free(struct common_chat_templates * tmpls) {
     delete tmpls;
 }
 
+bool common_chat_templates_supports_tool_calls(const struct common_chat_templates * tmpls) {
+    const auto & tmpl = tmpls->template_tool_use
+                            ? *tmpls->template_tool_use
+                            : *tmpls->template_default;
+    return tmpl.original_caps().supports_tool_calls;
+}
+
 bool common_chat_templates_was_explicit(const struct common_chat_templates * tmpls) {
     return tmpls->has_explicit_template;
 }
@@ -1668,6 +1675,83 @@ static common_chat_params common_chat_templates_apply_jinja(
     return common_chat_params_init_generic(tmpl, params);
 }
 
+// Legacy template route (adhoc C++ implementation of known templates), forward to llama_chat_apply_template.
+static common_chat_params common_chat_templates_apply_legacy2(
+    const struct llama_model * model,
+    const struct common_chat_templates * tmpls,
+    const struct common_chat_templates_inputs & inputs)
+{
+    int alloc_size = 0;
+    std::vector<llama_chat_message> chat;
+    std::vector<std::string> roles;
+    std::vector<std::string> contents;
+    for (const common_chat_msg & msg : inputs.messages) {
+        std::string role = msg.role;
+        std::string content = msg.content;
+        if (msg.tool_calls.empty()) {
+            for (const common_chat_msg_content_part & part : msg.content_parts) {
+                if (part.type != "text") {
+                    LOG_WRN("Ignoring non-text content part: %s\n", part.type.c_str());
+                    continue;
+                }
+                if (!content.empty()) {
+                    content += "\n";;
+                }
+                content += part.text;
+            }
+        } else {
+            role = "tool_call";
+            for (const common_chat_tool_call & tc : msg.tool_calls) {
+                if (!content.empty()) {
+                    content += "\n";
+                }
+                content += "{\"name\":\"" + tc.name + "\",\"arguments\":" + tc.arguments + "}";
+            }
+        }
+        roles.emplace_back(role);
+        contents.emplace_back(content);
+    }
+    for (size_t i = 0; i < contents.size(); ++i) {
+        const std::string & role = roles[i];
+        const std::string & content = contents[i];
+        chat.push_back({role.c_str(), content.c_str()});
+        alloc_size += (role.size() + content.size()) * 1.25;
+    }
+    std::vector<llama_chat_function> func;
+    for (const common_chat_tool & tool : inputs.tools) {
+        func.push_back({tool.name.c_str(), tool.description.c_str(), tool.parameters.c_str()});
+        alloc_size += (tool.name.size() + tool.description.size() + tool.parameters.size()) * 1.25;
+    }
+
+    std::vector<char> buf(alloc_size);
+
+    // run the first time to get the total output length
+    const auto & src = tmpls->template_default->source();
+    int32_t res = llama_chat_apply_template2(model, src.c_str(), chat.data(), chat.size(), func.data(), func.size(), inputs.tool_choice == COMMON_CHAT_TOOL_CHOICE_REQUIRED, inputs.add_generation_prompt, buf.data(), buf.size());
+
+    // error: chat template is not supported
+    if (res < 0) {
+        // if the custom "tmpl" is not supported, we throw an error
+        // this is a bit redundant (for good), since we're not sure if user validated the custom template with llama_chat_verify_template()
+        throw std::runtime_error("this custom template is not supported");
+    }
+
+    // if it turns out that our buffer is too small, we resize it
+    if ((size_t) res > buf.size()) {
+        buf.resize(res);
+        res = llama_chat_apply_template2(model, src.c_str(), chat.data(), chat.size(), func.data(), func.size(), inputs.tool_choice == COMMON_CHAT_TOOL_CHOICE_REQUIRED, inputs.add_generation_prompt, buf.data(), buf.size());
+    }
+
+    common_chat_params params;
+    params.prompt = std::string(buf.data(), res);
+    if (!inputs.json_schema.empty()) {
+        params.grammar = json_schema_to_grammar(json::parse(inputs.json_schema));
+    } else {
+        params.grammar = inputs.grammar;
+    }
+    return params;
+}
+
 // Legacy template route (adhoc C++ implementation of known templates), forward to llama_chat_apply_template.
 static common_chat_params common_chat_templates_apply_legacy(
     const struct common_chat_templates * tmpls,
@@ -1726,6 +1810,17 @@ static common_chat_params common_chat_templates_apply_legacy(
     return params;
 }
 
+common_chat_params common_chat_templates_apply2(
+    const struct llama_model * model,
+    const struct common_chat_templates * tmpls,
+    const struct common_chat_templates_inputs & inputs)
+{
+    GGML_ASSERT(tmpls != nullptr);
+    return inputs.use_jinja
+               ? common_chat_templates_apply_jinja(tmpls, inputs)
+               : common_chat_templates_apply_legacy2(model, tmpls, inputs);
+}
+
 common_chat_params common_chat_templates_apply(
     const struct common_chat_templates * tmpls,
     const struct common_chat_templates_inputs & inputs)
diff --git a/common/chat.h b/common/chat.h
index 9aad84e8..733556aa 100644
--- a/common/chat.h
+++ b/common/chat.h
@@ -98,10 +98,15 @@ common_chat_templates_ptr common_chat_templates_init(
                                            const std::string & bos_token_override = "",
                                            const std::string & eos_token_override = "");
 
+bool         common_chat_templates_supports_tool_calls(const struct common_chat_templates * tmpls);
 bool         common_chat_templates_was_explicit(const struct common_chat_templates * tmpls);
 const char * common_chat_templates_source(const struct common_chat_templates * tmpls, const char * variant = nullptr);
 
 
+struct common_chat_params common_chat_templates_apply2(
+    const struct llama_model * model,
+    const struct common_chat_templates * tmpls,
+    const struct common_chat_templates_inputs & inputs);
 struct common_chat_params      common_chat_templates_apply(
     const struct common_chat_templates * tmpls,
     const struct common_chat_templates_inputs & inputs);
diff --git a/include/llama.h b/include/llama.h
index a18f365b..9b0bf3c0 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -394,6 +394,11 @@ extern "C" {
         const char * role;
         const char * content;
     } llama_chat_message;
+    typedef struct llama_chat_function {
+        const char * name;
+        const char * description;
+        const char * parameters;
+    } llama_chat_function;
 
     // lora adapter
     struct llama_adapter_lora;
@@ -1121,6 +1126,35 @@ extern "C" {
     // Chat templates
     //
 
+    // Get chat template alias
+    LLAMA_API const char * llama_chat_template_alias(const char * tmpl);
+
+    /// Apply chat template. Inspired by hf apply_chat_template() on python.
+    /// Both "model" and "custom_template" are optional, but at least one is required. "custom_template" has higher precedence than "model"
+    /// NOTE: This function does not use a jinja parser. It only support a pre-defined list of template. See more: https://github.com/ggml-org/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template
+    /// @param model A llama model to use for this chat.
+    /// @param tmpl A Jinja template to use for this chat. If this is nullptr, the modelâ€™s default chat template will be used instead.
+    /// @param chat Pointer to a list of multiple llama_chat_message
+    /// @param n_msg Number of llama_chat_message in this chat
+    /// @param func Pointer to a list of multiple llama_chat_function (In JSON format)
+    /// @param n_func Number of llama_chat_function in this chat
+    /// @param req_func Whether to must call function call.
+    /// @param add_ass Whether to end the prompt with the token(s) that indicate the start of an assistant message.
+    /// @param buf A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)
+    /// @param length The size of the allocated buffer
+    /// @return The total number of bytes of the formatted prompt. If is it larger than the size of buffer, you may need to re-alloc it and then re-apply the template.
+    LLAMA_API int32_t llama_chat_apply_template2(
+              const struct llama_model * model,
+                            const char * tmpl,
+       const struct llama_chat_message * chat,
+                                size_t   n_msg,
+      const struct llama_chat_function * func,
+                                size_t   n_func,
+                                  bool   req_func,
+                                  bool   add_ass,
+                                  char * buf,
+                               int32_t   length);
+
     /// Apply chat template. Inspired by hf apply_chat_template() on python.
     /// Both "model" and "custom_template" are optional, but at least one is required. "custom_template" has higher precedence than "model"
     /// NOTE: This function does not use a jinja parser. It only support a pre-defined list of template. See more: https://github.com/ggml-org/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template
diff --git a/src/llama-chat.cpp b/src/llama-chat.cpp
index 46d43c58..e27c9866 100644
--- a/src/llama-chat.cpp
+++ b/src/llama-chat.cpp
@@ -186,6 +186,472 @@ llm_chat_template llm_chat_detect_template(const std::string & tmpl) {
     return LLM_CHAT_TEMPLATE_UNKNOWN;
 }
 
+const char * llama_chat_template_alias(const char * tmpl) {
+    llm_chat_template t = llm_chat_detect_template(std::string(tmpl));
+    for (const auto & it : LLM_CHAT_TEMPLATES) {
+        if (it.second != t) {
+            continue;
+        }
+        return it.first.c_str();
+    }
+    return "unknown";
+}
+
+int32_t llm_chat_apply_template2(
+    llm_arch arch,
+    llm_chat_template tmpl,
+    const std::vector<const llama_chat_message *> & chat,
+    const std::vector<const llama_chat_function *> & func,
+    std::string & dest, bool req_func, bool add_ass) {
+    if (tmpl != LLM_CHAT_TEMPLATE_CHATML &&
+        tmpl != LLM_CHAT_TEMPLATE_MISTRAL_V7 &&
+        tmpl != LLM_CHAT_TEMPLATE_MISTRAL_V3 &&
+        tmpl != LLM_CHAT_TEMPLATE_MISTRAL_V3_TEKKEN &&
+        tmpl != LLM_CHAT_TEMPLATE_LLAMA_3 &&
+        tmpl != LLM_CHAT_TEMPLATE_CHATGLM_4 &&
+        tmpl != LLM_CHAT_TEMPLATE_GRANITE) {
+        return llm_chat_apply_template(tmpl, chat, dest, add_ass);
+    }
+
+    // Taken from the research: https://github.com/ggml-org/llama.cpp/issues/5527
+    std::stringstream ss;
+    if (tmpl == LLM_CHAT_TEMPLATE_CHATML) {
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "<|im_start|>system\n";
+            if (root_msg) {
+                ss << root_msg->content << "\n\n";
+            } else {
+                ss << "You are a helpful assistant.\n\n";
+            }
+            if (arch == LLM_ARCH_QWEN2VL) {
+                ss << "## Tools\n\n";
+            } else {
+                ss << "# Tools\n\n";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions.\n\n";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions.\n\n";
+            }
+            if (arch == LLM_ARCH_QWEN2VL) {
+                ss << "You are provided with following function tools:\n\n";
+                for (const auto & fn : func) {
+                    ss << "### " << fn->name << "\n\n";
+                    ss << fn->name << ": " << fn->description << " Parameters: " << fn->parameters << "Format the arguments as a JSON object.\n\n";
+                }
+                if (!req_func) {
+                    ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                    ss << "Otherwise, try to do function calls without any explanations. ";
+                }
+                ss << "For each function call, just generate an answer, no explanation before or after your answer, MUST return an JSON object with function name and arguments within <tool_call></tool_call> XML tags:\n";
+                ss << "<tool_call>\n";
+                ss << "{\"name\": The name of the function to use, \"arguments\": The input of the function, must be an JSON object in compact format}\n";
+                ss << "</tool_call>\n";
+                ss << "<tool_result>\n";
+                ss << "The function results.\n";
+                ss << "</tool_result>\n";
+                ss << "Reply based on the function results." << "<|im_end|>\n";
+            } else {
+                ss << "You are provided with following function signatures within <tools></tools> XML tags:\n";
+                ss << "<tools>\n";
+                for (const auto & fn : func) {
+                    ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}\n";
+                }
+                ss << "</tools>\n\n";
+                if (!req_func) {
+                    ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                    ss << "Otherwise, try to do function calls without any explanations. ";
+                }
+                ss << "For each function call, just generate an answer, no explanation before or after your answer, MUST return an JSON object with function name and arguments within <tool_call></tool_call> XML tags:\n";
+                ss << "<tool_call>\n";
+                ss << "{\"name\": <function-name>, \"arguments\": <arguments-json-object>}\n";
+                ss << "</tool_call>" << "<|im_end|>\n";
+            }
+        }
+        bool previous_tool_response = false;
+        // chatml template
+        for (auto message : chat) {
+            if (!func.empty()) {
+                std::string role(message->role);
+                if (role == "tool_call") {
+                    if (arch == LLM_ARCH_QWEN2VL) {
+                        if (!previous_tool_response) {
+                            ss << "<|im_start|>assistant\n";
+                        }
+                        ss << "<tool_call>\n" << message->content << "\n</tool_call>\n";
+                    } else {
+                        ss << "<|im_start|>assistant\n";
+                        ss << "<tool_call>\n" << message->content << "\n</tool_call>";
+                        ss << "<|im_end|>\n";
+                    }
+                    previous_tool_response = false;
+                    continue;
+                }
+                previous_tool_response = false;
+                if (role == "system") {
+                    continue;
+                }
+                if (role == "tool") {
+                    if (arch == LLM_ARCH_QWEN2VL) {
+                        ss << "<tool_result>\n" << message->content << "\n</tool_result>\n";
+                        add_ass = message != chat.back();
+                    } else {
+                        ss << "<|im_start|>user\n" << message->content << "<|im_end|>\n";
+                    }
+                    previous_tool_response = true;
+                    continue;
+                }
+                if (role == "assistant" && arch == LLM_ARCH_QWEN2VL) {
+                    ss << message->content << "<|im_end|>\n";
+                    continue;
+                }
+            }
+            ss << "<|im_start|>" << message->role << "\n" << message->content << "<|im_end|>\n";
+        }
+        if (add_ass) {
+            ss << "<|im_start|>assistant\n";
+        }
+    } else if (tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V7) {
+        // Official mistral 'v7' template
+        // See: https://huggingface.co/mistralai/Mistral-Large-Instruct-2411#basic-instruct-template-v7
+        // See: https://github.com/mistralai/mistral-common/releases/tag/v1.5.0
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "[AVAILABLE_TOOLS] " << "[";
+            for (const auto & fn : func) {
+                ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}";
+                ss << ((fn == func.back()) ? "" : ",");
+            }
+            ss << "]" << "[/AVAILABLE_TOOLS]";
+            if (root_msg) {
+                ss << "[SYSTEM_PROMPT] " << root_msg->content;
+            } else {
+                ss << "[SYSTEM_PROMPT] " << "You are a helpful assistant. ";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions. ";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions. ";
+                ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                ss << "Otherwise, try to call functions without any explanations. ";
+            }
+            ss << "[/SYSTEM_PROMPT]";
+        }
+        for (auto message : chat) {
+            std::string role(message->role);
+            std::string content(message->content);
+            if (role == "system") {
+                if (!func.empty()) {
+                    continue;
+                }
+                ss << "[SYSTEM_PROMPT] " << content << "[/SYSTEM_PROMPT]";
+            } else if (role == "user") {
+                ss << "[INST] " << content << "[/INST]";
+            }
+            else {
+                if (!func.empty()) {
+                    if (role == "tool_call") {
+                        ss << "[TOOL_CALLS] ";
+                        ss << "[" << message->content << "]</s>";
+                        continue;
+                    }
+                    if (role == "tool") {
+                        ss << "[TOOL_RESULTS] " << message->content << "[/TOOL_RESULTS]";
+                        continue;
+                    }
+                }
+                ss << " " << content << "</s>";
+            }
+        }
+    } else if (tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V1
+            || tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V3
+            || tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V3_TEKKEN) {
+        // See: https://github.com/mistralai/cookbook/blob/main/concept-deep-dive/tokenization/chat_templates.md
+        // See: https://github.com/mistralai/cookbook/blob/main/concept-deep-dive/tokenization/templates.md
+        // See: https://github.com/mistralai/cookbook/blob/main/concept-deep-dive/tokenization/tool_calling.md
+        std::string leading_space = tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V1 ? " " : "";
+        std::string trailing_space = tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V3_TEKKEN ? "" : " ";
+        bool trim_assistant_message = tmpl == LLM_CHAT_TEMPLATE_MISTRAL_V3;
+        bool is_inside_turn = false;
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << leading_space << "[AVAILABLE_TOOLS]" << trailing_space << "[";
+            for (const auto & fn : func) {
+                ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}";
+                ss << ((fn == func.back()) ? "" : ",");
+            }
+            ss << "]" << leading_space << "[/AVAILABLE_TOOLS]";
+            ss << leading_space << "[INST]" << trailing_space;
+            is_inside_turn = true;
+            if (root_msg) {
+                ss << root_msg->content << "\n\n";
+            } else {
+                ss << "You are a helpful assistant.\n\n";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions.\n\n";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions. ";
+                ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                ss << "Otherwise, try to call functions without any explanations.\n\n";
+            }
+        }
+        for (auto message : chat) {
+            if (!is_inside_turn) {
+                ss << leading_space << "[INST]" << trailing_space;
+                is_inside_turn = true;
+            }
+            std::string role(message->role);
+            std::string content(message->content);
+            if (role == "system") {
+                if (!func.empty()) {
+                    continue;
+                }
+                ss << content << "\n\n";
+            } else if (role == "user") {
+                ss << content << leading_space << "[/INST]";
+            } else {
+                if (!func.empty()) {
+                    if (role == "tool_call") {
+                        ss << leading_space << "[TOOL_CALLS]" << trailing_space;
+                        ss << "[" << message->content << "]</s>";
+                        continue;
+                    }
+                    if (role == "tool") {
+                        ss << leading_space << "[TOOL_RESULTS]" << trailing_space;
+                        ss << message->content;
+                        ss << leading_space << "[/TOOL_RESULTS]";
+                        continue;
+                    }
+                }
+                ss << trailing_space << (trim_assistant_message ? trim(content) : content) << "</s>";
+                is_inside_turn = false;
+            }
+        }
+    } else if (tmpl == LLM_CHAT_TEMPLATE_LLAMA_3) {
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "<|start_header_id|>system<|end_header_id|>\n\n";
+            if (root_msg) {
+                ss << trim(root_msg->content) << "\n\n";
+            } else {
+                ss << "You are a helpful assistant.\n\n";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions.";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions.";
+            }
+            ss << "<|eot_id|>";
+        }
+        // Llama 3
+        for (auto message : chat) {
+            std::string role(message->role);
+            if (!func.empty()) {
+                if (role == "system") {
+                    continue;
+                }
+                if (role == "tool_call") {
+                    ss << "<|start_header_id|>assistant<|end_header_id|>\n\n" << trim(message->content) << "<|eot_id|>";
+                    continue;
+                }
+                if (role == "tool") {
+                    ss << "<|start_header_id|>ipython<|end_header_id|>\n\n" << trim(message->content) << "<|eot_id|>";
+                    continue;
+                }
+                if (role == "user" && message == chat.back()) {
+                    ss << "<|start_header_id|>user<|end_header_id|>\n\n";
+                    ss << "You are provided with following function signatures within <tools></tools> XML tags:\n";
+                    ss << "<tools>\n";
+                    for (const auto & fn : func) {
+                        ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}\n";
+                    }
+                    ss << "</tools>\n\n";
+                    if (!req_func) {
+                        ss << "When you can reply with your internal knowledge, reply directly without any function call. ";
+                        ss << "Otherwise, try to call functions without any explanations. ";
+                    }
+                    ss << "For each function call, just generate an answer, no explanation before or after your answer, MUST return an JSON object with function name and arguments in the format {\"name\": <function-name>, \"arguments\": <arguments-json-object>}.\n";
+                    ss << trim(message->content) << "<|eot_id|>";
+                    continue;
+                }
+            }
+            ss << "<|start_header_id|>" << role << "<|end_header_id|>\n\n" << trim(message->content) << "<|eot_id|>";
+        }
+        if (add_ass) {
+            ss << "<|start_header_id|>assistant<|end_header_id|>\n\n";
+        }
+    } else if (tmpl == LLM_CHAT_TEMPLATE_CHATGLM_4) {
+        ss << "[gMASK]" << "<sop>";
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "<|system|>\n";
+            if (root_msg) {
+                ss << root_msg -> content << "\n";
+            } else {
+                ss << "You are a helpful assistant.\n";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more functions to assist with the user query. Do not make assumptions about what values to plug into functions.\n";
+            } else {
+                ss << "You CAN call functions to assist with the user query. Do not make assumptions about what values to plug into functions.\n";
+            }
+            ss << "# Functions\n";
+            ss << "You are provided with following functions:\n";
+            for (size_t i = 0; i < func.size(); i++) {
+                const llama_chat_function *fn = func[i];
+                ss << "## Function " << i << "\n";
+                ss << "### Name\n" << fn->name << "\n";
+                ss << "### Description\n" << fn->description << "\n";
+                ss << "### Parameters\n" << fn->parameters << "\n";
+            }
+            if (!req_func) {
+                ss << "When you can reply with your internal knowledge, reply directly without any function calls. ";
+                ss << "Otherwise, try to call functions without any explanations. ";
+            }
+            ss << "For each function call, just generate an answer, no explanation before or after your answer, MUST return an JSON object with function name and arguments within <tool_call></tool_call> XML tags:\n";
+            ss << "<tool_call>\n";
+            ss << "{\"name\": The name of the function to use, \"arguments\": The input of the function, must be an JSON object in compact format}\n";
+            ss << "</tool_call>\n";
+            ss << "<tool_result>\n";
+            ss << "The function results.\n";
+            ss << "</tool_result>\n";
+            ss << "Reply based on the function results.\n";
+        }
+        bool previous_tool_response = false;
+        for (auto message : chat) {
+            std::string role(message->role);
+            if (!func.empty()) {
+                if (role == "tool_call") {
+                    if (!previous_tool_response) {
+                        ss << "<|assistant|>\n";
+                    }
+                    ss << "<tool_call>\n" << message->content << "\n</tool_call>\n";
+                    previous_tool_response = false;
+                    continue;
+                }
+                previous_tool_response = false;
+                if (role == "system") {
+                    continue;
+                }
+                if (role == "tool") {
+                    ss << "<tool_result>\n" << message->content << "\n</tool_result>\n";
+                    add_ass = message != chat.back();
+                    previous_tool_response = true;
+                    continue;
+                }
+                if (role == "assistant") {
+                    ss << "<|assistant|>\n" << message->content;
+                    continue;
+                }
+            }
+            ss << "<|" << role << "|>" << "\n" << message->content;
+        }
+        if (add_ass) {
+            ss << "<|assistant|>\n";
+        }
+    } else if (tmpl == LLM_CHAT_TEMPLATE_GRANITE) {
+        if (!func.empty()) {
+            const llama_chat_message *root_msg = nullptr;
+            for (const auto *message: chat) {
+                std::string role(message->role);
+                if (role == "system") {
+                    root_msg = message;
+                    break;
+                }
+            }
+            ss << "<|start_of_role|>tools<|end_of_role|>[";
+            for (const auto & fn : func) {
+                ss << R"({"type": "function", "function": {"name": ")" << fn->name << R"(", "description": ")" << fn->description << R"(", "parameters": )" << fn->parameters << "}}";
+                ss << ((fn == func.back()) ? "" : ",");
+            }
+            ss << "]<|end_of_text|>\n";
+            ss << "<|start_of_role|>system<|end_of_role|>";
+            if (root_msg) {
+                ss << trim(root_msg->content) << " ";
+            } else {
+                ss << "You are a helpful assistant with tool calling capabilities. ";
+            }
+            if (req_func) {
+                ss << "You MUST call one or more tools to assist with the user query. Do not make assumptions about what values to plug into tools. ";
+            } else {
+                ss << "You CAN call tools to assist with the user query. Do not make assumptions about what values to plug into tools. ";
+            }
+            if (!req_func) {
+                ss << "When you can reply with your internal knowledge, reply directly without any tool calls. ";
+                ss << "Otherwise, try to call tools without any explanations. ";
+            }
+            ss << "For each tool call, just generate an answer, no explanation before or after your answer, MUST return <|tool_call|><tool_call> followed by an JSON list of tool used as follows: ";
+            ss << R"(<|tool_call|><tool_call>[{"name": <function-name>, "arguments": <arguments-json-object>}])";
+            ss << "Write the response to the user's input by strictly aligning with the facts in the provided documents.";
+            ss << "<|end_of_text|>\n";
+        }
+        // IBM Granite template
+        for (const auto & message : chat) {
+            std::string role(message->role);
+            if (!func.empty()) {
+                if (role == "system") {
+                    continue;
+                }
+                if (role == "tool_call") {
+                    ss << "<|start_of_role|>assistant<|start_of_role|><|tool_call|><tool_call>" << message->content << "<|end_of_text|>\n";
+                    continue;
+                }
+                if (role == "tool") {
+                    ss << "<|start_of_role|>tool_response<|end_of_role|>" << message->content << "<|end_of_text|>\n";
+                    continue;
+                }
+            }
+            ss << "<|start_of_role|>" << role << "<|end_of_role|>";
+            ss << message->content << "<|end_of_text|>\n";
+        }
+        if (add_ass) {
+            ss << "<|start_of_role|>assistant<|end_of_role|>\n";
+        }
+    } else {
+        // template not supported
+        return -1;
+    }
+    dest = ss.str();
+    return dest.size();
+}
+
 // Simple version of "llama_apply_chat_template" that only works with strings
 // This function uses heuristic checks to determine commonly used template. It is not a jinja parser.
 int32_t llm_chat_apply_template(
diff --git a/src/llama-chat.h b/src/llama-chat.h
index 3f584346..d13e5e29 100644
--- a/src/llama-chat.h
+++ b/src/llama-chat.h
@@ -3,6 +3,7 @@
 #include <string>
 #include <vector>
 #include <cstdint>
+#include "llama-arch.h"
 
 enum llm_chat_template {
     LLM_CHAT_TEMPLATE_CHATML,
@@ -47,10 +48,19 @@ enum llm_chat_template {
 
 struct llama_chat_message;
 
+struct llama_chat_function;
+
 llm_chat_template llm_chat_template_from_str(const std::string & name);
 
 llm_chat_template llm_chat_detect_template(const std::string & tmpl);
 
+int32_t llm_chat_apply_template2(
+    llm_arch arch,
+    llm_chat_template tmpl,
+    const std::vector<const llama_chat_message *> & chat,
+    const std::vector<const llama_chat_function *> & func,
+    std::string & dest, bool req_func, bool add_ass);
+
 int32_t llm_chat_apply_template(
     llm_chat_template tmpl,
     const std::vector<const llama_chat_message *> & chat,
diff --git a/src/llama.cpp b/src/llama.cpp
index d5164720..b21050ee 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -257,6 +257,49 @@ struct llama_model * llama_model_load_from_splits(
 // chat templates
 //
 
+int32_t llama_chat_apply_template2(
+                const struct llama_model * model,
+                              const char * tmpl,
+         const struct llama_chat_message * chat,
+                                  size_t   n_msg,
+        const struct llama_chat_function * func,
+                                    size_t n_func,
+                                      bool req_func,
+                                      bool add_ass,
+                                    char * buf,
+                                 int32_t   length) {
+    const std::string curr_tmpl(tmpl == nullptr ? "chatml" : tmpl);
+
+    // format the chat to string
+    std::vector<const llama_chat_message *> chat_vec;
+    chat_vec.resize(n_msg);
+    for (size_t i = 0; i < n_msg; i++) {
+        chat_vec[i] = &chat[i];
+    }
+
+    // format the func to string
+    std::vector<const llama_chat_function *> func_vec;
+    func_vec.resize(n_func);
+    for (size_t i = 0; i < n_func; i++) {
+        func_vec[i] = &func[i];
+    }
+
+    std::string       formatted_chat;
+    llm_chat_template detected_tmpl = llm_chat_detect_template(curr_tmpl);
+    if (detected_tmpl == LLM_CHAT_TEMPLATE_UNKNOWN) {
+        return -1;
+    }
+    int32_t res = llm_chat_apply_template2(model ? model->arch : LLM_ARCH_LLAMA, detected_tmpl, chat_vec, func_vec,
+                                          formatted_chat, req_func, add_ass);
+    if (res < 0) {
+        return res;
+    }
+    if (buf && length > 0) {
+        strncpy(buf, formatted_chat.c_str(), length);
+    }
+    return res;
+}
+
 int32_t llama_chat_apply_template(
                               const char * tmpl,
          const struct llama_chat_message * chat,
