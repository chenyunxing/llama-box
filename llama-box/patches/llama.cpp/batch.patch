diff --git a/src/llama-batch.cpp b/src/llama-batch.cpp
index 8b6d14fe..72667346 100644
--- a/src/llama-batch.cpp
+++ b/src/llama-batch.cpp
@@ -502,35 +502,35 @@ bool llama_batch_allocr::init(
     // consistency checks
     //
 
-    for (int32_t s = 0; s < LLAMA_MAX_SEQ; ++s) {
-        if (seq_pos[s].empty()) {
-            continue;
-        }
-
-        if (memory && seq_pos_min(s) != memory->seq_pos_max(s) + 1) {
-            LLAMA_LOG_ERROR("%s: sequence %d does not start from the last position stored in the memory\n", __func__, s);
-            return false;
-        }
-
-        if (seq_pos_max(s) - seq_pos_min(s) + 1 > (int) seq_pos[s].size()) {
-            LLAMA_LOG_ERROR("%s: sequence %d positions are not continuous\n", __func__, s);
-            return false;
-        }
-    }
-
-    if (memory) {
-        for (int32_t s0 = 0; s0 < LLAMA_MAX_SEQ; ++s0) {
-            for (int32_t s1 = 0; s1 < LLAMA_MAX_SEQ; ++s1) {
-                if (seq_cpl[s0][s1]) {
-                    if (memory->seq_pos_min(s0) != memory->seq_pos_min(s1) ||
-                        memory->seq_pos_max(s0) != memory->seq_pos_max(s1)) {
-                        LLAMA_LOG_ERROR("%s: sequence %d is coupled to %d in the input batch, but have divereged\n", __func__, s0, s1);
-                        return false;
-                    }
-                }
-            }
-        }
-    }
+//    for (int32_t s = 0; s < LLAMA_MAX_SEQ; ++s) {
+//        if (seq_pos[s].empty()) {
+//            continue;
+//        }
+//
+//        if (memory && seq_pos_min(s) != memory->seq_pos_max(s) + 1) {
+//            LLAMA_LOG_ERROR("%s: sequence %d does not start from the last position stored in the memory\n", __func__, s);
+//            return false;
+//        }
+//
+//        if (seq_pos_max(s) - seq_pos_min(s) + 1 > (int) seq_pos[s].size()) {
+//            LLAMA_LOG_ERROR("%s: sequence %d positions are not continuous\n", __func__, s);
+//            return false;
+//        }
+//    }
+//
+//    if (memory) {
+//        for (int32_t s0 = 0; s0 < LLAMA_MAX_SEQ; ++s0) {
+//            for (int32_t s1 = 0; s1 < LLAMA_MAX_SEQ; ++s1) {
+//                if (seq_cpl[s0][s1]) {
+//                    if (memory->seq_pos_min(s0) != memory->seq_pos_min(s1) ||
+//                        memory->seq_pos_max(s0) != memory->seq_pos_max(s1)) {
+//                        LLAMA_LOG_ERROR("%s: sequence %d is coupled to %d in the input batch, but have divereged\n", __func__, s0, s1);
+//                        return false;
+//                    }
+//                }
+//            }
+//        }
+//    }
 
     return true;
 }
