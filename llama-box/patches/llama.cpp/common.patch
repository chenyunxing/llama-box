diff --git a/common/common.cpp b/common/common.cpp
index 2afa9b2d..e7cfb5b6 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -1015,7 +1015,16 @@ struct common_init_result common_init_from_params(common_params & params) {
 
         // some models (e.g. T5) don't have a BOS token
         if (bos != LLAMA_TOKEN_NULL) {
+#ifdef GGML_USE_CANN
+            uint32_t n_ctx = llama_n_ctx(lctx) / params.n_parallel;
+            tmp.reserve(n_ctx);
+#endif
             tmp.push_back(bos);
+#ifdef GGML_USE_CANN
+            for (uint32_t i = 0; i < n_ctx-2; i++) {
+                tmp.push_back(0);
+            }
+#endif
         }
         if (eos != LLAMA_TOKEN_NULL) {
             tmp.push_back(eos);
@@ -1025,7 +1034,9 @@ struct common_init_result common_init_from_params(common_params & params) {
         }
 
         if (llama_model_has_encoder(model)) {
-            llama_encode(lctx, llama_batch_get_one(tmp.data(), tmp.size()));
+            size_t size = tmp.size();
+            LOG_INF("warming up encoder with %zu tokens\n", size);
+            llama_encode(lctx, llama_batch_get_one(tmp.data(), size));
             llama_token decoder_start_token_id = llama_model_decoder_start_token(model);
             if (decoder_start_token_id == LLAMA_TOKEN_NULL) {
                 decoder_start_token_id = bos;
@@ -1034,7 +1045,9 @@ struct common_init_result common_init_from_params(common_params & params) {
             tmp.push_back(decoder_start_token_id);
         }
         if (llama_model_has_decoder(model)) {
-            llama_decode(lctx, llama_batch_get_one(tmp.data(), std::min(tmp.size(), (size_t) params.n_batch)));
+            size_t size = std::min(tmp.size(), (size_t) params.n_batch);
+            LOG_INF("warming up decoder with %zu tokens\n", size);
+            llama_decode(lctx, llama_batch_get_one(tmp.data(), size));
         }
         llama_kv_self_clear(lctx);
         llama_synchronize(lctx);
diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index e153351a..c39a0818 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -908,9 +908,9 @@ int llama_context::decode(llama_batch & inp_batch) {
         }
     }
 
-    GGML_ASSERT(n_tokens_all <= cparams.n_batch);
+    // GGML_ASSERT(n_tokens_all <= cparams.n_batch);
 
-    GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) && "non-causal attention requires n_ubatch >= n_tokens");
+    // GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) && "non-causal attention requires n_ubatch >= n_tokens");
 
     if (t_compute_start_us == 0) {
         t_compute_start_us = ggml_time_us();
