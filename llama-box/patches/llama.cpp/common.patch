diff --git a/common/common.cpp b/common/common.cpp
index d4882c51..35743fee 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -994,7 +994,11 @@ struct common_init_result common_init_from_params(common_params & params) {
 
         // some models (e.g. T5) don't have a BOS token
         if (bos != LLAMA_TOKEN_NULL) {
+            tmp.reserve(llama_n_ctx(lctx));
             tmp.push_back(bos);
+            for (int i = 0; i < int32_t(llama_n_ctx(lctx))-2; i++) {
+                tmp.push_back(0);
+            }
         }
         if (eos != LLAMA_TOKEN_NULL) {
             tmp.push_back(eos);
@@ -1013,7 +1017,7 @@ struct common_init_result common_init_from_params(common_params & params) {
             tmp.push_back(decoder_start_token_id);
         }
         if (llama_model_has_decoder(model)) {
-            llama_decode(lctx, llama_batch_get_one(tmp.data(), std::min(tmp.size(), (size_t) params.n_batch)));
+            llama_decode(lctx, llama_batch_get_one(tmp.data(), tmp.size()));
         }
         llama_kv_self_clear(lctx);
         llama_synchronize(lctx);
diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index 39270794..03b3f1e3 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -1215,9 +1215,9 @@ int llama_context::decode(llama_batch & inp_batch) {
         }
     }
 
-    GGML_ASSERT(n_tokens_all <= cparams.n_batch);
+    // GGML_ASSERT(n_tokens_all <= cparams.n_batch);
 
-    GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) && "non-causal attention requires n_ubatch >= n_tokens");
+    // GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) && "non-causal attention requires n_ubatch >= n_tokens");
 
     if (t_compute_start_us == 0) {
         t_compute_start_us = ggml_time_us();
