diff --git a/common/common.cpp b/common/common.cpp
index 18ffb4e7..8ff9dbc8 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -1041,7 +1041,11 @@ struct common_init_result common_init_from_params(common_params & params) {
 
         // some models (e.g. T5) don't have a BOS token
         if (bos != LLAMA_TOKEN_NULL) {
+            tmp.reserve(llama_n_ctx(lctx));
             tmp.push_back(bos);
+            for (int i = 0; i < int32_t(llama_n_ctx(lctx))-2; i++) {
+                tmp.push_back(0);
+            }
         }
         if (eos != LLAMA_TOKEN_NULL) {
             tmp.push_back(eos);
@@ -1060,7 +1064,7 @@ struct common_init_result common_init_from_params(common_params & params) {
             tmp.push_back(decoder_start_token_id);
         }
         if (llama_model_has_decoder(model)) {
-            llama_decode(lctx, llama_batch_get_one(tmp.data(), std::min(tmp.size(), (size_t) params.n_batch)));
+            llama_decode(lctx, llama_batch_get_one(tmp.data(), tmp.size()));
         }
         llama_kv_self_clear(lctx);
         llama_synchronize(lctx);
diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index 5bec63e2..8fbd41e4 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -1223,9 +1223,9 @@ int llama_context::decode(llama_batch & inp_batch) {
         }
     }
 
-    GGML_ASSERT(n_tokens_all <= cparams.n_batch);
+    // GGML_ASSERT(n_tokens_all <= cparams.n_batch);
 
-    GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) && "non-causal attention requires n_ubatch >= n_tokens");
+    // GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) && "non-causal attention requires n_ubatch >= n_tokens");
 
     if (t_compute_start_us == 0) {
         t_compute_start_us = ggml_time_us();
