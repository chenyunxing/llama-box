diff --git a/common/common.cpp b/common/common.cpp
index e4e71ad13..11e63a789 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -1035,7 +1035,16 @@ struct common_init_result common_init_from_params(common_params & params) {
 
         // some models (e.g. T5) don't have a BOS token
         if (bos != LLAMA_TOKEN_NULL) {
+#ifdef GGML_USE_CANN
+            uint32_t n_ctx = llama_n_ctx(lctx) / params.n_parallel;
+            tmp.reserve(n_ctx);
+#endif
             tmp.push_back(bos);
+#ifdef GGML_USE_CANN
+            for (uint32_t i = 0; i < n_ctx-2; i++) {
+                tmp.push_back(0);
+            }
+#endif
         }
         if (eos != LLAMA_TOKEN_NULL) {
             tmp.push_back(eos);
@@ -1045,7 +1054,9 @@ struct common_init_result common_init_from_params(common_params & params) {
         }
 
         if (llama_model_has_encoder(model)) {
-            llama_encode(lctx, llama_batch_get_one(tmp.data(), tmp.size()));
+            size_t size = tmp.size();
+            LOG_INF("warming up encoder with %zu tokens\n", size);
+            llama_encode(lctx, llama_batch_get_one(tmp.data(), size));
             llama_token decoder_start_token_id = llama_model_decoder_start_token(model);
             if (decoder_start_token_id == LLAMA_TOKEN_NULL) {
                 decoder_start_token_id = bos;
@@ -1054,7 +1065,9 @@ struct common_init_result common_init_from_params(common_params & params) {
             tmp.push_back(decoder_start_token_id);
         }
         if (llama_model_has_decoder(model)) {
-            llama_decode(lctx, llama_batch_get_one(tmp.data(), std::min(tmp.size(), (size_t) params.n_batch)));
+            size_t size = std::min(tmp.size(), (size_t) params.n_batch);
+            LOG_INF("warming up decoder with %zu tokens\n", size);
+            llama_decode(lctx, llama_batch_get_one(tmp.data(), size));
         }
         llama_memory_clear(llama_get_memory(lctx), true);
         llama_synchronize(lctx);
diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index 7c07b047b..fdc973903 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -927,9 +927,9 @@ int llama_context::decode(const llama_batch & batch_inp) {
         }
     }
 
-    GGML_ASSERT(n_tokens_all <= cparams.n_batch);
+//    GGML_ASSERT(n_tokens_all <= cparams.n_batch);
 
-    GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) && "non-causal attention requires n_ubatch >= n_tokens");
+//    GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) && "non-causal attention requires n_ubatch >= n_tokens");
 
     if (t_compute_start_us == 0) {
         t_compute_start_us = ggml_time_us();
