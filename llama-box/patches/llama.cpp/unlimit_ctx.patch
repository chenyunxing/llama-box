diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index 5a2eef9b..7e5a0fbe 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -84,6 +84,9 @@ llama_context::llama_context(
 
     // with causal attention, the batch size is limited by the context size
     cparams.n_batch = cparams.causal_attn ? std::min(cparams.n_ctx, params.n_batch) : params.n_batch;
+    if (getenv("LLAMA_BOX_V2") != nullptr) {
+        cparams.n_batch = params.n_ctx;
+    }
 
     // the batch has to be at least GGML_KQ_MASK_PAD because we will be padding the KQ_mask
     // this is required by GPU kernels in order to avoid out-of-bounds accesses (e.g. ggml_flash_attn_ext)
