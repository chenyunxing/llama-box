diff --git a/flux.hpp b/flux.hpp
index 20ff410..2fadb03 100644
--- a/flux.hpp
+++ b/flux.hpp
@@ -979,12 +979,16 @@ namespace Flux {
             struct ggml_context* work_ctx = ggml_init(params);
             GGML_ASSERT(work_ctx != NULL);
 
+            auto * back_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
+            auto * reg_cpu = ggml_backend_dev_backend_reg(ggml_backend_get_device(back_cpu));
+            auto * ggml_set_f32_fn = (decltype(ggml_set_f32)*)ggml_backend_reg_get_proc_address(reg_cpu, "ggml_set_f32");
+
             {
                 // cpu f16:
                 // cuda f16: nan
                 // cuda q8_0: pass
                 auto x = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 16, 16, 16, 1);
-                ggml_set_f32(x, 0.01f);
+                ggml_set_f32_fn(x, 0.01f);
                 // print_ggml_tensor(x);
 
                 std::vector<float> timesteps_vec(1, 999.f);
@@ -994,11 +998,11 @@ namespace Flux {
                 auto guidance = vector_to_ggml_tensor(work_ctx, guidance_vec);
 
                 auto context = ggml_new_tensor_3d(work_ctx, GGML_TYPE_F32, 4096, 256, 1);
-                ggml_set_f32(context, 0.01f);
+                ggml_set_f32_fn(context, 0.01f);
                 // print_ggml_tensor(context);
 
                 auto y = ggml_new_tensor_2d(work_ctx, GGML_TYPE_F32, 768, 1);
-                ggml_set_f32(y, 0.01f);
+                ggml_set_f32_fn(y, 0.01f);
                 // print_ggml_tensor(y);
 
                 struct ggml_tensor* out = NULL;
@@ -1014,7 +1018,7 @@ namespace Flux {
 
         static void load_from_file_and_test(const std::string& file_path) {
             // ggml_backend_t backend    = ggml_backend_cuda_init(0);
-            ggml_backend_t backend           = ggml_backend_cpu_init();
+            ggml_backend_t backend           = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
             ggml_type model_data_type        = GGML_TYPE_Q8_0;
             std::shared_ptr<FluxRunner> flux = std::shared_ptr<FluxRunner>(new FluxRunner(backend));
             {
diff --git a/ggml_extend.hpp b/ggml_extend.hpp
index 236e7c1..afea9dd 100644
--- a/ggml_extend.hpp
+++ b/ggml_extend.hpp
@@ -122,10 +122,14 @@ __STATIC_INLINE__ struct ggml_tensor* ggml_kronecker(ggml_context* ctx, struct g
 }
 
 __STATIC_INLINE__ void ggml_tensor_set_f32_randn(struct ggml_tensor* tensor, std::shared_ptr<RNG> rng) {
+    auto * back_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
+    auto * reg_cpu = ggml_backend_dev_backend_reg(ggml_backend_get_device(back_cpu));
+    auto * ggml_set_f32_1d_fn =  (decltype(ggml_set_f32_1d) *) ggml_backend_reg_get_proc_address(reg_cpu, "ggml_set_f32_1d");
+
     uint32_t n                        = (uint32_t)ggml_nelements(tensor);
     std::vector<float> random_numbers = rng->randn(n);
     for (uint32_t i = 0; i < n; i++) {
-        ggml_set_f32_1d(tensor, i, random_numbers[i]);
+        ggml_set_f32_1d_fn(tensor, i, random_numbers[i]);
     }
 }
 
@@ -286,9 +290,14 @@ __STATIC_INLINE__ void copy_ggml_tensor(struct ggml_tensor* dst, struct ggml_ten
     }
     ggml_tensor* final = ggml_cpy(ctx, src, dst);
 
+    auto * back_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
+    auto * reg_cpu = ggml_backend_dev_backend_reg(ggml_backend_get_device(back_cpu));
+    auto * ggml_graph_compute_with_ctx_fn =
+        (decltype(ggml_graph_compute_with_ctx) *) ggml_backend_reg_get_proc_address(reg_cpu, "ggml_graph_compute_with_ctx");
+
     struct ggml_cgraph* graph = ggml_new_graph(ctx);
     ggml_build_forward_expand(graph, final);
-    ggml_graph_compute_with_ctx(ctx, graph, 1);
+    ggml_graph_compute_with_ctx_fn(ctx, graph, 1);
     ggml_free(ctx);
 }
 
@@ -597,7 +606,11 @@ typedef std::function<void(ggml_tensor*, ggml_tensor*, bool)> on_tile_process;
 
 // Tiling
 __STATIC_INLINE__ void sd_tiling(ggml_tensor* input, ggml_tensor* output, const int scale, const int tile_size, const float tile_overlap_factor, on_tile_process on_processing, bool scaled_out = true) {
-    output = ggml_set_f32(output, 0);
+    auto * back_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
+    auto * reg_cpu = ggml_backend_dev_backend_reg(ggml_backend_get_device(back_cpu));
+    auto * ggml_set_f32_fn = (decltype(ggml_set_f32) *) ggml_backend_reg_get_proc_address(reg_cpu, "ggml_set_f32");
+
+    output = ggml_set_f32_fn(output, 0);
 
     int input_width   = (int)input->ne[0];
     int input_height  = (int)input->ne[1];
@@ -1001,16 +1014,8 @@ __STATIC_INLINE__ struct ggml_tensor* ggml_nn_group_norm(struct ggml_context* ct
 }
 
 __STATIC_INLINE__ void ggml_backend_tensor_get_and_sync(ggml_backend_t backend, const struct ggml_tensor* tensor, void* data, size_t offset, size_t size) {
-#if defined(SD_USE_CUDA) || defined(SD_USE_SYCL)
-    if (!ggml_backend_is_cpu(backend)) {
-        ggml_backend_tensor_get_async(backend, tensor, data, offset, size);
-        ggml_backend_synchronize(backend);
-    } else {
-        ggml_backend_tensor_get(tensor, data, offset, size);
-    }
-#else
-    ggml_backend_tensor_get(tensor, data, offset, size);
-#endif
+    ggml_backend_tensor_get_async(backend, tensor, data, offset, size);
+    ggml_backend_synchronize(backend);
 }
 
 __STATIC_INLINE__ float ggml_backend_tensor_get_f32(ggml_tensor* tensor) {
@@ -1140,6 +1145,7 @@ protected:
     std::map<struct ggml_tensor*, const void*> backend_tensor_data_map;
 
     ggml_backend_t backend = NULL;
+    enum ggml_backend_dev_type backend_dev_type = GGML_BACKEND_DEVICE_TYPE_CPU;
 
     void alloc_params_ctx() {
         struct ggml_init_params params;
@@ -1196,7 +1202,7 @@ protected:
         LOG_DEBUG("%s compute buffer size: %.2f MB(%s)",
                   get_desc().c_str(),
                   compute_buffer_size / 1024.0 / 1024.0,
-                  ggml_backend_is_cpu(backend) ? "RAM" : "VRAM");
+                  backend_dev_type == GGML_BACKEND_DEVICE_TYPE_CPU ? "RAM" : "VRAM");
         return true;
     }
 
@@ -1216,6 +1222,7 @@ public:
 
     GGMLRunner(ggml_backend_t backend)
         : backend(backend) {
+        backend_dev_type = ggml_backend_dev_type(ggml_backend_get_device(backend));
         alloc_params_ctx();
     }
 
@@ -1244,12 +1251,12 @@ public:
         LOG_DEBUG("%s params backend buffer size = % 6.2f MB(%s) (%i tensors)",
                   get_desc().c_str(),
                   params_buffer_size / (1024.0 * 1024.0),
-                  ggml_backend_is_cpu(backend) ? "RAM" : "VRAM",
+                  backend_dev_type == GGML_BACKEND_DEVICE_TYPE_CPU  ? "RAM" : "VRAM",
                   num_tensors);
         // printf("%s params backend buffer size = % 6.2f MB(%s) (%i tensors)\n",
         //           get_desc().c_str(),
         //           params_buffer_size / (1024.0 * 1024.0),
-        //           ggml_backend_is_cpu(backend) ? "RAM" : "VRAM",
+        //           backend_dev_type == GGML_BACKEND_DEVICE_TYPE_CPU ? "RAM" : "VRAM",
         //           num_tensors);
         return true;
     }
@@ -1286,7 +1293,7 @@ public:
             return NULL;
         }
         // it's performing a compute, check if backend isn't cpu
-        if (!ggml_backend_is_cpu(backend) && (tensor->buffer == NULL || ggml_backend_buffer_is_host(tensor->buffer))) {
+        if (backend_dev_type != GGML_BACKEND_DEVICE_TYPE_CPU && (tensor->buffer == NULL || ggml_backend_buffer_is_host(tensor->buffer))) {
             // pass input tensors to gpu memory
             auto backend_tensor = ggml_dup_tensor(compute_ctx, tensor);
 
diff --git a/mmdit.hpp b/mmdit.hpp
index dee7b1c..e92713e 100644
--- a/mmdit.hpp
+++ b/mmdit.hpp
@@ -938,6 +938,10 @@ struct MMDiTRunner : public GGMLRunner {
         struct ggml_context* work_ctx = ggml_init(params);
         GGML_ASSERT(work_ctx != NULL);
 
+        auto * back_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
+        auto * reg_cpu = ggml_backend_dev_backend_reg(ggml_backend_get_device(back_cpu));
+        auto * ggml_set_f32_fn = (decltype(ggml_set_f32)*)ggml_backend_reg_get_proc_address(reg_cpu, "ggml_set_f32");
+
         {
             // cpu f16: pass
             // cpu f32: pass
@@ -946,15 +950,15 @@ struct MMDiTRunner : public GGMLRunner {
             auto x = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 128, 128, 16, 1);
             std::vector<float> timesteps_vec(1, 999.f);
             auto timesteps = vector_to_ggml_tensor(work_ctx, timesteps_vec);
-            ggml_set_f32(x, 0.01f);
+            ggml_set_f32_fn(x, 0.01f);
             // print_ggml_tensor(x);
 
             auto context = ggml_new_tensor_3d(work_ctx, GGML_TYPE_F32, 4096, 154, 1);
-            ggml_set_f32(context, 0.01f);
+            ggml_set_f32_fn(context, 0.01f);
             // print_ggml_tensor(context);
 
             auto y = ggml_new_tensor_2d(work_ctx, GGML_TYPE_F32, 2048, 1);
-            ggml_set_f32(y, 0.01f);
+            ggml_set_f32_fn(y, 0.01f);
             // print_ggml_tensor(y);
 
             struct ggml_tensor* out = NULL;
@@ -970,7 +974,7 @@ struct MMDiTRunner : public GGMLRunner {
 
     static void load_from_file_and_test(const std::string& file_path) {
         // ggml_backend_t backend    = ggml_backend_cuda_init(0);
-        ggml_backend_t backend             = ggml_backend_cpu_init();
+        ggml_backend_t backend             = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
         ggml_type model_data_type          = GGML_TYPE_F16;
         std::shared_ptr<MMDiTRunner> mmdit = std::shared_ptr<MMDiTRunner>(new MMDiTRunner(backend));
         {
diff --git a/model.cpp b/model.cpp
index d9cf7b4..1d69efb 100644
--- a/model.cpp
+++ b/model.cpp
@@ -2133,7 +2133,7 @@ bool ModelLoader::save_to_gguf_file(const std::string& file_path, ggml_type outt
     LOG_INFO("save t5xxl weight type: %s", ggml_type_name(t5xxl_outtype));
     LOG_INFO("save diffusion model weight type: %s", ggml_type_name(outtype));
 
-    auto backend    = ggml_backend_cpu_init();
+    auto backend    = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
     size_t mem_size = 1 * 1024 * 1024;  // for padding
     mem_size += tensor_storages.size() * ggml_tensor_overhead();
     mem_size += get_params_mem_size(backend, outtype, vae_outtype, clip_l_outtype, clip_g_outtype, t5xxl_outtype);
diff --git a/preprocessing.hpp b/preprocessing.hpp
index 4ea1dba..dee28e7 100644
--- a/preprocessing.hpp
+++ b/preprocessing.hpp
@@ -15,7 +15,13 @@ void convolve(struct ggml_tensor* input, struct ggml_tensor* output, struct ggml
     ggml_tensor* h  = ggml_conv_2d(ctx0, kernel_fp16, input, 1, 1, padding, padding, 1, 1);
     ggml_cgraph* gf = ggml_new_graph(ctx0);
     ggml_build_forward_expand(gf, ggml_cpy(ctx0, h, output));
-    ggml_graph_compute_with_ctx(ctx0, gf, 1);
+
+    auto * back_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
+    auto * reg_cpu = ggml_backend_dev_backend_reg(ggml_backend_get_device(back_cpu));
+    auto * ggml_graph_compute_with_ctx_fn =
+        (decltype(ggml_graph_compute_with_ctx) *) ggml_backend_reg_get_proc_address(reg_cpu, "ggml_graph_compute_with_ctx");
+
+    ggml_graph_compute_with_ctx_fn(ctx0, gf, 1);
     ggml_free(ctx0);
 }
 
diff --git a/stable-diffusion.cpp b/stable-diffusion.cpp
index 0ea4c2a..efa2ef5 100644
--- a/stable-diffusion.cpp
+++ b/stable-diffusion.cpp
@@ -142,6 +142,7 @@ void calculate_alphas_cumprod(float* alphas_cumprod,
 
 class StableDiffusionGGML {
 public:
+    ggml_backend_t backend_cpu         = NULL;
     ggml_backend_t backend             = NULL;  // general backend
     ggml_backend_t clip_backend        = NULL;
     ggml_backend_t vae_backend         = NULL;
@@ -186,6 +187,12 @@ public:
 
     std::shared_ptr<Denoiser> denoiser = std::make_shared<CompVisDenoiser>();
 
+    enum ggml_backend_dev_type backend_dev_type = GGML_BACKEND_DEVICE_TYPE_CPU;
+    enum ggml_backend_dev_type clip_backend_dev_type = GGML_BACKEND_DEVICE_TYPE_CPU;
+    enum ggml_backend_dev_type vae_backend_dev_type = GGML_BACKEND_DEVICE_TYPE_CPU;
+    enum ggml_backend_dev_type control_net_backend_dev_type = GGML_BACKEND_DEVICE_TYPE_CPU;
+    ggml_backend_reg_t reg_cpu          = nullptr;
+
     StableDiffusionGGML() = default;
 
     StableDiffusionGGML(int n_threads,
@@ -275,10 +282,12 @@ public:
             }
         }
 
+        backend_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
+
         // initialize the backend
         if (gpu_devices.empty()) {
             // no GPU devices available
-            backend = ggml_backend_cpu_init();
+            backend = backend_cpu;
         } else if (gpu_devices.size() < 3) {
             // use the last GPU device: device 0, device 1
             backend = ggml_backend_dev_init(gpu_devices[gpu_devices.size() - 1].first, nullptr);
@@ -298,17 +307,17 @@ public:
                 clip_backend = backend;
                 if (clip_on_cpu) {
                     LOG_INFO("CLIP: Using CPU backend");
-                    clip_backend = ggml_backend_cpu_init();
+                    clip_backend = backend_cpu;
                 }
                 vae_backend = backend;
                 if (vae_on_cpu) {
                     LOG_INFO("VAE Autoencoder: Using CPU backend");
-                    vae_backend = ggml_backend_cpu_init();
+                    vae_backend = backend_cpu;
                 }
                 control_net_backend = backend;
                 if (control_net_cpu) {
                     LOG_INFO("ControlNet: Using CPU backend");
-                    control_net_backend = ggml_backend_cpu_init();
+                    control_net_backend = backend_cpu;
                 }
                 break;
             }
@@ -316,19 +325,19 @@ public:
                 // device 0: clip, vae, control_net
                 if (clip_on_cpu) {
                     LOG_INFO("CLIP: Using CPU backend");
-                    clip_backend = ggml_backend_cpu_init();
+                    clip_backend = backend_cpu;
                 } else {
                     clip_backend = ggml_backend_dev_init(gpu_devices[0].first, nullptr);
                 }
                 if (vae_on_cpu) {
                     LOG_INFO("VAE Autoencoder: Using CPU backend");
-                    vae_backend = ggml_backend_cpu_init();
+                    vae_backend = backend_cpu;
                 } else {
                     vae_backend = ggml_backend_dev_init(gpu_devices[0].first, nullptr);
                 }
                 if (control_net_cpu) {
                     LOG_INFO("ControlNet: Using CPU backend");
-                    control_net_backend = ggml_backend_cpu_init();
+                    control_net_backend = backend_cpu;
                 } else {
                     control_net_backend = ggml_backend_dev_init(gpu_devices[0].first, nullptr);
                 }
@@ -339,25 +348,37 @@ public:
                 // device 1: vae
                 if (clip_on_cpu) {
                     LOG_INFO("CLIP: Using CPU backend");
-                    clip_backend = ggml_backend_cpu_init();
+                    clip_backend = backend_cpu;
                 } else {
                     clip_backend = ggml_backend_dev_init(gpu_devices[0].first, nullptr);
                 }
                 if (vae_on_cpu) {
                     LOG_INFO("VAE Autoencoder: Using CPU backend");
-                    vae_backend = ggml_backend_cpu_init();
+                    vae_backend = backend_cpu;
                 } else {
                     vae_backend = ggml_backend_dev_init(gpu_devices[1].first, nullptr);
                 }
                 if (control_net_cpu) {
                     LOG_INFO("ControlNet: Using CPU backend");
-                    control_net_backend = ggml_backend_cpu_init();
+                    control_net_backend = backend_cpu;
                 } else {
                     control_net_backend = ggml_backend_dev_init(gpu_devices[0].first, nullptr);
                 }
             }
         }
 
+        backend_dev_type = ggml_backend_dev_type(ggml_backend_get_device(backend));
+        if (clip_backend) {
+            clip_backend_dev_type = ggml_backend_dev_type(ggml_backend_get_device(clip_backend));
+        }
+        if (vae_backend) {
+            vae_backend_dev_type = ggml_backend_dev_type(ggml_backend_get_device(vae_backend));
+        }
+        if (control_net_backend) {
+            control_net_backend_dev_type = ggml_backend_dev_type(ggml_backend_get_device(control_net_backend));
+        }
+        reg_cpu = ggml_backend_dev_backend_reg(ggml_backend_get_device(backend_cpu));
+
         ModelLoader model_loader;
 
         vae_tiling = vae_tiling_;
@@ -669,25 +690,25 @@ public:
 
             size_t total_params_ram_size  = 0;
             size_t total_params_vram_size = 0;
-            if (ggml_backend_is_cpu(clip_backend)) {
+            if (clip_backend_dev_type == GGML_BACKEND_DEVICE_TYPE_CPU) {
                 total_params_ram_size += clip_params_mem_size + pmid_params_mem_size;
             } else {
                 total_params_vram_size += clip_params_mem_size + pmid_params_mem_size;
             }
 
-            if (ggml_backend_is_cpu(backend)) {
+            if (backend_dev_type == GGML_BACKEND_DEVICE_TYPE_CPU) {
                 total_params_ram_size += unet_params_mem_size;
             } else {
                 total_params_vram_size += unet_params_mem_size;
             }
 
-            if (ggml_backend_is_cpu(vae_backend)) {
+            if (vae_backend_dev_type == GGML_BACKEND_DEVICE_TYPE_CPU) {
                 total_params_ram_size += vae_params_mem_size;
             } else {
                 total_params_vram_size += vae_params_mem_size;
             }
 
-            if (ggml_backend_is_cpu(control_net_backend)) {
+            if (control_net_backend_dev_type == GGML_BACKEND_DEVICE_TYPE_CPU) {
                 total_params_ram_size += control_net_params_mem_size;
             } else {
                 total_params_vram_size += control_net_params_mem_size;
@@ -701,15 +722,15 @@ public:
                 total_params_vram_size / 1024.0 / 1024.0,
                 total_params_ram_size / 1024.0 / 1024.0,
                 clip_params_mem_size / 1024.0 / 1024.0,
-                ggml_backend_is_cpu(clip_backend) ? "RAM" : "VRAM",
+                clip_backend_dev_type == GGML_BACKEND_DEVICE_TYPE_CPU ? "RAM" : "VRAM",
                 unet_params_mem_size / 1024.0 / 1024.0,
-                ggml_backend_is_cpu(backend) ? "RAM" : "VRAM",
+                backend_dev_type == GGML_BACKEND_DEVICE_TYPE_CPU ? "RAM" : "VRAM",
                 vae_params_mem_size / 1024.0 / 1024.0,
-                ggml_backend_is_cpu(vae_backend) ? "RAM" : "VRAM",
+                vae_backend_dev_type == GGML_BACKEND_DEVICE_TYPE_CPU ? "RAM" : "VRAM",
                 control_net_params_mem_size / 1024.0 / 1024.0,
-                ggml_backend_is_cpu(control_net_backend) ? "RAM" : "VRAM",
+                control_net_backend_dev_type == GGML_BACKEND_DEVICE_TYPE_CPU ? "RAM" : "VRAM",
                 pmid_params_mem_size / 1024.0 / 1024.0,
-                ggml_backend_is_cpu(clip_backend) ? "RAM" : "VRAM");
+                clip_backend_dev_type == GGML_BACKEND_DEVICE_TYPE_CPU ? "RAM" : "VRAM");
         }
 
         int64_t t1 = ggml_time_ms();
@@ -798,17 +819,19 @@ public:
     }
 
     bool is_using_v_parameterization_for_sd2(ggml_context* work_ctx, bool is_inpaint = false) {
+        auto * ggml_set_f32_fn = (decltype(ggml_set_f32)*)ggml_backend_reg_get_proc_address(reg_cpu, "ggml_set_f32");
+
         struct ggml_tensor* x_t = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 8, 8, 4, 1);
-        ggml_set_f32(x_t, 0.5);
+        ggml_set_f32_fn(x_t, 0.5);
         struct ggml_tensor* c = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 1024, 2, 1, 1);
-        ggml_set_f32(c, 0.5);
+        ggml_set_f32_fn(c, 0.5);
 
         struct ggml_tensor* timesteps = ggml_new_tensor_1d(work_ctx, GGML_TYPE_F32, 1);
-        ggml_set_f32(timesteps, 999);
+        ggml_set_f32_fn(timesteps, 999);
 
         struct ggml_tensor* concat = is_inpaint ? ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 8, 8, 5, 1) : NULL;
         if (is_inpaint) {
-            ggml_set_f32(concat, 0);
+            ggml_set_f32_fn(concat, 0);
         }
 
         int64_t t0              = ggml_time_ms();
@@ -927,8 +950,9 @@ public:
         struct ggml_tensor* c_crossattn = NULL;
         {
             if (force_zero_embeddings) {
+                auto * ggml_set_f32_fn = (decltype(ggml_set_f32)*)ggml_backend_reg_get_proc_address(reg_cpu, "ggml_set_f32");
                 c_crossattn = ggml_new_tensor_1d(work_ctx, GGML_TYPE_F32, clip_vision->vision_model.projection_dim);
-                ggml_set_f32(c_crossattn, 0.f);
+                ggml_set_f32_fn(c_crossattn, 0.f);
             } else {
                 sd_image_f32_t image         = sd_image_t_to_sd_image_f32_t(init_image);
                 sd_image_f32_t resized_image = clip_preprocess(image, clip_vision->vision_model.image_size);
@@ -950,8 +974,9 @@ public:
         struct ggml_tensor* c_concat = NULL;
         {
             if (force_zero_embeddings) {
+                auto * ggml_set_f32_fn = (decltype(ggml_set_f32)*)ggml_backend_reg_get_proc_address(reg_cpu, "ggml_set_f32");
                 c_concat = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, width / 8, height / 8, 4, 1);
-                ggml_set_f32(c_concat, 0.f);
+                ggml_set_f32_fn(c_concat, 0.f);
             } else {
                 ggml_tensor* init_img = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, width, height, 3, 1);
 
@@ -1912,6 +1937,8 @@ sd_image_t* txt2img(sd_ctx_t* sd_ctx,
         return NULL;
     }
 
+    auto * ggml_set_f32_fn = (decltype(ggml_set_f32)*)ggml_backend_reg_get_proc_address(sd_ctx->sd->reg_cpu, "ggml_set_f32");
+
     struct ggml_init_params params;
     params.mem_size = static_cast<size_t>(10 * 1024 * 1024);  // 10 MB
     if (sd_version_is_sd3(sd_ctx->sd->version)) {
@@ -1952,11 +1979,11 @@ sd_image_t* txt2img(sd_ctx_t* sd_ctx,
     int H                    = height / 8;
     ggml_tensor* init_latent = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, W, H, C, 1);
     if (sd_version_is_sd3(sd_ctx->sd->version)) {
-        ggml_set_f32(init_latent, 0.0609f);
+        ggml_set_f32_fn(init_latent, 0.0609f);
     } else if (sd_version_is_flux(sd_ctx->sd->version)) {
-        ggml_set_f32(init_latent, 0.1159f);
+        ggml_set_f32_fn(init_latent, 0.1159f);
     } else {
-        ggml_set_f32(init_latent, 0.f);
+        ggml_set_f32_fn(init_latent, 0.f);
     }
 
     if (sd_version_is_inpaint(sd_ctx->sd->version)) {
@@ -2200,6 +2227,8 @@ SD_API sd_image_t* img2vid(sd_ctx_t* sd_ctx,
         return NULL;
     }
 
+    auto * ggml_set_f32_fn = (decltype(ggml_set_f32)*)ggml_backend_reg_get_proc_address(sd_ctx->sd->reg_cpu, "ggml_set_f32");
+
     LOG_INFO("img2vid %dx%d", width, height);
 
     std::vector<float> sigmas = sd_ctx->sd->denoiser->get_sigmas(sample_steps);
@@ -2232,10 +2261,10 @@ SD_API sd_image_t* img2vid(sd_ctx_t* sd_ctx,
                                                      augmentation_level);
 
     auto uc_crossattn = ggml_dup_tensor(work_ctx, cond.c_crossattn);
-    ggml_set_f32(uc_crossattn, 0.f);
+    ggml_set_f32_fn(uc_crossattn, 0.f);
 
     auto uc_concat = ggml_dup_tensor(work_ctx, cond.c_concat);
-    ggml_set_f32(uc_concat, 0.f);
+    ggml_set_f32_fn(uc_concat, 0.f);
 
     auto uc_vector = ggml_dup_tensor(work_ctx, cond.c_vector);
 
@@ -2251,7 +2280,7 @@ SD_API sd_image_t* img2vid(sd_ctx_t* sd_ctx,
     int W                   = width / 8;
     int H                   = height / 8;
     struct ggml_tensor* x_t = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, W, H, C, video_frames);
-    ggml_set_f32(x_t, 0.f);
+    ggml_set_f32_fn(x_t, 0.f);
 
     struct ggml_tensor* noise = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, W, H, C, video_frames);
     ggml_tensor_set_f32_randn(noise, rng);
@@ -2638,6 +2667,8 @@ sd_sampling_stream_t* txt2img_stream(sd_ctx_t* sd_ctx,
         return nullptr;
     }
 
+    auto * ggml_set_f32_fn = (decltype(ggml_set_f32)*)ggml_backend_reg_get_proc_address(sd_ctx->sd->reg_cpu, "ggml_set_f32");
+
     std::vector<int> skip_layers_vec;
     if (skip_layers != nullptr) {
         skip_layers_vec.assign(skip_layers, skip_layers + skip_layers_count);
@@ -2676,11 +2707,11 @@ sd_sampling_stream_t* txt2img_stream(sd_ctx_t* sd_ctx,
     int H                    = height / 8;
     ggml_tensor* init_latent = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, W, H, C, 1);
     if (sd_version_is_sd3(sd_ctx->sd->version)) {
-        ggml_set_f32(init_latent, 0.0609f);
+        ggml_set_f32_fn(init_latent, 0.0609f);
     } else if (sd_version_is_flux(sd_ctx->sd->version)) {
-        ggml_set_f32(init_latent, 0.1159f);
+        ggml_set_f32_fn(init_latent, 0.1159f);
     } else {
-        ggml_set_f32(init_latent, 0.f);
+        ggml_set_f32_fn(init_latent, 0.f);
     }
 
     if (sd_version_is_inpaint(sd_ctx->sd->version)) {
diff --git a/t5.hpp b/t5.hpp
index 2a53e27..e5e59a5 100644
--- a/t5.hpp
+++ b/t5.hpp
@@ -954,7 +954,7 @@ struct T5Embedder {
 
     static void load_from_file_and_test(const std::string& file_path) {
         // ggml_backend_t backend    = ggml_backend_cuda_init(0);
-        ggml_backend_t backend         = ggml_backend_cpu_init();
+        ggml_backend_t backend         = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
         ggml_type model_data_type      = GGML_TYPE_F32;
         std::shared_ptr<T5Embedder> t5 = std::shared_ptr<T5Embedder>(new T5Embedder(backend));
         {
diff --git a/unet.hpp b/unet.hpp
index 98abde0..0e78c23 100644
--- a/unet.hpp
+++ b/unet.hpp
@@ -635,6 +635,10 @@ struct UNetModelRunner : public GGMLRunner {
         struct ggml_context* work_ctx = ggml_init(params);
         GGML_ASSERT(work_ctx != NULL);
 
+        auto * back_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
+        auto * reg_cpu = ggml_backend_dev_backend_reg(ggml_backend_get_device(back_cpu));
+        auto * ggml_set_f32_fn = (decltype(ggml_set_f32)*)ggml_backend_reg_get_proc_address(reg_cpu, "ggml_set_f32");
+
         {
             // CPU, num_video_frames = 1, x{num_video_frames, 8, 8, 8}: Pass
             // CUDA, num_video_frames = 1, x{num_video_frames, 8, 8, 8}: Pass
@@ -645,15 +649,15 @@ struct UNetModelRunner : public GGMLRunner {
             auto x = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 8, 8, 8, num_video_frames);
             std::vector<float> timesteps_vec(num_video_frames, 999.f);
             auto timesteps = vector_to_ggml_tensor(work_ctx, timesteps_vec);
-            ggml_set_f32(x, 0.5f);
+            ggml_set_f32_fn(x, 0.5f);
             // print_ggml_tensor(x);
 
             auto context = ggml_new_tensor_3d(work_ctx, GGML_TYPE_F32, 1024, 1, num_video_frames);
-            ggml_set_f32(context, 0.5f);
+            ggml_set_f32_fn(context, 0.5f);
             // print_ggml_tensor(context);
 
             auto y = ggml_new_tensor_2d(work_ctx, GGML_TYPE_F32, 768, num_video_frames);
-            ggml_set_f32(y, 0.5f);
+            ggml_set_f32_fn(y, 0.5f);
             // print_ggml_tensor(y);
 
             struct ggml_tensor* out = NULL;
diff --git a/upscaler.cpp b/upscaler.cpp
index a808638..dfa6aba 100644
--- a/upscaler.cpp
+++ b/upscaler.cpp
@@ -57,7 +57,7 @@ struct UpscalerGGML {
         // initialize the backend
         if (gpu_devices.empty()) {
             // no GPU devices available
-            backend = ggml_backend_cpu_init();
+            backend = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
         } else {
             // use the first GPU device: device 0
             backend = ggml_backend_dev_init(gpu_devices[0].first, nullptr);
diff --git a/util.cpp b/util.cpp
index 8c358dd..687cc57 100644
--- a/util.cpp
+++ b/util.cpp
@@ -413,20 +413,20 @@ void sd_set_progress_callback(sd_progress_cb_t cb, void* data) {
 const char* sd_get_system_info() {
     static char buffer[1024];
     std::stringstream ss;
-    ss << "System Info: \n";
-    ss << "    SSE3 = " << ggml_cpu_has_sse3() << std::endl;
-    ss << "    AVX = " << ggml_cpu_has_avx() << std::endl;
-    ss << "    AVX2 = " << ggml_cpu_has_avx2() << std::endl;
-    ss << "    AVX512 = " << ggml_cpu_has_avx512() << std::endl;
-    ss << "    AVX512_VBMI = " << ggml_cpu_has_avx512_vbmi() << std::endl;
-    ss << "    AVX512_VNNI = " << ggml_cpu_has_avx512_vnni() << std::endl;
-    ss << "    FMA = " << ggml_cpu_has_fma() << std::endl;
-    ss << "    NEON = " << ggml_cpu_has_neon() << std::endl;
-    ss << "    ARM_FMA = " << ggml_cpu_has_arm_fma() << std::endl;
-    ss << "    F16C = " << ggml_cpu_has_f16c() << std::endl;
-    ss << "    FP16_VA = " << ggml_cpu_has_fp16_va() << std::endl;
-    ss << "    WASM_SIMD = " << ggml_cpu_has_wasm_simd() << std::endl;
-    ss << "    VSX = " << ggml_cpu_has_vsx() << std::endl;
+//    ss << "System Info: \n";
+//    ss << "    SSE3 = " << ggml_cpu_has_sse3() << std::endl;
+//    ss << "    AVX = " << ggml_cpu_has_avx() << std::endl;
+//    ss << "    AVX2 = " << ggml_cpu_has_avx2() << std::endl;
+//    ss << "    AVX512 = " << ggml_cpu_has_avx512() << std::endl;
+//    ss << "    AVX512_VBMI = " << ggml_cpu_has_avx512_vbmi() << std::endl;
+//    ss << "    AVX512_VNNI = " << ggml_cpu_has_avx512_vnni() << std::endl;
+//    ss << "    FMA = " << ggml_cpu_has_fma() << std::endl;
+//    ss << "    NEON = " << ggml_cpu_has_neon() << std::endl;
+//    ss << "    ARM_FMA = " << ggml_cpu_has_arm_fma() << std::endl;
+//    ss << "    F16C = " << ggml_cpu_has_f16c() << std::endl;
+//    ss << "    FP16_VA = " << ggml_cpu_has_fp16_va() << std::endl;
+//    ss << "    WASM_SIMD = " << ggml_cpu_has_wasm_simd() << std::endl;
+//    ss << "    VSX = " << ggml_cpu_has_vsx() << std::endl;
     snprintf(buffer, sizeof(buffer), "%s", ss.str().c_str());
     return buffer;
 }
diff --git a/vae.hpp b/vae.hpp
index 9f42b6c..7bea8fe 100644
--- a/vae.hpp
+++ b/vae.hpp
@@ -583,13 +583,17 @@ struct AutoEncoderKL : public GGMLRunner {
         struct ggml_context* work_ctx = ggml_init(params);
         GGML_ASSERT(work_ctx != NULL);
 
+        auto * back_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);
+        auto * reg_cpu = ggml_backend_dev_backend_reg(ggml_backend_get_device(back_cpu));
+        auto * ggml_set_f32_fn = (decltype(ggml_set_f32)*)ggml_backend_reg_get_proc_address(reg_cpu, "ggml_set_f32");
+
         {
             // CPU, x{1, 3, 64, 64}: Pass
             // CUDA, x{1, 3, 64, 64}: Pass, but sill get wrong result for some image, may be due to interlnal nan
             // CPU, x{2, 3, 64, 64}: Wrong result
             // CUDA, x{2, 3, 64, 64}: Wrong result, and different from CPU result
             auto x = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 64, 64, 3, 2);
-            ggml_set_f32(x, 0.5f);
+            ggml_set_f32_fn(x, 0.5f);
             print_ggml_tensor(x);
             struct ggml_tensor* out = NULL;
 
@@ -607,7 +611,7 @@ struct AutoEncoderKL : public GGMLRunner {
             // CPU, z{3, 4, 8, 8}: Wrong result
             // CUDA, z{3, 4, 8, 8}: Wrong result, and different from CPU result
             auto z = ggml_new_tensor_4d(work_ctx, GGML_TYPE_F32, 8, 8, 4, 1);
-            ggml_set_f32(z, 0.5f);
+            ggml_set_f32_fn(z, 0.5f);
             print_ggml_tensor(z);
             struct ggml_tensor* out = NULL;
 
